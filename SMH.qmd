---
title: "Metropolis-Hastings"
---

```{r setup}
#| echo: false

knitr::opts_chunk$set(fig.align='center')

evaluate_log_prior <- function(v, p) {
  n <- length(v)
  if (n != nrow(p)) {
    print("Priors/params inconsistent")
    return(0)
  }
  d <- matrix(0, n, 1)
  for (k in 1:n) {
    d[k] <- exec(paste0("d", p$PDF[k]), v[k], p$p1[k], p$p2[k])
  }
  return(sum(log(d)))
}

posterior <- function(v, p) {
  return(evaluate_log_prior(v, p))
}
```

## Complicated densities

What if we have a posterior density that is too complicated to factor into a full set of conditional densities? Recall that Gibbs Sampling is a procedure that generates _marginal densities_ from _conditional densities_. What if we don't have conditional densities?

::: {.callout-tip}
## Why can't we use Gibbs for DSGEs?

Say a model only has two unknown parameters, $\alpha$ and $\gamma$. Any predictions of that model conditional on them is:
\begin{equation}
  y = f(X,\ \alpha,\ \gamma)
\end{equation}
The likelihood of that model
\begin{equation}
  \mathcal{L}(\alpha,\ \gamma\ | \ y,\ X)
\end{equation}
is readily available but the conditional density of say, $\alpha$ 
\begin{equation}
  \mathcal{P}(\alpha\ | \ y,\ X,\ \gamma)
\end{equation}
typically isn't. This is because the predictive model $f(\cdot)$ depends on all the parameters through the reduced form solution used in DSGEs even if the underlying model is linear, i.e. the solution in @BK1980. As $f(X,\ \alpha,\ \gamma)$ ***always*** depends on both $\alpha$ and $\gamma$ it is difficult to find an appropriate conditional likelihood which isolates one of them.
:::

Can we derive a technique that generates marginal densities from a _joint density_? Turns out we (perhaps surprisingly) can, using the _Metropolis-Hastings algorithm_.

### Target density

We'll derive the simplest posterior density that we can use in an exercise, but we begin with a more general case. Consider a posterior likelihood that is the product of a likelihood and $k$ prior densities, say
\begin{equation}
 \mathcal{H}(\theta\ | \ y) = \mathcal{L}(\theta\ | \ y)\times \mathcal{P}_1(\theta_1)\times \mathcal{P}_2(\theta_2)\times \mathcal{P}_3(\theta_3)\times ...\times \mathcal{P}_k(\theta_k)
\end{equation}
This is the _target density_. How can we estimate the densities of the underlying $\theta_i$ from this posterior alone? This (somewhat amazingly) turns out to be rather simple. 

The trick is to simulate draws for all the elements of $\theta$ from the target density -- despite not having a generating function to draw from the density -- and then estimating the marginal densities from the resulting series.

## Simplified problem: estimating (known) prior

As the aim is to introduce MH in as simple a context as possible, we will sample from a posterior distribution for which we don't have an appropriate random number generator but for marginal distributions that we do. This means we can compare analytical and estimated results.

We will try an estimate the marginal distributions for the priors alone; essentially $\mathcal{H}(\theta\ | \ y)$ where $\mathcal{L}(y\ |\ \theta)$ is flat for all values of the prior so 
\begin{equation}
 \mathcal{H}(\theta\ | \ y) = \mathcal{P}_1(\theta_1)\times \mathcal{P}_2(\theta_2)\times \mathcal{P}_3(\theta_3)\times ...\times \mathcal{P}_k(\theta_k)
\end{equation}
This means we know what the marginals should look like -- they are just the priors! 

Assume there are $k$ unknown parameters with a prior density set by the investigator, such as
\begin{equation}
  \mathcal{P}(\rho_1) \sim \text{Beta}(1.2,1.8)
\end{equation}
subject to the arbitrary bounds that $0.001 < \rho_1 <0.999$. 

We have $k$ of these, so in any code we could specify this in a matrix where each prior is specified in a row containing a name, a PDF type, the parameters of the PDF, as well as a lower and upper bound.

## Example 1

First example: target density is the product of six parameters densities of four types: Normal, Gamma, Inverse Gamma and Beta. Any suitable density could be used as a prior, so the Uniform, say, or the Inverse Weibull or Log Gamma could be slotted in -- and we will later on. All that is required is that the some function exists to evaluate the density. Obviously we could generalize this to one or three or more parameter distributions with appropriate code. 

For example we could fit a Skew-$t$ say, as long as we can evaluate the (log) density for this 4 parameter distribution. See @LossModels for a very comprehensive list of densities we could use -- I cannot recommend this highly enough.

```{r Ex1}
#| warning: false
#| echo: false
#| fig-cap: Parameters of six densities used in Example 1
library(knitr)
library(kableExtra)
library(actuar)
library(tidyverse)

pp <- tibble(name = c("beta", "rho[1]", "kappa",  "mu",    "sigma[1]",  "sigma[2]"), 
             lb   = c(0.001,  0.001,    0.001,    -5,      0.001,       0.001),
             ub   = c(0.999,  0.999,    3,        1,       5,           5),
             PDF  = c("beta", "beta",   "gamma",  "norm",  "invgamma",  "invgamma"), 
             p1   = c(2.3,    1.2,      2,        -2,      12,          9),
             p2   = c(1.2,    1.8,      4,        0.55,    0.05,        0.075))

kable(pp, digits=3) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

### Analytic densities

```{r theoretical}
#| message: false
#| warning: false
#| echo: false
#| fig-cap: Plots of the theoretical densities given parameters in Table
densd <- NULL
for (k in 1:dim(pp)[1]) {
  x <- seq(pp$lb[k], pp$ub[k], length.out=100)
  p <- exec(paste0("d", pp$PDF[k]), x, pp$p1[k], pp$p2[k])
  densd <- bind_rows(densd, tibble(dens=p, dom=x, PDF=pp$PDF[k], Parameter=pp$name[k]))
}

ggplot(densd) +
  geom_area(aes(x=dom, y=dens, fill=PDF), color=NA, alpha=.7) +
  theme_minimal() +
  labs(title="Priors", x="", y="") +
  facet_wrap(~Parameter, scales="free", labeller=label_parsed, ncol=2)
```

### Random draws

```{r warning=FALSE}
#| warning: false
#| echo: false
#| fig-cap: True density known so we can draw from an appropriate random number generator
drawd <- NULL
n     <- 5000
for (k in 1:dim(pp)[1]) {
  d     <- exec(paste0("r", pp$PDF[k]), n, pp$p1[k], pp$p2[k])
  drawd <- bind_rows(drawd, tibble(val=d, PDF=pp$PDF[k], Parameter=pp$name[k]))
}

ggplot(bind_rows(drawd, densd)) +
  geom_histogram(aes(x=val, y=after_stat(density), fill=PDF), alpha=.5, bins=50, color="grey77") +
  geom_area(aes(x=dom, y=dens, group=PDF, fill=PDF), color=NA, alpha=.3) +
  theme_minimal() +
  labs(title=paste("Simulated vs. theoretical densities, n =",n), x="", y="") +
  facet_wrap(~ Parameter, ncol=2, scales="free", labeller=label_parsed)
```

## Estimating the marginals from the joint density

First we need to understand the estimation procedure. The MH algorithm uses only information from the posterior to estimate the marginal processes that generated it, in stark contrast with Gibbs Sampling that uses conditional densities to approximate the unconditional one and then back out the marginals. 

### Metropolis-Hastings

A thorough explanation can be found in @Chib or @BDA, and here we describe the procedure without proof. Our aim is to draw samples from some distribution
$$
 \mathcal{H}(\theta)
$$
where a direct approach is not feasible, because we don't have a random number generator. The Metropolis-Hastings algorithm requires that we can evaluate this posterior density at some arbitrary points. As the form of the marginals is (potentially) unknown, we draw values from some arbitrary density and decide whether it looks like it came from the marginals that generated the posterior. $\mathcal{H}(\theta)$ is typically the posterior density where this distribution is far too complex to directly sample.

This indirect approach is to specify a _candidate density_
$$
  q(\theta^{k+1}|\theta^k)
$$
from which we _can_ make candidate draws. Given some value for the parameters $\theta^k$, we can randomly generate new values, which may or may not be independent of this draws.

The MH algorithm requires that we are able to evaluate $\frac{H(\theta^{k+1})}{H(\theta^k)}$, and then draw a _candidate_ value $\theta^{k+1}$ from $q(\theta^{k+1}|\theta^k)$. We then accept this candidate value with the probability
$$
  \alpha = \min \left(\frac{H(\theta^{k+1})/q(\theta^{k+1}|\theta^k)}{H(\theta^k)/q(\theta^k|\theta^{k+1})}, 1\right)
$$
Practically, this requires we compute $\alpha$ and draw a number $u$ from $U(0,1)$, and if $u<\alpha$ accept $\theta^{k+1}$ otherwise keep $\theta^k$. 

### Simplification

The _random walk_ version of the algorithm takes the specific candidate density $q(\theta^{k+1}|\theta^k)$ as  
$$
 \theta^{k+1} = \theta^k + \epsilon_t
$$ 
where $\epsilon_t\sim N(0,\Sigma)$ for some $\Sigma$ which we need to choose. This is a simple vector-random walk. Let $\theta^k$ be some existing draw and $\theta^{k+1}$ be a new draw. We can write
$$
  \epsilon_t = \theta^{k+1}-\theta^k
$$
then
$$
  P(\epsilon_t) = P(\theta^{k+1}-\theta^k)
$$

Because this is a normal density (which is symmetric) then  
$$
  P(\epsilon_t) = P(-\epsilon_t)
$$
Symmetry implies an acceptance probability of  
$$
  \frac{H(\theta^{k+1})}{H(\theta^k)}
$$
as $q(\theta^{k+1}|\theta^k) = q(\theta^k|\theta^{k+1})$ so these terms cancel. 

#### Algorithm

::: {}
**Step 1** Draw a _candidate_ value $\theta^{G+1}$ from $q(\theta^{k+1}|\theta^k)$, specifically $\theta^{k+1} = \theta^k + \epsilon_t$ where $\epsilon_t\sim N(0,\Sigma)$

**Step 2** Compute the acceptance probability 
$$
  \alpha = \min \left(\frac{H(\theta^{k+1})}{H(\theta^k)}, 1\right)
$$

**Step 3** If $u\sim U(0,1)$ is less than $\alpha$, keep $\theta^{k+1}$, else repeat $\theta^k$ and discard the new draw
:::

::: {.callout-note}
- The density $\mathcal{H}(\theta)$ will usually be a posterior, combining the priors and likelihood information;
- At present we have no likelihood information, so all we need is a function to evaluate the (log) joint prior.
:::

## A simpler problem

We have no data (or indeed model) to pass to the posterior function (as there is no likelihood). Generalizing this to incorporate likelihood information is straightforward.
```{r MH_function}
#| echo: false
MH_RW <- function(init_val, p, reps, burn, scale) {
  np     <- length(init_val)
  b_old  <- init_val
  lp_old <- posterior(b_old, p)
  draws  <- matrix(0, np, reps-burn, dimnames=list(p$name))  # Store draws
  nacc   <- 0                                                # Number of acceptances
  for (i in 1:reps) {
    b_new <- b_old + scale*rnorm(np)                         # New draw of the parameters
    if (all(b_new < p$ub & b_new > p$lb)) {                  # Test if draw withing bounds
      lp_new <- posterior(b_new, p)
      if (is.nan(lp_new)) {lp_new <- -Inf}
      if (runif(1) < min(exp(lp_new-lp_old), 1)) {           # Test to accept or not
        b_old  <- b_new
        lp_old <- lp_new
        if (i > burn) nacc <- nacc+1
        }
      }
    if (i > burn) draws[,i-burn] <- b_old  # Store past the burn in period only
    }   
  print(paste("Acceptance ratio:", nacc/(reps-burn)))
  return(draws)
  }
```
We need to specify the scale of the random walk: assume 
$$
  \Sigma = sI
$$
We should choose a value of $s$ to ensure that the whole parameter space is explored as

- if $s$ too small we don't walk far enough, and stay too close to potentially only local maxima;
- if $s$ too large may jump over highest density points at every step and take a long time to converge.

We check if it is a suitable value by monitoring the acceptance rate: between about 1/5 and 2/5 fine.

### Estimating example 1

```{r Posterior}
#| echo: false
init_val  <- c(.9,.2,.4,-2,1.5,1.5)
lp <- posterior(init_val, pp)
```

Choose some arbitrary initial values at which we can evaluate our posterior likelihood (remembering for this example this only the joint prior). These are `r init_val`. As we have chosen values close to the highest density this evaluates as `r lp`. We choose $s=0.25$ and do 100000 iterations and discard the first half. For the run that generates the graphs below we get the message:
```{r MHcall}
#| echo: false
reps  <- 100000
burn  <- reps/2
draws <- MH_RW(init_val, pp, reps, burn, 0.25) 
```
The acceptance ratio is good, so we plot the draws:
```{r warning=FALSE}
#| warning: false
#| echo: false
#| fig-cap: Plots of the 50,000 draws
#| fig-cap-location: margin
as_tibble(t(draws), .name_repair = ~ rownames(draws)) %>% 
  mutate(X = 1:(reps-burn)) %>%
  pivot_longer(cols=-X, names_to="Parameter", values_to="val") %>%
  bind_rows(densd) %>%
  ggplot() + 
  geom_line(aes(x=X, y=val, color=Parameter), alpha=.75, show.legend=F) +
  facet_wrap(~Parameter, scales="free", labeller=label_parsed, ncol=2) +
  theme_minimal() + 
  labs(title="RW-MH estimate of pure prior", x="", y="") 
```
or just the first few to better see the algorithm in action:
```{r warning=FALSE}
#| warning: false
#| echo: false
#| fig-cap: Plots of first 333 draws
as_tibble(t(draws), .name_repair = ~ rownames(draws)) %>% 
  mutate(X = 1:(reps-burn)) %>%
  slice(1:333) %>% 
  pivot_longer(cols=-X, names_to="Parameter", values_to="val") %>%
  bind_rows(densd) %>%
  ggplot() + 
  geom_line(aes(x=X, y=val, color=Parameter), alpha=.75, show.legend=F) +
  facet_wrap(~Parameter, scales="free", labeller=label_parsed, ncol=2) +
  theme_minimal() + 
  labs(title="RW-MH estimate of pure prior", x="", y="") 
```
Clearly there are some repeat values. What do the estimated densiites from each of these sequences look like?
```{r warning=FALSE}
#| warning: false
#| echo: false
#| fig-cap: Histograms of draws and theoretical priors
as_tibble(t(draws), .name_repair = ~ rownames(draws)) %>% 
  pivot_longer(cols=everything(), names_to="Parameter", values_to="val") %>%
  bind_rows(densd) %>%
  ggplot() + 
  geom_area(aes(x=dom, y=dens, group=PDF), fill="grey66", color=NA, alpha=.7) +
  geom_histogram(aes(x=val, y=after_stat(density), color=Parameter, fill=Parameter), 
                 alpha=.3, bins=50, show.legend=F) +
  facet_wrap(~Parameter, scales="free", labeller=label_parsed, ncol=2) +
  theme_minimal() + 
  labs(title="RW-MH estimate of pure prior", x="", y="") 
```
Pretty good! Compare this with the sequence of draws we got from the correct random number generators for each density. 

### Example 2

```{r prior2}
#| echo: false
#| fig-cap: All-different distributions -- Inverse Weibull, Paralogistic, Inverse Pareto, Log Gamma and Uniform
p2 <- tibble(name = c("eta", "zeta[1]", "zeta[2]", "delta", "alpha[1]", "alpha[2]", "upsilon[1]", "upsilon[2]"), 
             lb   = c(0.25, 0.001, 0.001, 0.001, 1.001, 1.001, 0.001, 0.001),
             ub   = c(5, 6, 3, 5, 5, 5, 0.999, 0.999),
             PDF  = c("invweibull", "paralogis", "paralogis", "invpareto", "lgamma", "lgamma", "unif", "unif"), 
             p1   = c(2.3, 1, 2,  2,   2, 3, 0.3, 0.5),
             p2   = c(1.2, 4, 3,  0.3, 5, 4, 0.7, 1))

kable(p2, digits=3) %>%
  kable_styling(bootstrap_options = "striped", full_width = F)
```

```{r}
#| echo: false
#| fig-cap: Plots of the theoretical densities given parameters in Table 2
densd2 <- NULL
for (k in 1:dim(p2)[1]) {
  x <- seq(p2$lb[k], p2$ub[k], length.out=1000)
  p <- exec(paste0("d", p2$PDF[k]), x, p2$p1[k], p2$p2[k])
  densd2 <- bind_rows(densd2, tibble(dens=p, dom=x, PDF=p2$PDF[k], Parameter=p2$name[k]))
}

ggplot(densd2) +
  geom_area(aes(x=dom, y=dens, group=Parameter, fill=PDF), color=NA, alpha=.7) +
  theme_minimal() +
  labs(title="Priors", x="", y="") +
  facet_wrap(~Parameter, scales="free", labeller=label_parsed, ncol=2)
```

```{r MHcall2}
#| warning: false
#| results: hide
#| echo: false
#| fig-cap: Walk a little less $s=0.125$ (could iterate a little more?)
init_val2  <- c(1,1,1,1,2,2,.5,.5)
draws2     <- MH_RW(init_val2, p2, reps, burn, 0.125) 
as_tibble(t(draws2), .name_repair=~rownames(draws2)) %>% 
  pivot_longer(cols=everything(), names_to="Parameter", values_to="val") %>%
  bind_rows(densd2) %>%
  ggplot() + 
  geom_histogram(aes(x=val, y=..density.., group=Parameter), 
                 alpha=.7, bins=75, color="grey55", fill="grey88") +
  geom_area(aes(x=dom, y=dens, group=PDF, fill=PDF), color=NA, alpha=.5) +
  facet_wrap(~Parameter, scales="free", labeller=label_parsed, ncol=2) +
  theme_minimal() + 
  labs(title="RW-MH estimate of pure prior, Example 2", x="", y="") 
```

