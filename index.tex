% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  letterpaper,
]{book}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{makeidx}
\makeindex
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Quantiles, Networks, Time},
  pdfauthor={andrew p blake},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Quantiles, Networks, Time}
\author{andrew p blake}
\date{2025-01-01}

\begin{document}
\frontmatter
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[breakable, frame hidden, interior hidden, boxrule=0pt, borderline west={3pt}{0pt}{shadecolor}, enhanced, sharp corners]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoffigures
\listoftables
\mainmatter
\bookmarksetup{startatroot}

\hypertarget{sec-genesis}{%
\chapter*{Genesis}\label{sec-genesis}}
\addcontentsline{toc}{chapter}{Genesis}

\markboth{Genesis}{Genesis}

Central banks have been around quite a while. They began as a useful
institution that mostly benefited banks, and were not universally seen
as something that particularly benefited society. Central banking was
long associated with grey men in grey suits, pondering deeply the
impenetrable machinations of high finance, fuelled by cigar smoke and
mystique. In truth, this -- at best -- Capraesque view of central
bankers is substantially out of date, and has been for decades. They are
no longer monochrome, or exclusively male, and need skills their
forebears could have barely imagined -- and they have most decidedly
different priorities.

Like all institutions, central banks are the people who work in them.
And as with many occupations with a public service element, the
substantial expertise embodied in those people for the very particular
-- and evolving -- challenges of central banking could usefully be
shared. Global challenges often need global solutions, and local
challenges are faced everywhere and someone, somewhere has probably
faced the same one as you. Recognising this led to a striking initiative
taken by the Bank of England in the early 1990s. It was decided that the
Bank should create a forum where the central bankers of the world could
gather to commune, discuss, and above all, learn together.

The timing, of course, was not incidental and the initiative turned out
to be a prescient one. This was at a major historical turning point, one
that signalled a burgeoning new world order, as the Iron Curtain
crumbled, the European experiment gathered momentum, and industrial
might continued an inexorable shift eastwards. Economic policy had
shifted too. Monetary policy was beginning a new and -- as it turned out
-- lasting phase. There was indeed much to learn, and new monetary
policy needed new approaches better suited to those policies.
(Post-Great Financial Crisis, the necessary tool kit would change
again.)

And so the Centre for Central Banking Studies was founded.
\href{https://www.bankofengland.co.uk/-/media/boe/files/quarterly-bulletin/2006/the-centre-for-central-banking-studies.pdf}{Hammond
(2006)} provides a history of the early years of the CCBS, charting an
ambitious project that had an immediate impact on international central
banking practices. More than thirty years later it remains a key forum
for learning, discussion and networking, just as intended. Literally
thousands of central bankers have taken part in CCBS events, and many
alumni now occupy senior policymaking positions around the world.
Activities have evolved to include many more of the disparate areas of
responsibility that now involve the central banking community, and with
a truly global reach.

This book is about a small part of that output. It is (mostly) about
applied economics and central banking. It is born of the experience of
the many who have participated in events over the years, with literally
thousands of suggestions that have improved and expanded the content
delivered to properly reflect the daily concerns of the central bank
economist.

Mostly, but not entirely. Mostly, because the tool kit continues to
expand and the techniques of data analysis evolve. We are all data
scientists of some sort now, with a domain specialisation of central
banking. And that domain is somewhat different to economists in academia
or industry, with more of a focus on what we might dub causal
forecasting. Underlying much of central bank analysis are a number of
useful statistical methods and machine learning models that complement
the default econometric approach. Taken together these constitute a
language that central bankers need to understand. Some of this book is
about how to build, interpret, and use models that used to be anathema
to the econometrician but are increasingly part of the predictive
landscape.

\begin{figure}

\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{Yogyakarta.jpg}

}

\caption{Puppet, Yogyakarta}

}

\end{minipage}%
%
\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{AbuDhabi.jpg}

}

\caption{Abu Dhabi, United Arab Emirates}

}

\end{minipage}%
\newline
\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{Specs.gif}

}

\caption{Villa Sterne, Pretoria}

}

\end{minipage}%
%
\begin{minipage}[t]{0.50\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{Korea.jpg}

}

\caption{Insadong, Seoul}

}

\end{minipage}%

\end{figure}

\begin{figure}

{\centering \includegraphics{Montevideo.jpg}

}

\caption{Old Town, Montevideo}

\end{figure}

Placeholders abound.

\begin{tcolorbox}[enhanced jigsaw, breakable, left=2mm, arc=.35mm, toptitle=1mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, bottomrule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Disclaimer}, colframe=quarto-callout-note-color-frame, coltitle=black, titlerule=0mm, toprule=.15mm, bottomtitle=1mm, rightrule=.15mm, colback=white]

The Bank of England does not accept any liability for misleading or
inaccurate information or omissions in the information provided. The
subject matter reflects the views of the author and not the wider Bank
of England or its Policy Committees.

\end{tcolorbox}

\part{Methods matter}

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

This is a book created using \href{quarto.org}{Quarto} and includes all
examples as executable code.

See Andrew P. Blake and Mumtaz (2017).

\hypertarget{reading-is-good-for-you}{%
\section{Reading is good for you}\label{reading-is-good-for-you}}

For me, the best (although slightly dated) text is Hastie, Tibshirani,
and Friedman (2009)
\href{https://web.stanford.edu/~hastie/ElemStatLearn/}{The Elements of
Statistical Learning} and the best source for the mathematics, with an
easy-reading version by some of the same authors James et al. (2021)
\href{https://www.statlearning.com/}{Introduction to Statistical
Learning}.

I also rather like Boehmke and Greenwell (2019)
\href{https://bradleyboehmke.github.io/HOML/}{Hands-On Machine Learning
with R} which is something of a cookbook rather than a technical manual
but with wide scope. Taddy (2019) is more elementary.

On text, just read Silge and Robinson (2017)
\href{https://www.tidytextmining.com/}{Text Mining with R: A Tidy
Approach} and then Hvitfeldt and Silge (2021)
\href{https://smltar.com/}{Supervised Machine Learning for Text Analysis
in R}. That's it.

Two books I would solidly recommend to make us all into better
statisticians and not just econometricians are Gelman, Hill, and Vehtari
(2019) \href{http://www.stat.columbia.edu/~gelman/regression}{Regression
and Other Stories}, and McElreath (2020)
\href{https://github.com/rmcelreath/rethinking}{Statistical Rethinking}.

\hypertarget{non-econometric-methods-for-econometricians}{%
\chapter{Non-econometric methods for
econometricians}\label{non-econometric-methods-for-econometricians}}

\hypertarget{selected-ml-and-dataviz-techniques-in-r}{%
\section{Selected ML and Dataviz techniques in
R}\label{selected-ml-and-dataviz-techniques-in-r}}

Econometricians are used to handling data, performing analysis and
reporting results. But somewhere along the line data became big and
unstructured, analysis was now machines learning about something and
outputs became visualisations.

This online seminar takes some big(ish) datasets, sets the machines on
them and draws some great graphs. If you ever wondered what use a tree
was for forecasting or why everything is a network (including neural
ones), or wanted to draw a map with your house in it, or to understand a
document without the bother of reading it (or a few other things
besides) then you might find something to interest you. All done in R.

Specifically, this seminar is designed to introduce some of the key
methods used outside of econometrics that econometricians will find very
useful in their work in a central bank. This includes some important
machine learning techniques as a gateway to others, particularly
tree-based methods and neural networks, as well as text processing and
map making. All the way through there is an emphasis on the network
properties of many of these techniques. We make extensive use of the
\texttt{tidyverse}, including \texttt{ggplot2} and \texttt{tidytext},
and a number of statistics, machine learning, geographical data and
other packages.

The framework for each day is the following:

\begin{itemize}
\tightlist
\item
  Each day is divided into two two-hour sessions starting at 10.30 am
  and 2.00 pm GMT.
\item
  The first hour of each will be an online presentation covering a
  particular topic (or topics) with a look at both techniques and code.
\item
  After a quick break the second hour will be largely devoted to the
  code itself or resources to understand how to code the material.
\end{itemize}

We may run polls during the event to prioritize the topics covered in
the webinars as it is not expected that everyone will be able to try out
everything.

\hypertarget{the-code}{%
\subsection{The code}\label{the-code}}

All code and some of the data will be made available through the Juno
portal. For each presentation the .Rmd (R markdown) file is supplied
that creates the presentation, an HTML file of the presentation for you
to step through which can be re-created from the .Rmd file, and a
further .R file of the code that we use. Some additional code and data
is included, including links to a number of videos that cover some
additional aspects both in this file and in the presentations.

Some data will need to be downloaded from original other sites if all
the examples are to be followed. All code is additionally available at
\url{https://github.com/andrewpeterblake/R2020} or
\url{https://github.com/andrewpeterblake/R2021} or through the QR codes
below.

\begin{figure}

\begin{minipage}[t]{0.05\linewidth}

{\centering 

~

}

\end{minipage}%
%
\begin{minipage}[t]{0.40\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{R2021_files/figure-pdf/unnamed-chunk-1-1.pdf}

}

\caption{2020}

}

\end{minipage}%
%
\begin{minipage}[t]{0.10\linewidth}

{\centering 

~

}

\end{minipage}%
%
\begin{minipage}[t]{0.40\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{R2021_files/figure-pdf/unnamed-chunk-1-2.pdf}

}

\caption{2021}

}

\end{minipage}%
%
\begin{minipage}[t]{0.05\linewidth}

{\centering 

~

}

\end{minipage}%
\newline
\begin{minipage}[t]{0.05\linewidth}

{\centering 

~

}

\end{minipage}%
%
\begin{minipage}[t]{0.40\linewidth}

{\centering 

GitHub repositories for historic CCBS courses

}

\end{minipage}%
%
\begin{minipage}[t]{0.10\linewidth}

{\centering 

~

}

\end{minipage}%
%
\begin{minipage}[t]{0.05\linewidth}

{\centering 

~

}

\end{minipage}%

\end{figure}

\hypertarget{how-to-ensure-rstudio-finds-the-code}{%
\section{HOW TO ENSURE RSTUDIO FINDS THE
CODE}\label{how-to-ensure-rstudio-finds-the-code}}

To use the code, in particular so that R Studio finds the data files
etc, create a directory for each topic, (e.g.~Trees, ANN etc) and copy
the contents from the zip file or GitHub. Then create a new project in R
Studio that uses that directory as its home directory, using ``File/New
Project'' in the drop down menu. Opening files within a project sets the
home directory to that directory, so everything (including the
sub-directories) can be found.

\hypertarget{typical-program-structure}{%
\section{Typical program structure}\label{typical-program-structure}}

\hypertarget{day-1-trees-and-maps}{%
\subsection{Day 1: Trees and maps}\label{day-1-trees-and-maps}}

\hypertarget{trees}{%
\subsubsection{Trees}\label{trees}}

\begin{itemize}
\tightlist
\item
  Classification and regression trees
\item
  Econometrics strikes back: Bootstrap/bagging and Boosting/Model
  selection
\item
  Random forests
\item
  Visualising decision trees
\item
  Use example: House prices
\end{itemize}

The presentations for this are \texttt{Trees.html} and
\texttt{LondonHP.html}; The two programs \texttt{TreeCancer.R} and
\texttt{TreeNW.R} are the use examples.

\hypertarget{maps}{%
\subsubsection{Maps}\label{maps}}

\begin{itemize}
\tightlist
\item
  How to draw a map in R
\item
  A guide to some resources
\item
  Choropleths
\item
  Use examples: Climate change, regional data, postcode wrangling
\end{itemize}

The presentation for this is \texttt{MapAER.html} (see also
\texttt{Weatherpretty.html}); The program \texttt{MapAERcode.R} is the
main map drawing code, I've included \texttt{ZAF.R} as as short simple
way and source for two countries, and the directory \texttt{Trendz}
contains the program (\texttt{app.R}) and data for the weather example.

\begin{figure}

\begin{minipage}[t]{0.50\linewidth}

{\centering 

\includegraphics{R2021_files/figure-pdf/unnamed-chunk-2-1.pdf}

}

\end{minipage}%
%
\begin{minipage}[t]{0.50\linewidth}

{\centering 

I've included an additional video (red QR code) for more about Shiny.
This uses unemployment data from the
\href{https://www.philadelphiafed.org/surveys-and-data/real-time-data-research/survey-of-professional-forecasters}{Survey
of Professional Forecasters}. The code we look at is for climate change
data \href{https://climateknowledgeportal.worldbank.org/}{World Bank
data}.

}

\end{minipage}%

\end{figure}

A comprehensive treatment of maps is Lovelace, Nowosad, and Muenchow
(2019) \emph{Geocomputation in R}, but it is quite a lot to assimilate
all at once.

\hypertarget{day-2-networks}{%
\subsection{Day 2: Networks}\label{day-2-networks}}

\hypertarget{neural-networks}{%
\subsubsection{Neural networks}\label{neural-networks}}

\begin{itemize}
\tightlist
\item
  What is an ANN? Deep learning?
\item
  Function approximation via a network
\item
  Data: fit, validate, test
\item
  Network architecture
\item
  Use examples: House prices revisited
\end{itemize}

The presentation for this is \texttt{IntroANN.html}; The program
\texttt{ANN.R} replicates the ANN estimation. The data used is the same
as for Day 1.

\hypertarget{networks-real-ones}{%
\subsubsection{Networks (real ones)}\label{networks-real-ones}}

\begin{itemize}
\tightlist
\item
  DAGs and ANNs as network graphs
\item
  Incidence matrices
\item
  Measuring connectivity: Degree and betweenness
\item
  Plotting with \texttt{igraph}
\item
  Use examples: Industry inter-relationships
\end{itemize}

\begin{figure}

\begin{minipage}[t]{0.05\linewidth}

{\centering 

~

}

\end{minipage}%
%
\begin{minipage}[t]{0.40\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{R2021_files/figure-pdf/unnamed-chunk-3-1.pdf}

}

\caption{Coding club}

}

\end{minipage}%
%
\begin{minipage}[t]{0.10\linewidth}

{\centering 

~

}

\end{minipage}%
%
\begin{minipage}[t]{0.40\linewidth}

{\centering 

\raisebox{-\height}{

\includegraphics{R2021_files/figure-pdf/unnamed-chunk-3-2.pdf}

}

\caption{R-Bloggers article}

}

\end{minipage}%
%
\begin{minipage}[t]{0.05\linewidth}

{\centering 

~

}

\end{minipage}%
\newline
\begin{minipage}[t]{0.05\linewidth}

{\centering 

~

}

\end{minipage}%
%
\begin{minipage}[t]{0.40\linewidth}

{\centering 

Network examples

}

\end{minipage}%
%
\begin{minipage}[t]{0.10\linewidth}

{\centering 

~

}

\end{minipage}%
%
\begin{minipage}[t]{0.05\linewidth}

{\centering 

~

}

\end{minipage}%

\end{figure}

The presentation used for the first part of this is \texttt{DAG.html}
and the program \texttt{Draw\_DAG\_ANN.R} draws the ANN examples from
Day 2 Session 1 as well as some of the DAG examples. The example is
modified from Cunningham (2021) \emph{Causal Inference: The Mixtape},
which is a great read with R code. The pdf \texttt{HandShake3.pdf} is
the source of the director network graphs, and \texttt{Graph101a.R} is a
subset of the analytical work on the corruption data set as described in
the post
\href{https://www.r-bloggers.com/2020/01/graph-theory-101-with-corruption-cases-in-spain/}{\emph{Graph
Theory 101}} (purple QR code), which is the work of
\href{https://codingclubuc3m.rbind.io/talk/2020-01-21/}{Marina Medina}
(blue QR code link to presentation site).

\hypertarget{day-3-text}{%
\subsection{Day 3: Text}\label{day-3-text}}

Text modelling, a `tidytext' approach.

\begin{figure}

\begin{minipage}[t]{0.50\linewidth}

{\centering 

\hypertarget{session-1}{%
\subsubsection{Session 1}\label{session-1}}

\begin{itemize}
\tightlist
\item
  Data cleaning
\item
  Sentiment
\item
  Topic modelling
\end{itemize}

}

\end{minipage}%
%
\begin{minipage}[t]{0.50\linewidth}

{\centering 

\hypertarget{session-2}{%
\subsubsection{Session 2}\label{session-2}}

\begin{itemize}
\tightlist
\item
  Parts-of-speech tagging
\item
  Text regression
\item
  Use examples: Central bank minutes, reports
\end{itemize}

}

\end{minipage}%

\end{figure}

\hypertarget{state-space-models-time-series-models-with-structure}{%
\chapter{State-Space Models: time series models with
structure}\label{state-space-models-time-series-models-with-structure}}

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

What is (a) state space (model) and why do we use it? Here we

\begin{itemize}
\tightlist
\item
  explain the model form (and explain what state space is);
\item
  discuss the reasons for using them.
\end{itemize}

This is best illustrated by discussing a number of example models and
applications. We will also explore what this implies for estimation.

The standard textbook for this is Harvey (1989) (which understandably
feels a little dated both in terms of content and notation) as well as
J. D. Hamilton (1994), Durbin and Koopman (2001) and Kim and Nelson
(1999). Each of these have numerous classical and Bayesian applications
and extensions.

\hypertarget{a-useful-class-of-models}{%
\section{A useful class of models}\label{a-useful-class-of-models}}

State-space models are a useful framework amenable to both classical and
Bayesian estimation. Their set-up encompasses a number of common
economic models with interesting features such as time variation or
unobserved components. In either case this implies we need to estimate
unknown values of a time series.

Model form is quite general but has multiple interpretations and is
\emph{often not unique}. There is also quite a lot of additional
apparatus required before we can apply either maximum likelihood
estimation or Gibbs sampling.

\hypertarget{unobserved-variables-and-coefficients}{%
\subsection{Unobserved variables and
coefficients}\label{unobserved-variables-and-coefficients}}

A lot of \textbf{\emph{quantities}} that we routinely use to build
models and analyse policy are \textbf{\emph{unobservable}}: often
correspond to concepts with economic meaning rather than being
inherently measurable, e.g.:

\begin{itemize}
\tightlist
\item
  Business cycle or output gap, natural rate of interest, persistent
  exogenous shocks such as productivity or preferences
\item
  Other models have latent variables with no clear interpretation:
  popular procedure to reduce large data sets to dynamic factors
\end{itemize}

Alternatively we might be interested in models with
\textbf{\emph{time-varying parameters}}:

\begin{itemize}
\tightlist
\item
  Contrasting examples are models with slowly evolving parameters or
  with regime switches;
\item
  The coefficients of these models are both unobserved and (potentially)
  time-varying;
\end{itemize}

Econometricians face major problems estimating such models: the set up
is complicated and data requirements may be excessive. Fortunately there
is a method we can use to estimate where we are (`the state' of state
space) given available observable data.

The estimation method we usually use is the \textbf{\emph{Kalman
filter}}, see Kalman (1960). As an estimation process, this has the
useful spin-off that we can also use it to calculate the value of the
likelihood function at the same time.

\hypertarget{dynamic-economic-models}{%
\section{Dynamic economic models}\label{dynamic-economic-models}}

Consider a first order VAR (we'll show how this isn't restrictive) \[
     x_t = \mu + A x_{t-1} + v_t
\] with \(v_t\sim N(0,Q)\). Estimation of this model is easy -- actually
very easy as the unrestricted \(A\) matrix means we can use OLS (the
proof of this is left as an exercise\ldots). Instead we augment this
with a second set of equations to turn it into a system where we map the
data to the state \[
    \overbrace{y_t}^{Data} = \overbrace{x_t}^{State}
\] This reflects an important characteristic of state space models: all
data needs to be fed into the model through \textbf{\emph{observation
equations}}. This seems an odd way to go about estimating a VAR: but we
now generalize this to allow that for variables we \textbf{\emph{can't}}
see.

\hypertarget{general-state-space-model}{%
\subsection{General state space model}\label{general-state-space-model}}

State space models consist of two sets of equations: (1)
\textbf{\emph{Observation}} equations and (2) \textbf{\emph{State}} or
\textbf{\emph{Transition}} equations. The classical Kalman filter set up
might be:

\begin{itemize}
\tightlist
\item
  Observation equations \[
  \overbrace{y_t}^{Data} = \underbrace{H}_{Coefficients}  \overbrace{\beta_t}^{State} +\ e_t
  \]
\item
  State/Transition equations \[
   \beta_t = \mu + F\beta_{t-1} + v_t
  \]
\end{itemize}

We assume \(v_t\sim N(0,Q)\), \(e_t\sim N(0,R)\) and \(cov(e_t,v_t)=0\).
Set up like this, coefficients of \(H\) are akin to linear regression
parameters. Even more generally there could be further coefficients
i.e.~\(y_t = H \beta_t + B e_t\).

Now consider the following variation \[
    \begin{align*}
    y_t    &= \underbrace{H_t}_{Data} \beta_t + e_t \\
     \beta_t &= \mu + F \beta_{t-1} + v_t
    \end{align*}
\] This allows \(H_t\) to \emph{potentially} vary through time; written
like this we can think of \(H_t\) as data and \(\beta_t\) as the
regression parameters. For the time being consider \(F\) and \(\mu\) to
be time-invariant and the disturbances to have constant variances,
although we can easily generalize this.

Written one way the measurement equation is familiar from our
understanding of regression models; written another it looks like a
time-varying parameter model. The state equation is familiar from our
analysis of dynamic macromodels but here looks more like a potential
dynamic process for the regression coefficients to vary through time.
One big difference is the combination of the two. In particular, this
formalizes the role of `signal' -- \(v_t\) -- and `noise' -- \(e_t\).

Model allows for the possibility that we neither directly observe the
`driving' variables of the system nor do we measure them accurately. To
make sense of state space it is convenient to look at a variety of
example models.

\hypertarget{examples}{%
\section{Examples}\label{examples}}

\hypertarget{structural-time-series-models-stsms}{%
\subsection{Structural time series models
(STSMs)}\label{structural-time-series-models-stsms}}

Alternative time series decomposition associated with Harvey (1989).
Suggests instead that economic time series might be split into \[
    \begin{align}
    y_t &=\mu_t + e_t     &e_t\sim N\left(0,\sigma_v^2\right) \\
    \mu_t &=\mu_{t-1}+\lambda_{t-1}+\xi_t   &\xi_t\sim N\left(0,\sigma_\xi^2\right) \\
    \lambda_t &=\lambda_{t-1}+\zeta_t   &\zeta_t\sim N\left(0,\sigma_\zeta^2 \right)
    \end{align}
\] This is known as the \textbf{\emph{local linear trend}} model. Both
the trend level and slope can vary over time.

In state space this is: \[
    \begin{align}
    y_t &= \overset{H}{\begin{bmatrix} 1 & 0 \end{bmatrix}}
    \overset{\beta_t}{\begin{bmatrix}\mu_t \\ \lambda_t \end{bmatrix} } + e_{t} \\
    \overset{\beta_t}{\begin{bmatrix}\mu_t \\ \lambda_t \end{bmatrix}} &=
    \overset{F}{\begin{bmatrix} 1 & 1 \\  0 & 1 \end{bmatrix}}
    \overset{\beta_{t-1}}{\begin{bmatrix}\mu_{t-1} \\ \lambda_{t-1} \end{bmatrix}} + 
    \begin{bmatrix} \xi_t \\ \zeta_t\end{bmatrix} 
    \end{align}
\] The only parameters of this model that we need to estimate are the
variances.

\hypertarget{trend-cycle-model}{%
\subsection{Trend-cycle model}\label{trend-cycle-model}}

What if we wanted to decompose GDP into a trend and a cycle. Let
observed GDP be modeled as: \[
    y_t = \chi_t + \tau_t
    \] where the cycle \(\chi_t\) is an \(AR(2)\) process and the trend
\(\tau_t\) a random walk \[
    \begin{align}
    \chi_t &= c + \rho_1 \chi_{t-1} + \rho_2 \chi_{t-2} + v_{1t} \\
    \tau_t &= \tau_{t-1} + v_{2t}
    \end{align}
\] In state space form this is: \[
    \begin{align}
    y_{t}& =\overset{H}{\left[\begin{array}{ccc} 1 & 0 & 1 \end{array} \right] }
    \overset{\beta_t} {\left[ \begin{array}{c} \chi_t \\  \chi_{t-1} \\ \tau_t  \end{array} \right] } \\
    \overset{\beta_t} {\left[\begin{array}{c} \chi_t \\  \chi_{t-1} \\ \tau_t \end{array}\right] }
    & =\overset{\mu }{\left[\begin{array}{c} c \\  0 \\  0 \end{array} \right] } + 
        \overset{F} {\left[\begin{array}{ccc} \rho_1 & \rho_2 & 0 \\ 
    1 & 0 & 0 \\ 
    0 & 0 & 1
    \end{array}
    \right] }\overset{\beta_{t-1}}{\left[ 
    \begin{array}{c}
    \chi_{t-1} \\ 
    \chi_{t-2} \\
    \tau_{t-1}  
    \end{array}
    \right] }+\left[ 
    \begin{array}{c}
    v_{1t} \\ 
    0 \\
    v_{2t} 
    \end{array}
    \right]
    \end{align}
\] where \(R=0\) (there is no measurement error) and \(H\) is time
invariant

\hypertarget{time-varying-parameters-tvp}{%
\subsection{Time-varying parameters
(TVP)}\label{time-varying-parameters-tvp}}

Sometimes we wish to model structural change by incorporating
time-varying parameters. From the equations above we could set this up
as \begin{align*}
    y_t     &= H_t \beta_t + e_t \\
    \beta_t &= \beta_{t-1} + v_t
    \end{align*} The time-varying coefficients (\(\beta_t\)) multiply a
vector of time-varying regressors (\(H_t\)). As the data is fixed the
distribution of \(\beta_t\) will be conditionally normal if all the
errors are normal.

We set \(F=I\) so the parameters are assumed to follow a random walk and
the steady-state variance of \(\beta_t\) is infinite. The model becomes
time-invariant with \(\beta=\beta_0\) if \(Q = (\sigma_{\nu}^2=)\ 0\).
We now explicitly write the simplest TVP model \[
    y_t = c_t + X_t B_t + e_t
\] where \(c_t\) and \(B_t\) both follow a random walk. In state space
form \[
    \begin{align}
    y_t &= \overset{H_t} {\begin{bmatrix} 1 & X_t \end{bmatrix} } 
     \overset{\beta_t} {\begin{bmatrix} c_t \\ B_t \end{bmatrix} } + e_t & var(e) = R \\
     \overset{\beta_t} {\begin{bmatrix} c_t \\ B_t \end{bmatrix} } &= 
     \overset{\beta_{t-1}} {\begin{bmatrix} c_{t-1} \\  B_{t-1} \end{bmatrix}} 
     + \begin{bmatrix} v_{1t} \\ v_{2t} \end{bmatrix} & var(v) = Q 
    \end{align}
\] where \(F=I\) and \(\mu=0\). As \(\sigma_{\nu}^2 \neq 0\) the
coefficients aren't fixed even though \(F=I\).

\hypertarget{dynamic-factor-model}{%
\subsection{Dynamic factor model}\label{dynamic-factor-model}}

Assume a panel of series \(y_{it}\) has a common component \(f_t\) \[
\begin{align}
    y_{it} &= B_i f_t + e_{it}, \qquad &i=1,\ldots ,N \\
    f_t    &= c+\rho_1 f_{t-1} + \rho_2 f_{t-2} + v_t
\end{align}
\] In state space form \[
    \begin{align*}
    \left[ 
    \begin{array}{c}y_{1t} \\ y_{2t} \\ \vdots \\ y_{Nt}\end{array}\right] &= 
    \overset{H}{\left[ \begin{array}{cc}B_1 & 0 \\ B_2 & 0 \\ \vdots & \vdots \\ B_N & 0\end{array}\right] }
    \overset{\beta_t}{\left[ \begin{array}{c}f_t \\ f_{t-1} \end{array}\right] } + \left[ \begin{array}{c} e_{1t} \\ e_{2t} \\ \vdots \\ e_{Nt} \end{array}\right] \\
    \overset{\beta_t}{\left[ \begin{array}{c} f_t  \\ f_{t-1} \end{array}\right] } &=
    \overset{\mu }{\left[  \begin{array}{c}c \\ 0 \end{array} \right] }+
    \overset{F}{\left[ \begin{array}{cc}
    \rho_1 & \rho_2 \\  1 & 0 \end{array} \right] }
    \overset{\beta_{t-1}}{\left[  \begin{array}{c} f_{t-1} \\ f_{t-2} \end{array} \right] }+\left[ \begin{array}{c} v_t \\ 0\end{array}
    \right]
    \end{align*}
\] - The factors have dynamics but the observed variables do not

\hypertarget{var}{%
\subsection{VAR}\label{var}}

At a more abstract level, a \(k\)-variable, \(p\)-lag VAR \[
  X_t = c + \sum_{i=1}^p A_i X_{t-i} + \varepsilon_t,\quad\hbox{where}\ Y_t = X_t
\] This can always be written \[
    \begin{align*}
    Y_t &= 
    \overset{H}{\left[ I \ 0 \  ...\right] } 
    \overset{\beta_t}{\left[ \begin{array}{c}X_t \\ X_{t-1} \\ \vdots\end{array}\right] } \\
    \overset{\beta_t}{\left[ \begin{array}{c} X_t  \\ X_{t-1} \\ \vdots \end{array}\right] } &=
    \overset{\mu}{\left[  \begin{array}{c}c \\ 0 \\ \vdots\end{array} \right] }+
    \overset{F}{\left[ \begin{array}{ccc}
    A_1 & A_2 & \ldots \\  I & 0 & \ldots \\ \vdots & \vdots & \ddots \end{array} \right] }
    \overset{\beta_{t-1}}{\left[ \begin{array}{c}X_{t-1} \\ X_{t-2} \\ \vdots\end{array}\right] } + 
    \left[ \begin{array}{c} \varepsilon_t \\ 0 \\ \vdots\end{array}
    \right]
    \end{align*}
\] This is an important example: this model has entirely observed
variables. The size of the resulting state is \(p\times k\), potentially
quite large: however written this way the model is easy to simulate.

We can ask the question: can we find \textbf{\emph{unobserved}}
variables that preserve the properties of \(Y_t\)? What if: \[
     Y_t = X_t = P \tilde X_t
\] then \[
    \tilde X_t = P^{-1}A P \tilde X_{t-1} + P^{-1} \varepsilon_t = \tilde A \tilde X_{t-1} + \tilde \varepsilon_t
\] This shows that the state space representation is not unique.

\hypertarget{multiple-representations-arma11}{%
\subsection{Multiple representations:
ARMA(1,1)}\label{multiple-representations-arma11}}

This reflects the fact that model representations in general are not
unique. For example, two representations of \[
    y_t = \phi y_{t-1} + \varepsilon_t + \theta \varepsilon_{t-1}  
\] This can be written either as \[
    y_t = \left[ 1 \ 0\right]  \left[ \begin{array}{c}y_t \\ \varepsilon_t \end{array}\right], \qquad
    \left[ \begin{array}{c} y_t  \\ \varepsilon_t \end{array}\right]  =
    \left[ \begin{array}{ccc} \phi & \theta \\ 0 & 0 \end{array} \right] 
    \left[ \begin{array}{c}y_{t-1} \\ \varepsilon_{t-1} \end{array}\right]   + 
    \left[ \begin{array}{c} 1 \\ 1 \end{array} \right] \varepsilon_t 
\] or \[
    y_t = \begin{bmatrix} 1 & 0 \end{bmatrix} \begin{bmatrix} y_t \\ \theta\varepsilon_t \end{bmatrix}, \qquad
    \begin{bmatrix} y_t  \\ \theta\varepsilon_t \end{bmatrix} =
    \begin{bmatrix} \phi & 1 \\ 0 & 0 \end{bmatrix}
    \begin{bmatrix} y_{t-1} \\ \theta\varepsilon_{t-1} \end{bmatrix} + 
    \begin{bmatrix} 1 \\ \theta \end{bmatrix} \varepsilon_t
\]

\hypertarget{dsge-model}{%
\subsection{DSGE model}\label{dsge-model}}

Take a simple New Keynesian model \begin{align}
y_t    &= y_{t+1}^e-\frac{1}{\sigma} (i_t - \pi_{t+1}^e) + e_t^1 \\
\pi_t  &= \beta \pi_{t+1}^e + \kappa y_t + e_t^2 \\
i_t    &= \gamma i_{t-1} + (1-\gamma) \delta \pi_t + \varepsilon_t^3 \\ 
e_t^1  &= \rho_1 e_{t-1}^1 + \varepsilon_t^1 \\ 
e_t^2  &= \rho_2 e_{t-1}^2 + \varepsilon_t^2 
\end{align} The model comprises a dynamic IS curve, a Phillips Curve and
a policy rule with smoothing. There are three shocks, two of which are
persistent. This we need to write in the general algebraic linear
state-space form: \[
E\begin{bmatrix} z_t \\ x_{t+1}^e \end{bmatrix} = A \begin{bmatrix} z_{t-1} \\ x_t \end{bmatrix} + B \varepsilon_t  
\] We map our variables to their algebraic equivalent as (\(z_t\),
\(x_t\)) \(=\) ((\(e^1_t\), \(e^2_t\), \(i_t\)), (\(y_t\), \(\pi_t\))).
Then the model in state-space form but including the matrix \(E\) is \[
\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\ 
                0 & 1 & 0 & 0 & 0 \\ 
                0 & 0 & 1 & 0 & 0 \\ 
                1 & 0 & -\frac{1}{\sigma} & 1 & \frac{1}{\sigma} \\ 
                0 & 1 & 0 & 0 & \beta
\end{bmatrix}
\begin{bmatrix} e^1_t \\ e^2_t \\ i_t \\ y^e_{t+1} \\ \pi^e_{t+1} \end{bmatrix} 
   = 
   \begin{bmatrix} \rho_1 & 0 & 0 & 0 & 0 \\ 
                0 & \rho_2 & 0 & 0 & 0 \\ 
                0 & 0 & \gamma & 0 & (1-\gamma)\delta \\ 
                0 & 0 & 0 & 1 & 0 \\ 
                0 & 0 & 0 & -\kappa & 1
   \end{bmatrix}
\begin{bmatrix} e^1_{t-1} \\ e^2_{t-1} \\ i_{t-1} \\ y_t \\ \pi_t \end{bmatrix}    
   + 
      \begin{bmatrix} 
                1 & 0 & 0  \\ 
                0 & 1 & 0 \\ 
                0 & 0 & 1 \\ 
                0 & 0 & 0 \\ 
                0 & 0 & 0 
   \end{bmatrix}
   \begin{bmatrix} \varepsilon^1_t \\ \varepsilon^2_t \\ \varepsilon^3_t \end{bmatrix}    
\] As the left matrix is invertible we can write this in the form \[
\begin{bmatrix} z_t \\ x_{t+1}^e \end{bmatrix} = C \begin{bmatrix} z_{t-1} \\ x_t \end{bmatrix} + D \varepsilon_t  
\] where \(C = E^{-1}A\), \(D=E^{-1}B\). However, even in this form it
doesn't quite conform to the standard state space model. This is because
we retain the expectations in the model. Typically we need to solve the
model for the expectations using the (\textbf{BK1980?}) method or
similar such that \[
 x_t = -N z_{t-1} - G \varepsilon_t
\] Substituting this in we get a final state-space form \begin{align}
\begin{bmatrix} z_t \\ x_t \end{bmatrix} &= \begin{bmatrix} C_{11} - C_{12}N & 0 \\ -N & 0 \end{bmatrix} \begin{bmatrix} z_{t-1} \\ x_{t-1} \end{bmatrix} + \begin{bmatrix} D_1-C_{12}G \\ -G \end{bmatrix} \varepsilon_t \\
 &= P \begin{bmatrix} z_{t-1} \\ x_{t-1} \end{bmatrix} + Q \varepsilon_t
\end{align} This needs to be augmented with an observation equation,
perhaps \[
\begin{bmatrix} i_t \\ y_t \\ \pi_t \end{bmatrix} 
 = \begin{bmatrix} 0 & 0 & 1 & 0 & 0 \\ 
                0 & 0 & 0 & 1 & 0 \\ 
                0 & 0 & 0 & 0 & 1 
\end{bmatrix}
\begin{bmatrix} e^1_t \\ e^2_t \\ i_t \\ y_t \\ \pi_t \end{bmatrix} 
\] where we don't observe the autoregressive shocks.

\hypertarget{estimation-problem}{%
\section{Estimation problem}\label{estimation-problem}}

Recall the observation equation and transition equations \[
    \begin{align}
    y_t       &= H\beta_t + e_t,         &var(e_t)=R \\
    \beta_t   &= \mu + F\beta_{t-1}+v_t  &var(v_t)=Q
    \end{align}
\] where here we treat \(H\) as time invariant.

Notice that at least some parameters of the state space model (\(H\),
\(\mu\), \(F\), \(R\) and \(Q\)) and the state variables (\(\beta_t\))
are both unknown. \textbf{\emph{Simultaneous}} estimation of both sets
of unknowns is usually required. This is usually approached iteratively,
with the unknown state estimated conditional on some initial estimate of
the parameters (or in the Bayesian case \emph{sampled}) using an
appropriate method, usually the \emph{Kalman Filter} and then then the
parameters updated conditional on the state which is repeated until
convergence. This can be done using Gibb's Sampling if we can derive the
a conditional density for the unobserved components.

\hypertarget{the-regression-lemma}{%
\chapter{The regression lemma}\label{the-regression-lemma}}

\hypertarget{a-different-view-of-regression}{%
\section{A different view of
regression}\label{a-different-view-of-regression}}

Here we derive a simple characterization of predicting a vector of
unknown stochastic variables using observed data that is essentially
regression using population statistics rather than using a sample proxy.
Anderson (2004), James D. Hamilton (1994) and many multivariate
statistics textbooks contain similar treatments.

\hypertarget{unconditional-moments}{%
\subsection{Unconditional moments}\label{unconditional-moments}}

Let \(z\) be vector of stochastic variables such that \[
  z\sim N(\mu,\Sigma)
\] or \[
\mathbb{E}\left[ \left(z-\mathbb{E}[z]\right) \left(z-\mathbb{E}[z]\right)' \right] = \mathbb{E}\left[ \left(z-\mu\right) \left(z-\mu\right)'\right] = \Sigma
\] Divide the vector into two, \(z_1\) and \(z_2\) so that \[
\begin{bmatrix}
z_1 \\ 
z_2
\end{bmatrix}
\sim N\left( 
\begin{bmatrix}
\mu_1 \\ 
\mu_2
\end{bmatrix},
\begin{bmatrix}
\Sigma_{11} & \Sigma_{12} \\ 
\Sigma_{21} & \Sigma_{22}
\end{bmatrix}
\right)
\] The \emph{unconditional} covariance of \(z_1\) is unaffected by the
known covariance between it and \(z_2\), \(\Sigma_{12}\).

\hypertarget{conditional-mean-for-z_1}{%
\subsection{\texorpdfstring{Conditional mean for
\(z_1\)}{Conditional mean for z\_1}}\label{conditional-mean-for-z_1}}

How do we best use the information in an observed part of the vector
\(z\) -- say \(z_2\) -- to better predict the unobserved part?

Assume there is linear relationship between the two \(z\) variables such
that\\
\[
z_1 = Bz_2 + \varepsilon\qquad \text{or}\qquad \varepsilon =z_1-Bz_2
\] Now choose \(B\) such that \(\varepsilon\) and \(z_2\) are
uncorrelated -- a value of \(B\) where this holds is a fundamental
assumption for a regression model as it implies there is no useful
information in \(z_2\) that we aren't using. This implies\\
\[
\begin{aligned}
 0 &= \mathbb{E}\left[ \left(\varepsilon - \mathbb{E}[\varepsilon ]\right) \left(z_2 - \mathbb{E}[z_2]\right)'\right]\\
   &= E\left[ \left(z_1 - Bz_2 - \mathbb{E}[z_1-Bz_2]\right) \left(z_2 - \mathbb{E}[z_2]\right)'\right] \\
   &= E\left[ \left(z_1 - \mu_1 - B\left(z_2-\mu_2\right) \right) \left( z_2-\mu_2\right)'\right] \\
   & =\Sigma_{12}-B\Sigma_{22}
\end{aligned}   
\] We recover \(B=\Sigma_{12}\Sigma_{22}^{-1}\). This is simply
regression using a `method of moments' characterization where we use
population rather sample moments. For known conditioning values \(z_2\)
and known \(\Sigma_{12}\), \(\Sigma_{22}\) \[
\begin{aligned}
  \mathbb{E}\left[ z_1|z_2\right] &= \mathbb{E}\left[ Bz_2+\varepsilon |z_2\right] \\
                         &= Bz_2 + \mathbb{E}\left[\varepsilon \right] \\
                         &= Bz_2 + \mathbb{E}\left[z_1-Bz_2\right] \\
                         &= B z_2 + \mu_1 - B \mu_2 \\
                         &= \mu_1 + B(z_2-\mu_2) 
\end{aligned}
\] Plugging in the formula for \(B\) gives the conditional expectation
of \(z_1\) given \(z_2\) as \[
  \mathbb{E}[z_1 | z_2] = \mu_1 + \Sigma_{12} \Sigma_{22}^{-1}(z_2-\mu_2)
\] This is a formula that updates the unconditional expectation by a
weighted average of the ``innovation'' (deviation from predicted value)
of the observed variables.

\hypertarget{example}{%
\section{Example}\label{example}}

As an example, consider the \(4\times 4\) covariance matrix \(\Sigma\)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(kableExtra)}

\NormalTok{Sigma }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{rWishart}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DecValTok{4}\NormalTok{,}\FunctionTok{diag}\NormalTok{(}\DecValTok{4}\NormalTok{))),}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in matrix(c(rWishart(100, 4, diag(4))), 4, 4): data length differs from
size of matrix: [1600 != 4 x 4]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B     }\OtherTok{\textless{}{-}}\NormalTok{ Sigma[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{3}\SpecialCharTok{:}\DecValTok{4}\NormalTok{] }\SpecialCharTok{\%*\%} \FunctionTok{solve}\NormalTok{(Sigma[}\DecValTok{3}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}\DecValTok{3}\SpecialCharTok{:}\DecValTok{4}\NormalTok{])}

\NormalTok{mat\_print }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  \FunctionTok{paste}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{left[}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{begin\{array\}\{r\}"}\NormalTok{, }
          \FunctionTok{paste}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{t}\NormalTok{(x)),}
                \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"\&"}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(x)}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{), }\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}\textbackslash{}\textbackslash{}\textbackslash{}}\StringTok{\textquotesingle{}}\NormalTok{), }\FunctionTok{ncol}\NormalTok{(x)),}
                \AttributeTok{collapse=}\StringTok{""}\NormalTok{), }
          \StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{end\{array\}}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{right]"}\NormalTok{), }\AttributeTok{collapse=}\StringTok{""}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\[
 \Sigma = \left[\begin{array}{r}1.28 &1.74 &-0.25 &-0.52 \\1.74 &10.16 &-4.79 &3.12 \\-0.25 &-4.79 &4.44 &-1.86 \\-0.52 &3.12 &-1.86 &2.42 \\\end{array}\right]
\] It is a simple matter to generate four sample sequences with this
covariance. We set the constants \(\mu_i^j = 0\) for \(i,j = 1,2\) as we
subtract them out anyway.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{nobs        }\OtherTok{\textless{}{-}} \DecValTok{42}
\NormalTok{Z           }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(nobs}\SpecialCharTok{*}\DecValTok{4}\NormalTok{), nobs, }\DecValTok{4}\NormalTok{) }\SpecialCharTok{\%*\%} \FunctionTok{chol}\NormalTok{(Sigma)}
\FunctionTok{colnames}\NormalTok{(Z) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"z"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"11"}\NormalTok{,}\StringTok{"12"}\NormalTok{,}\StringTok{"21"}\NormalTok{,}\StringTok{"22"}\NormalTok{))}
\NormalTok{ZZ   }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(Z, }\AttributeTok{x =} \DecValTok{0}\SpecialCharTok{:}\NormalTok{(nobs}\DecValTok{{-}1}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \SpecialCharTok{{-}}\NormalTok{x, }\AttributeTok{names\_to =} \StringTok{"Var"}\NormalTok{,}\AttributeTok{values\_to =} \StringTok{"Val"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{F =} \FunctionTok{as\_factor}\NormalTok{(Var), }
         \AttributeTok{F =} \FunctionTok{recode\_factor}\NormalTok{(F, }
                           \AttributeTok{z11 =} \StringTok{"z[1]\^{}1"}\NormalTok{, }
                           \AttributeTok{z12 =} \StringTok{"z[1]\^{}2"}\NormalTok{, }
                           \AttributeTok{z21 =} \StringTok{"z[2]\^{}1"}\NormalTok{, }
                           \AttributeTok{z22 =} \StringTok{"z[2]\^{}2"}\NormalTok{))}
\NormalTok{ZZm   }\OtherTok{\textless{}{-}} \FunctionTok{select}\NormalTok{(ZZ, F) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unique}\NormalTok{(F) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{C =} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)))}

\FunctionTok{ggplot}\NormalTok{(ZZ) }\SpecialCharTok{+}   
  \FunctionTok{geom\_rect}\NormalTok{(}\AttributeTok{data=}\NormalTok{ZZm, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill=}\NormalTok{C), }
            \AttributeTok{xmin=}\SpecialCharTok{{-}}\ConstantTok{Inf}\NormalTok{, }\AttributeTok{xmax=}\ConstantTok{Inf}\NormalTok{, }
            \AttributeTok{ymin=}\SpecialCharTok{{-}}\ConstantTok{Inf}\NormalTok{, }\AttributeTok{ymax=}\ConstantTok{Inf}\NormalTok{, }\AttributeTok{alpha=}\FloatTok{0.15}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{y=}\NormalTok{Val, }\AttributeTok{color=}\NormalTok{F)) }\SpecialCharTok{+}
  \CommentTok{\# scale\_fill\_manual(values = c("red", "blue")) + }
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{F, }\AttributeTok{ncol=}\DecValTok{2}\NormalTok{, }\AttributeTok{labeller=}\StringTok{"label\_parsed"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{""}\NormalTok{,}\AttributeTok{y=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{RegressionLemma_files/figure-pdf/unnamed-chunk-2-1.pdf}

}

\caption{Four sample series with covariance matrix \(\Sigma\)}

\end{figure}

Denote the first two series \(z_1^1\) and \(z_1^2\), and the second two
\(z_2^1\) and \(z_2^2\); thus \(z_1\) from earlier is a vector of the
first two (\(z_2^1\) and \(z_2^2\)) and \(z_2\) a vector of the second
two (\(z_2^1\) and \(z_2^2\)).

\hypertarget{problem}{%
\subsubsection{Problem}\label{problem}}

If we know \(\Sigma\), and can observe these last two series (on light
blue backgrounds), how well can we reconstruct the first (light red
backgrounds) two ?

\hypertarget{solution}{%
\subsection{Solution}\label{solution}}

Calculate \(B\) from the above formula. By inspection \[
 \Sigma_{22} = \left[\begin{array}{r}4.44 &-1.86 \\-1.86 &2.42 \\\end{array}\right]
\] and \[
 \Sigma_{12} = \left[\begin{array}{r}-0.25 &-0.52 \\-4.79 &3.12 \\\end{array}\right]
\] so \(B = \Sigma_{12}\Sigma_{22}^{-1}\) is \[
 B = \left[\begin{array}{r}-0.214 &-0.377 \\-0.795 &0.679 \\\end{array}\right]
\] Calculating the predicted variables and plotting them on the same
graph we get

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ZZfit }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(Z[,}\DecValTok{3}\SpecialCharTok{:}\DecValTok{4}\NormalTok{] }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(B), }\AttributeTok{x =} \DecValTok{0}\SpecialCharTok{:}\NormalTok{(nobs}\DecValTok{{-}1}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename\_all}\NormalTok{(}\SpecialCharTok{\textasciitilde{}} \FunctionTok{c}\NormalTok{(}\StringTok{"z11"}\NormalTok{, }\StringTok{"z12"}\NormalTok{, }\StringTok{"x"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{gather}\NormalTok{(Var, Fit, }\SpecialCharTok{{-}}\NormalTok{x) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{F =} \FunctionTok{as\_factor}\NormalTok{(Var), }
         \AttributeTok{F =} \FunctionTok{recode\_factor}\NormalTok{(F, }\AttributeTok{z11=}\StringTok{"z[1]\^{}1"}\NormalTok{, }\AttributeTok{z12=}\StringTok{"z[1]\^{}2"}\NormalTok{))}

\NormalTok{ZZhat }\OtherTok{\textless{}{-}} \FunctionTok{left\_join}\NormalTok{(ZZ, ZZfit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining with `by = join_by(x, Var, F)`
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cors }\OtherTok{\textless{}{-}}\NormalTok{ ZZhat }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(Fit)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Var) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{CC =} \FunctionTok{cor}\NormalTok{(Val, Fit)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{Cor =} \FunctionTok{mean}\NormalTok{(CC))}

\FunctionTok{ggplot}\NormalTok{(ZZhat) }\SpecialCharTok{+} 
  \FunctionTok{geom\_rect}\NormalTok{(}\AttributeTok{data=}\NormalTok{ZZm, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill=}\NormalTok{C), }
            \AttributeTok{xmin=}\SpecialCharTok{{-}}\ConstantTok{Inf}\NormalTok{, }\AttributeTok{xmax=}\ConstantTok{Inf}\NormalTok{, }
            \AttributeTok{ymin=}\SpecialCharTok{{-}}\ConstantTok{Inf}\NormalTok{, }\AttributeTok{ymax=}\ConstantTok{Inf}\NormalTok{, }\AttributeTok{alpha=}\FloatTok{0.15}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{y=}\NormalTok{Val, }\AttributeTok{color=}\NormalTok{F)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{y=}\NormalTok{Fit, }\AttributeTok{group=}\NormalTok{F), }\AttributeTok{linetype=}\StringTok{"dotted"}\NormalTok{, }\AttributeTok{color=}\StringTok{"black"}\NormalTok{, }\AttributeTok{size=}\FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{F, }\AttributeTok{ncol=}\DecValTok{2}\NormalTok{, }\AttributeTok{labeller=}\StringTok{"label\_parsed"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{""}\NormalTok{,}\AttributeTok{y=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
i Please use `linewidth` instead.
\end{verbatim}

\begin{verbatim}
Warning: Removed 84 rows containing missing values (`geom_line()`).
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{RegressionLemma_files/figure-pdf/unnamed-chunk-3-1.pdf}

}

\caption{Plots of actual and predicted first and second series}

\end{figure}

where the correlations between the variables and their fitted values are
0.59, 0.76 respectively.

\hypertarget{observing-different-z-variables}{%
\subsection{\texorpdfstring{Observing different \(z\)
variables}{Observing different z variables}}\label{observing-different-z-variables}}

What if instead of observing both of the last \(z\) variables we only
observe one, say the last one? Nothing we have said so far restricts us
from having only one observable, or three -- and of course ordering is
arbitrary.

Lets assume we just see the last variable. Calculate \(B\) from the
above formula. As before, by inspection \[
 \Sigma_{22} = \left[\begin{array}{r}2.42 \\\end{array}\right]
\] and \[
 \Sigma_{12} = \left[\begin{array}{r}-0.52 &3.12 &-1.86 \\\end{array}\right]'
\] and as now \(\Sigma_{22}\) is a scalar we can write
\(B = \Sigma_{12}/\Sigma_{22}\) which is

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B }\OtherTok{\textless{}{-}}\NormalTok{ Sigma[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{, drop}\OtherTok{=}\ConstantTok{FALSE}\NormalTok{]}\SpecialCharTok{/}\NormalTok{Sigma[}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\[
 B = \left[\begin{array}{r}-0.213 &1.289 &-0.766 \\\end{array}\right]'
\] Applying this we get predictions of the three unknown variables which
are

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{colnames}\NormalTok{(Z) }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"z"}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\StringTok{"11"}\NormalTok{,}\StringTok{"12"}\NormalTok{,}\StringTok{"13"}\NormalTok{,}\StringTok{"21"}\NormalTok{))}
\NormalTok{ZZ   }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(Z, }\AttributeTok{x =} \DecValTok{0}\SpecialCharTok{:}\NormalTok{(nobs}\DecValTok{{-}1}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \SpecialCharTok{{-}}\NormalTok{x, }\AttributeTok{names\_to =} \StringTok{"Var"}\NormalTok{,}\AttributeTok{values\_to =} \StringTok{"Val"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{F =} \FunctionTok{as\_factor}\NormalTok{(Var), }
         \AttributeTok{F =} \FunctionTok{recode\_factor}\NormalTok{(F, }
                           \AttributeTok{z11 =} \StringTok{"z[1]\^{}1"}\NormalTok{, }
                           \AttributeTok{z12 =} \StringTok{"z[1]\^{}2"}\NormalTok{, }
                           \AttributeTok{z13 =} \StringTok{"z[1]\^{}3"}\NormalTok{, }
                           \AttributeTok{z21 =} \StringTok{"z[2]\^{}1"}\NormalTok{))}
\NormalTok{ZZm   }\OtherTok{\textless{}{-}} \FunctionTok{select}\NormalTok{(ZZ, F) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unique}\NormalTok{(F) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{C =} \FunctionTok{as.factor}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{)))}

\NormalTok{ZZfit }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(Z[,}\DecValTok{4}\NormalTok{] }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(B), }\AttributeTok{x =} \DecValTok{0}\SpecialCharTok{:}\NormalTok{(nobs}\DecValTok{{-}1}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename\_all}\NormalTok{(}\SpecialCharTok{\textasciitilde{}} \FunctionTok{c}\NormalTok{(}\StringTok{"z11"}\NormalTok{, }\StringTok{"z12"}\NormalTok{, }\StringTok{"z13"}\NormalTok{, }\StringTok{"x"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{gather}\NormalTok{(Var, Fit, }\SpecialCharTok{{-}}\NormalTok{x) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{F =} \FunctionTok{as\_factor}\NormalTok{(Var), }
         \AttributeTok{F =} \FunctionTok{recode\_factor}\NormalTok{(F, }\AttributeTok{z11=}\StringTok{"z[1]\^{}1"}\NormalTok{, }\AttributeTok{z12=}\StringTok{"z[1]\^{}2"}\NormalTok{, }\AttributeTok{z13=}\StringTok{"z[1]\^{}3"}\NormalTok{))}

\NormalTok{ZZhat }\OtherTok{\textless{}{-}} \FunctionTok{left\_join}\NormalTok{(ZZ, ZZfit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining with `by = join_by(x, Var, F)`
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cors }\OtherTok{\textless{}{-}}\NormalTok{ ZZhat }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(Fit)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Var) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{CC =} \FunctionTok{cor}\NormalTok{(Val, Fit), }\AttributeTok{r2 =} \FunctionTok{summary}\NormalTok{(}\FunctionTok{lm}\NormalTok{(Fit }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Val))}\SpecialCharTok{$}\NormalTok{r.squared) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{Cor =} \FunctionTok{mean}\NormalTok{(CC), }\AttributeTok{R2 =} \FunctionTok{mean}\NormalTok{(r2))}

\FunctionTok{ggplot}\NormalTok{(ZZhat) }\SpecialCharTok{+} 
  \FunctionTok{geom\_rect}\NormalTok{(}\AttributeTok{data=}\NormalTok{ZZm, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill=}\NormalTok{C), }
            \AttributeTok{xmin=}\SpecialCharTok{{-}}\ConstantTok{Inf}\NormalTok{, }\AttributeTok{xmax=}\ConstantTok{Inf}\NormalTok{, }
            \AttributeTok{ymin=}\SpecialCharTok{{-}}\ConstantTok{Inf}\NormalTok{, }\AttributeTok{ymax=}\ConstantTok{Inf}\NormalTok{, }\AttributeTok{alpha=}\FloatTok{0.15}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{y=}\NormalTok{Val, }\AttributeTok{color=}\NormalTok{F)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{y=}\NormalTok{Fit, }\AttributeTok{group=}\NormalTok{F), }\AttributeTok{linetype=}\StringTok{"dotted"}\NormalTok{, }\AttributeTok{color=}\StringTok{"black"}\NormalTok{, }\AttributeTok{size=}\FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ F, }\AttributeTok{ncol=}\DecValTok{2}\NormalTok{, }\AttributeTok{labeller=}\StringTok{"label\_parsed"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{""}\NormalTok{,}\AttributeTok{y=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: Removed 42 rows containing missing values (`geom_line()`).
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{RegressionLemma_files/figure-pdf/unnamed-chunk-5-1.pdf}

}

\caption{Plots of actual and predicted series, one predictor}

\end{figure}

where we have color coded the backgrounds accordingly. In general,
predictions are usually a bit worse, but sometimes almost as good -- and
may even look a bit better! Comparing them to our previous results the
correlations between the variables and their predicted values are now
0.34, 0.51, 0.43 respectively, where of course we are predicting one
more series now.

\hypertarget{remarks}{%
\subsection{Remarks}\label{remarks}}

Depending on the covariance matrix, correlation can be a lot lower. If
the \(z_1\) and \(z_2\) vectors are uncorrelated then
\(\Sigma_{12}\approx 0\) so \(B\approx 0\).In these circumstances the
best prediction is then close to the unconditional mean; if
\(\Sigma_{11}>0\) then predictions will be poor although the absolute
value of those errors depends on \(\Sigma_{11}\).

This also illustrates that there are some variables that are better at
predicting the things you are interested in than others. It practice it
is frequently the case that there is low correlation between predicted
and actual -- as ever, you need good regressors to get a good fit
between predicted and actual.

\hypertarget{updating-the-update-and-assessing-uncertainty}{%
\section{Updating the update and assessing
uncertainty}\label{updating-the-update-and-assessing-uncertainty}}

\hypertarget{updating-an-estimate-with-new-information}{%
\subsection{Updating an estimate with new
information}\label{updating-an-estimate-with-new-information}}

This result easily generalizes: what if we already have an estimate but
we get some new information that we could use to improve our estimate?
We can solve this using the formula we have already derived. In effect
we can divide \(z_2\) and include some further variables denoted \(v\)
such that \[
\begin{bmatrix}
z_1 \\ 
z_2 \\ 
v
\end{bmatrix}
\sim N\left( 
\begin{bmatrix}
\mu_1 \\ 
\mu_2 \\ 
0
\end{bmatrix},
\begin{bmatrix}
\Sigma_{11} &\Sigma_{12} & \Sigma_{1v} \\ 
\Sigma_{21} &\Sigma_{22} & 0 \\ 
\Sigma_{v1} & 0 & \Sigma_{vv}
\end{bmatrix}
\right)
\] Notice we have imposed that \(v\) is mean zero (\(\mu_v = 0\)) and
uncorrelated with \(z_2\) (\(\Sigma_{v2} = \Sigma_{2v}'=0\)). It is
important that they are uncorrelated with previous conditioning
variables \textbf{\emph{but not the variables to be predicted}} so
\(\Sigma_{1v}\ne 0\). In this way \(v\) represents \textbf{\emph{news}}.
We couldn't have predicted it from information we already had.

Conditioning \(z_1\) on \((z_2,v)\) we get \[
\begin{aligned}
\mathbb{E}[z_1|z_2,v] &= \mu_1 + 
 \begin{bmatrix} \Sigma_{12} & \Sigma_{1v} \end{bmatrix}
 \begin{bmatrix} \Sigma_{22}^{-1} & 0 \\ 0 & \Sigma_{vv}^{-1} \end{bmatrix}
 \begin{bmatrix} z_2-\mu_2 \\  v  \end{bmatrix} \\
             &= \mu_1 +\Sigma_{12}\Sigma_{22}^{-1}(z_2-\mu_2)+\Sigma_{1v}\Sigma_{vv}^{-1}v
\end{aligned}
\] We can further write this \[
 \mathbb{E}[z_1|z_2, v] = \mathbb{E}[z_1|z_2] +\Sigma_{1v} \Sigma_{vv}^{-1}v
\]

\hypertarget{covariances}{%
\subsection{Covariances}\label{covariances}}

So far concentrated on the expected value, but can similarly construct
the conditional covariance. The conditional variance for the \(z_1\)
given \(z_2\) is \[
\begin{align}
var(z_1|z_2) &= \mathbb{E}\left[\left(z_1 - \mathbb{E}(z_1|z_2) \right) \left(z_1 - \mathbb{E}(z_1|z_2) \right)' \right] \\
&= \mathbb{E}\left[ \left(z_1 -\mu_1-\Sigma_{12}\Sigma_{22}^{-1}(z_2-\mu_2) \right) \left(z_1-\mu_1 -\Sigma_{12}\Sigma_{22}^{-1} (z_2-\mu_2) \right)' \right] \\
&= \Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} + \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{22}\Sigma_{22}^{-1}\Sigma_{21} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} \\
&= \Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}
\end{align}
\] so \[
  var(z_1|z_2) = \Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}
\] This tells us that the conditional covariance can be no bigger than
the unconditional one. We get an efficiency gain.

When we consider updating the covariance of a conditional expectation it
follows that \[
\begin{align}
var(z_1|z_2,v) &= \Sigma_{11}-
   \begin{bmatrix} \Sigma_{12} & \Sigma_{1v} \end{bmatrix} 
   \begin{bmatrix} \Sigma_{22}^{-1} & 0 \\  0 & \Sigma_{vv}^{-1} \end{bmatrix}
   \begin{bmatrix} \Sigma_{21} \\ \Sigma_{v1} \end{bmatrix} \\
  &= \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} - \Sigma_{1v}\Sigma_{vv}^{-1}\Sigma_{v1} \\
  &= var(z_1|z_2) - \Sigma_{1v}\Sigma_{vv}^{-1}\Sigma_{v1}
\end{align}
\] It follows that the covariance here must be no bigger than the
variance conditioned on \(z_2\) alone, consistent with our previous
result.

\hypertarget{kalman-filtering}{%
\chapter{Kalman filtering}\label{kalman-filtering}}

Estimating unobserved components

\hfill\break

\hypertarget{literature}{%
\section{Literature}\label{literature}}

Alternatives to what follows can be found in Harvey (1989), J. D.
Hamilton (1994), Kim and Nelson (1999), Durbin and Koopman (2001) or
Triantafyllopoulos (2021). There are many books devoted to the Kalman
filter
\href{https://www.amazon.co.uk/s?k=kalman+filtering\&i=stripbooks}{as a
casual Amazon search} mostly from an engineering perspective. However
econometricians since Harvey and Pierse (1984) have used it in a way
somewhat different from standard engineering applications. We will cover
the filter and then look at a simple example if filtering, then develop
a maximum likelihood estimation approach.

\hypertarget{reminder-of-a-state-space-model}{%
\subsection{Reminder of a state-space
model}\label{reminder-of-a-state-space-model}}

Consider the trend-cycle model \begin{equation}
  y_t = \chi_t + \tau_t + \varepsilon_t
\end{equation} where the cycle equation is \begin{equation}
  \chi_t = c+\rho_1 \chi_{t-1} + \rho_2 \chi_{t-2}+v_{1t}
\end{equation} and the trend equation is \begin{equation}
  \tau_t = \tau_{t-1} + v_{2t}
\end{equation}

In state space this can be written \begin{align}
y_t &=
\begin{bmatrix} 1 & 0 & 1 \end{bmatrix}
 \begin{bmatrix} \chi_t \\ \chi_{t-1} \\ \tau_t \end{bmatrix} + [1] \varepsilon_t\\
 \begin{bmatrix} \chi_t \\ \chi_{t-1} \\ \tau_t \end{bmatrix} &=
\begin{bmatrix} c \\ 0 \\ 0 \end{bmatrix} +
 \begin{bmatrix} \rho_1 & \rho_2 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix}
\begin{bmatrix} \chi_{t-1} \\ \chi_{t-2} \\ \tau_{t-1} \end{bmatrix} +
 \begin{bmatrix} 1 & 0 \\ 0 & 0 \\ 0 & 1 \end{bmatrix}
\begin{bmatrix} v_{1t} \\ v_{2t} \end{bmatrix}
\end{align}

\hypertarget{a-useful-class-of-models-1}{%
\section{A useful class of models}\label{a-useful-class-of-models-1}}

We need a framework that nests this type of model (and many more).
State-space models are one such framework, amenable to classical (and
Bayesian) estimation. Quite a lot of apparatus required before we can
apply maximum likelihood.

\textbf{Key points}:

\begin{itemize}
\tightlist
\item
  Many quantities routinely used to build models and analyse policy are
  \emph{unobservable}.
\item
  Econometricians face a major problem estimating such models:
  everything unobserved needs estimating simultaneously (states and
  parameters).
\item
  Fortunately there is a method we can use: the \textbf{\emph{Kalman
  filter}} (Kalman (1960)).
\item
  Has the useful spin-off that we can also use it to calculate the value
  of the likelihood function.
\end{itemize}

\hypertarget{what-is-a-filter}{%
\section{What is a filter?}\label{what-is-a-filter}}

Imagine we had a time-varying parameter model where we estimate
\(\beta_t\); this becomes a time series so we have a lot of parameters
to estimate: we outline an estimation method here, which we will then
characterise as the \emph{Kalman Filter}.

Simple observation and transition equations \begin{align}
y_t     &= H_t\beta_t + e_t, &var(e_t) = R \\
\beta_t &= \mu + F \beta_{t-1} + v_t, &var(v_t)=Q
\end{align} where \(\beta_t\) is a vector of stochastic variables,
\(y_t\) a vector of measurements and the data forms an information set
such that \(\psi_T = \{y_T,y_{T-1},...,y_1\}\).

Note that written this way the shocks are potentially reduced form ones.
Typically there will be less shocks then either states (or observables)
or it may be that structural shocks enter into one or more
relationships. We would then write \(e_t = B\eta_t\) and
\(v_t = G\nu_t\) where \(B\) and \(G\) are matrices of appropriate
dimension that feed the structural shocks into the right equations. If
we assume both \(eta_t\) and \(\nu_t\) only contains independent unit
variance shocks, then \(R = B\mathbb{E}(\eta_t\eta_t')B' = BB'\) and
similarly \(Q=GG'\).

The reason for using the reduced form shocks is toreduce the amount of
notation. We will also (to further reduce notation) mostly set
\(\mu = 0\), but it is straightforward to have a vector of constants,
which we include in the final formulae.

\hypertarget{three-estimation-problems}{%
\subsection{Three estimation problems}\label{three-estimation-problems}}

Jazwinski (1970) defines three type of estimation problem

\begin{itemize}
\tightlist
\item
  \textbf{Smoothing} is the problem of estimating \(\beta_k\) for any
  \(k<T\)
\item
  \textbf{Filtering} is the problem of estimating \(\beta_k\) for
  \(k=T\)
\item
  \textbf{Prediction} is the problem of estimating \(\beta_k\) for any
  \(k>T\)
\end{itemize}

\begin{quote}
``The object of filtering is to update our knowledge of the system each
time a new observation \(y_t\) is brought in.'' (Durbin and Koopman
(2001))
\end{quote}

\begin{tcolorbox}[enhanced jigsaw, breakable, left=2mm, arc=.35mm, toptitle=1mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, bottomrule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Notation}, colframe=quarto-callout-note-color-frame, coltitle=black, titlerule=0mm, toprule=.15mm, bottomtitle=1mm, rightrule=.15mm, colback=white]

If we understand the notation we are a long way to understanding the
solution. \[
\begin{align}
\beta_{t|t-1} &= \mathbb{E}[\beta_t | \psi_{t-1}] && \hbox{Estimate of } \beta_t \hbox{ conditional on } \psi_{t-1} \hbox{ (predicted)} \\ 
\beta_{t|t}   &= \mathbb{E}[\beta_t | \psi_t]     && \hbox{Current } (\psi_t) \hbox{ sample estimate} \hbox{ (filtered)}\\ 
\beta_{t|T}   &= \mathbb{E}[\beta_t | \psi_T]     && \hbox{Full } (\psi_T) \hbox{ sample estimate} \hbox{ (smoothed)}\\ 
P_{t|j}       &= \mathbb{E}\left[(\beta_t - \beta_{t|j}) (\beta_t - \beta _{t|j})'\right]  && \hbox{Conditional cov. based on } \psi_j,\ j=[t-1,t,T]\\ 
y_{t|t-1}     &= \mathbb{E}[y_t | \psi_{t-1}] = H_t \beta_{t|t-1}  && \hbox{Prediction of}\ y_t \hbox{ given } \psi_{t-1}\\ 
\eta_{t|t-1}  &= y_t - y_{t|t-1} = y_t - H_t \beta_{t|t-1}  && \hbox{Prediction error given }\psi_{t-1} \\ 
f_{t|t-1}     &= \mathbb{E}[\eta_{t|t-1}\eta_{t|t-1}']      && \hbox{Conditional } (\psi_{t-1}) \hbox{ variance of prediction error}
\end{align}
\]

\end{tcolorbox}

In what follows we will calculate some quantities that are consequences
of the state space formulation of our problem that will allow us to
apply the regression lemma to estimate the unobserved state variables.

\hypertarget{forecasting}{%
\subsection{Forecasting}\label{forecasting}}

Consider the simple first-order VAR model \begin{equation}
  \beta_t = F\beta_{t-1}+v_{t},\quad v_t\sim N(0,Q)
\end{equation} We can use to make the \emph{conditional} forecast
\begin{equation}
  \beta_{t|t-1} = F\beta_{t-1}
\end{equation} where \(\beta_{t|t-1}= \mathbb{E}[\beta_t|\psi_{t-1}]\)
and \(\psi_{t-1}\) is the information set available at time \(t-1\).

If \(\psi_{t-1}\) includes \(\beta_{t-1}\) we can straightforwardly
forecast next period (and the next-but-one period etc) using the model.
This is a standard forecasting exercise given any estimated (or even
calibrated) economic model.

\hypertarget{uncertainty}{%
\subsection{Uncertainty}\label{uncertainty}}

How can we assess the associated forecast uncertainty? The forecast
covariance \(P_{t|t-1} = var(\beta_t|\psi_{t-1})\) is given by
\begin{align*}
P_{t|t-1} &= \mathbb{E}\left[ (\beta_t - \beta_{t|t-1})(\beta_t-\beta_{t|t-1})'\right]   \\
  &= \mathbb{E}\left[ (F\beta_{t-1}+v_t-F\beta_{t-1|t-1}) \left(\beta_{t-1}'F'+v_t'-\beta_{t-1|t-1}'F'\right) \right] \\
&= \mathbb{E}\left[ F\left(\beta _{t-1}-\beta _{t-1|t-1}\right) \left( \beta_{t-1}-\beta _{t-1|t-1}\right) ' F' \right] + \mathbb{E}[v_t v_t'] \\
&= F\mathbb{E}\left[\left(\beta _{t-1}-\beta _{t-1|t-1}\right) \left( \beta_{t-1}-\beta _{t-1|t-1} \right)' \right] F' + \mathbb{E}[v_t v_t'] \\
&= FP_{t-1|t-1}F' + Q
\end{align*} where \(P_{t-1|t-1} = var(\beta_{t-1}|\psi_{t-1})\). The
forecast error variance depends on the previous error variance; that
value depends on the information set.

If \(\beta_{t-1}\) forms part of the information set \(\psi_{t-1}\) then
\begin{equation}
 P_{t-1|t-1} = var \left(\beta_{t-1}|\psi_{t-1}\right) = 0
\end{equation} and there is no uncertainty other than from the
disturbance terms and \(P_{t|t-1}=P_t=Q\).

If \(\beta_{t-1}\) does not form part of the information set
\(\psi_{t-1}\) but \(\beta _{t-2}\) does then \(P_{t-1|t-1}=Q\) and
\(P_{t|t-1}=FQF'+Q\). This can be continued backwards; the unconditional
(steady-state) covariance of \(\beta_t\) is the limit \(P=FPF'+Q\). We
can easily calculate error bands for \(\beta_t\) using the appropriate
information set.

\hypertarget{prediction-error}{%
\subsection{Prediction error}\label{prediction-error}}

We can turn this around, as it must be the \emph{prediction errors} are
given by \begin{align}
\eta_{t|t-1} &= y_t - \mathbb{E}[y_t|\psi_{t-1}]  \\
             &= y_t - \mathbb{E}[H_t\beta_t+e_t|\psi_{t-1}] \\
             &= y_t-H_t\beta_{t|t-1}
\end{align} where \(\eta_{t|t-1}\) is uncorrelated with \(\psi_{t-1}\).
So the `news' over that contained in \(y_t\) above \(\psi_{t-1}\) is
captured by \(\eta_{t|t-1}\). It will be that
\(\eta_{t|t-1}\sim N(0,\Sigma_{\eta\eta})\); we need to find an
expression for the covariance.

\hypertarget{current-data-predictions}{%
\subsection{Current-data predictions}\label{current-data-predictions}}

Now we find \(\mathbb{E}[\beta_t | \psi_t]\) -- the best prediction of
the unknown coefficient vector given \emph{current} information. Using
the regression lemma we know that \begin{align}
\mathbb{E}[\beta_t | \psi_t ] &= \mathbb{E}[\beta_t | \psi_{t-1}, \eta_{t|t-1} ]  \\
&= \mathbb{E}[\beta_t | \psi_{t-1}] +\Sigma_{\beta\eta} \Sigma_{\eta\eta}^{-1}\eta_{t|t-1}  \\
&= \beta_{t|t-1} + \Sigma_{\beta\eta}\Sigma_{\eta\eta}^{-1} \eta_{t|t-1}
\end{align} because \(\psi_{t-1}\) and \(\eta_{t|t-1}\) are uncorrelated
and \(\eta_{t|t-1}\) is mean zero.

Similarly, we can find \(P_{t|t}\) as the best prediction of the
variance of \(\beta_t\) given \(\psi_t\). Using the regression lemma we
know that \begin{align}
P_{t|t} &= \mathbb{E}[(\beta_t-\beta_{t|t})(\beta_t - \beta_{t|t})'|\psi_{t-1}, \eta_{t|t-1}] \\
        &= \mathbb{E}[(\beta_t - \beta_{t|t})(\beta_t-\beta_{t|t})'|\psi_{t-1}] - \Sigma_{\beta\eta}\Sigma_{\eta\eta}^{-1}\Sigma_{\eta\beta} \\
        &=P_{t|t-1}-\Sigma_{\beta\eta} \Sigma_{\eta\eta}^{-1} \Sigma_{\eta\beta} 
\end{align}

\hypertarget{estimated-model-covariances}{%
\subsection{Estimated model
covariances}\label{estimated-model-covariances}}

All we need do is plug the relevant expressions into the regression
lemma. So, what is \(\Sigma_{\beta\eta}\)? \begin{align}
\Sigma_{\beta\eta} &= \mathbb{E}[(\beta_t-\beta_{t|t-1}) \eta_{t|t-1}'] \\
&= \mathbb{E}\left[(\beta_t-\beta_{t|t-1}) (y_t-H_t\beta_{t|t-1})'\right] \\
&= \mathbb{E}\left[(\beta_t-\beta_{t|t-1}) (H_t\beta_t+e_t-H_t\beta_{t|t-1})'\right] \\
&= \mathbb{E}[(\beta_t-\beta_{t|t-1}) (\beta_t-\beta_{t|t-1})'H_t'] + \mathbb{E}[(\beta_t - \beta_{t|t-1}) e_t'] \\
&= P_{t|t-1} H_t' \tag{$\Sigma_{\beta\eta}$} 
\end{align} as \(\mathbb{E}[(\beta_t - \beta_{t|t-1})e_t']=0\).

What is \(\Sigma_{\eta\eta}\)? \begin{align}
\Sigma_{\eta\eta} &= \mathbb{E}[(y_t-H_t \beta_{t|t-1})(y_t-H_t\beta_{t|t-1})'] \\
                  &= \mathbb{E}[(H_t\beta_t+e_t-H_t\beta_{t|t-1})(H_t\beta_t+e_t-H_t\beta_{t|t-1})'] \\
                  &= \mathbb{E}[(H_t\beta_t-H_t \beta_{t|t-1})(H_t\beta_t-H_t\beta_{t|t-1})'] + \mathbb{E}[e_t e_t'] \\
                  &= \mathbb{E}[H_t(\beta_t-\beta_{t|t-1})(\beta_t-\beta_{t|t-1})'H_t'] + R \\
                  &= H_t P_{t|t-1} H_t' + R \\
                  &= f_{t|t-1} \tag{$\Sigma_{\eta\eta}$} \label{svv}
\end{align} where we define
\(f_{t|t-1}=\mathbb{E}[\eta_{t|t-1}\eta_{t|t-1}']\). Now we're ready.

\hypertarget{the-kalman-filter}{%
\section{The Kalman filter}\label{the-kalman-filter}}

The equations of the filter are

\begin{itemize}
\tightlist
\item
  the conditional expectation depending on \(\psi_{t-1}\);
\item
  an update that uses \(\eta_{t|t-1}\) to obtain the best \(t\)-period
  prediction now based on \(\psi_t\).
\end{itemize}

These must be of the form \begin{align}
\mathbb{E}[\beta_t | \psi_{t-1}] &= \mu + F \mathbb{E} [\beta_t | \psi_{t-1}] \\
\mathbb{E}[P_t|\psi_{t-1}]       &= F \mathbb{E}[P_{t-1}|\psi_{t-1}] F' + Q \\
\mathbb{E}[\beta_t | \psi_t]     &= \mathbb{E}[\beta_t|\psi_{t-1}] + \Sigma_{\beta\eta} \Sigma_{\eta\eta}^{-1}\eta_{t|t-1} \\
\mathbb{E}[P_t|\psi_t]           &= \mathbb{E}[P_t|\psi_{t-1}] -\Sigma_{\beta\eta}\Sigma_{\eta\eta}^{-1}\Sigma_{\eta \beta}
\end{align} These are specifically \begin{align}
\beta_{t|t-1} &= \mu +F\beta_{t-1|t-1}                                  &&\hbox{Predicted } \beta \\
P_{t|t-1}     &= F P_{t-1|t-1}F' + Q                                    &&\hbox{Predicted } P \\
\eta_{t|t-1}  &= y_t-H_t\beta_{t|t-1}                                   &&\hbox{Prediction error} \\
f_{t|t-1}     &= H_t P_{t|t-1}H_t' + R                                  &&\hbox{Pred. error variance} \\
\beta_{t|t}   &= \beta_{t|t-1}+P_{t|t-1}H_t' f_{t|t-1}^{-1}\eta_{t|t-1} &&\hbox{Updated } \beta \\
P_{t|t}       &= P_{t|t-1} - P_{t|t-1}H_t' f_{t|t-1}^{-1}H_tP_{t|t-1}   &&\hbox{Updated } P
\end{align} The filter evaluates these recursively, beginning from
\(\beta_0\), \(P_0\). Treatment of these initial condition reflects
knowledge/model:

\begin{itemize}
\tightlist
\item
  Stationary models can use the steady-state
\item
  Non-stationary models use something which is often (confusingly)
  called a \emph{diffuse prior} (zero mean, large variance)
\end{itemize}

\hypertarget{the-kalman-filter-evaluates-the-likelihood}{%
\section{The Kalman filter evaluates the
likelihood}\label{the-kalman-filter-evaluates-the-likelihood}}

There is an extremely useful spin-off to using the Kalman filter. For
\textbf{\emph{known}} initial conditions -- say \(\beta_0 = b_0\) -- the
likelihood of a state-space model with \(T\) observations of \(m\)
variables is \begin{align}
\log L(\theta | y) &= \sum_{t=1}^T \log \left(p (\theta | \psi_{t-1})\right) \\
  &= - \Phi - \frac{1}{2}\sum_{t=1}^T \left( \log(\det(f_{t|t-1})) + \eta_{t|t-1}' f_{t|t-1}^{-1} \eta_{t|t-1} | \psi_{t-1} \right)
\end{align} where \(\Phi = \frac{Tm}{2}\log \left( 2\pi \right)\) with
\(\theta\) all the non-state parameters to be estimated. We can use the
Kalman filter to obtain \(\eta_{t|t-1}\) and \(f_{t|t-1}\) as they are
the \emph{prediction error} and its \emph{variance}. This is the
\textbf{\emph{prediction error decomposition}} of the log-likelihood.

A maximum likelihood estimate maximizes \(\log L(\theta | y)\) by choice
of \(\theta\).

\hypertarget{plotting-unemployment-forecasts}{%
\chapter{Plotting unemployment
forecasts}\label{plotting-unemployment-forecasts}}

\hypertarget{survey-data-is-messy-people-store-it-in-strange-ways}{%
\section{Survey data is messy, people store it in strange
ways}\label{survey-data-is-messy-people-store-it-in-strange-ways}}

We're going to look at survey data because it has some useful
characteristics that we might want to explore. The data is obtained from
\href{https://www.philadelphiafed.org/}{Philadelphia Federal Reserve}'s
\href{https://www.philadelphiafed.org/research-and-data/real-time-center/survey-of-professional-forecasters}{Survey
of Professional Forecasters}. Their website explains the data.

\begin{tcolorbox}[enhanced jigsaw, breakable, left=2mm, arc=.35mm, toptitle=1mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, bottomrule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{The SPF}, colframe=quarto-callout-note-color-frame, coltitle=black, titlerule=0mm, toprule=.15mm, bottomtitle=1mm, rightrule=.15mm, colback=white]

On the landing page they explain how the survey began and continues:

\emph{'The Survey of Professional Forecasters is the oldest quarterly
survey of macroeconomic forecasts in the United States. The survey began
in 1968 and was conducted by the American Statistical Association and
the National Bureau of Economic Research. The Federal Reserve Bank of
Philadelphia took over the survey in 1990.}

\emph{`The Survey of Professional Forecasters' web page offers the
actual releases, documentation, mean and median forecasts of all the
respondents as well as the individual responses from each economist. The
individual responses are kept confidential by using identification
numbers.'}

From the
\href{https://www.philadelphiafed.org/research-and-data/real-time-center/survey-of-professional-forecasters}{\emph{Survey
of Professional Forecasters}}

\end{tcolorbox}

So we can look at the individual responses by ID number as well as the
aggregate ones, and we will look at both. The data is stored in single
files for different statistics and for different transforms. We will
download the underlying, ID-level responses as well as the average
(mean) level response, but there are also growth rates and so on.

\hypertarget{load-packages}{%
\section{Load packages}\label{load-packages}}

We load the packages we use a lot or would be fiddly to reference in the
code. These are all in the \texttt{tidyverse} including the packages for
date, \texttt{lubridate}, which we in particular will use to deal with
dates including the quarter number as a number.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

The following downloads the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{u }\OtherTok{\textless{}{-}} \StringTok{"https://www.philadelphiafed.org/{-}/media/frbp/assets/surveys{-}and{-}data/survey{-}of{-}professional{-}forecasters/historical{-}data/"}

\FunctionTok{download.file}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(u,}\StringTok{"meanlevel.xlsx"}\NormalTok{), }\AttributeTok{destfile =} \StringTok{"meanlevel.xlsx"}\NormalTok{, }\AttributeTok{mode =} \StringTok{"wb"}\NormalTok{)}
\FunctionTok{download.file}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(u,}\StringTok{"spfmicrodata.xlsx"}\NormalTok{), }\AttributeTok{destfile =} \StringTok{"spfmicrodata.xlsx"}\NormalTok{, }\AttributeTok{mode=}\StringTok{"wb"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This is a `belt-and-braces' approach, we download the data to a file
rather than reading the contents to an object. This is partly because
institutional firewall settings can cause difficulties for the latter
that seem absent from the former. Note the option \texttt{mode\ =\ "wb"}
is crucial for Excel files.

\hypertarget{levels-plots}{%
\section{Levels plots}\label{levels-plots}}

Now read in the levels data to \texttt{UNEMP} and create a date series.
We need to do the latter as the dates in the files are stored as the
year and the quarter as two separate variables. Also, we drop
\texttt{UNEMPA} to \texttt{UNEMPD} which are annual variables we don't
want, retaining the variables called \texttt{UNEMP1} to \texttt{UNEMP6}.

We create the date using \texttt{yq} from \texttt{lubridate} for a
suitably concatenated and formatted input, using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{UNEMP }\OtherTok{\textless{}{-}}\NormalTok{ readxl}\SpecialCharTok{::}\FunctionTok{read\_excel}\NormalTok{(}\StringTok{"meanlevel.xlsx"}\NormalTok{, }\AttributeTok{na=}\StringTok{"\#N/A"}\NormalTok{, }\AttributeTok{sheet=}\StringTok{"UNEMP"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Date =} \FunctionTok{yq}\NormalTok{(}\FunctionTok{paste}\NormalTok{(YEAR, QUARTER))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(Date, }\FunctionTok{num\_range}\NormalTok{(}\StringTok{"UNEMP"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{))}

\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(UNEMP))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrrrr@{}}
\toprule\noalign{}
Date & UNEMP1 & UNEMP2 & UNEMP3 & UNEMP4 & UNEMP5 & UNEMP6 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1968-10-01 & 3.5974 & 3.6218 & 3.8359 & 4.0231 & 3.9910 & 3.9397 \\
1969-01-01 & 3.4000 & 3.5656 & 3.7738 & 3.8525 & 3.8623 & NA \\
1969-04-01 & 3.3036 & 3.4607 & 3.6446 & 3.8196 & 3.9111 & NA \\
1969-07-01 & 3.5036 & 3.6375 & 3.8214 & 3.9304 & 3.9875 & NA \\
1969-10-01 & 3.7055 & 3.8382 & 4.1509 & 4.4127 & 4.4036 & 4.3473 \\
1970-01-01 & 3.6034 & 4.0310 & 4.3586 & 4.4086 & 4.3345 & NA \\
\end{longtable}

\begin{tcolorbox}[enhanced jigsaw, breakable, left=2mm, arc=.35mm, toptitle=1mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, bottomrule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{What are these variables?}, colframe=quarto-callout-note-color-frame, coltitle=black, titlerule=0mm, toprule=.15mm, bottomtitle=1mm, rightrule=.15mm, colback=white]

Consulting Table 3 on page 22 of the
\href{https://www.philadelphiafed.org/-/media/research-and-data/real-time-center/survey-of-professional-forecasters/spf-documentation.pdf}{documentation}
we find:

\emph{At each survey date, we record the projections for various
horizons in the same row. UNEMP1 is the real-time quarterly historical
value for the previous quarter---that is, the quarter before the quarter
when we conducted the survey. UNEMP2 is the forecast (nowcast) for the
current quarter---that is, the quarter when we conducted the survey.
UNEMP3 to UNEMP6 are the forecasts for the following four
quarters\ldots{}}

where we have replaced the actual variable name with UNEMP as Table 3
uses NGDP for illustration.

\end{tcolorbox}

Now we need to organize this by including a date series for each
forecast vintage. We rename the \texttt{UNEMP} variables consistent with
their forecast horizon except for \texttt{UNEMP1} which we rename
\texttt{UNRATE} and time shift to get in the right period.\footnote{Leading
  the variable one period is the correct shift, although it seems the
  wrong way to me every time I think about it.} We use the
\texttt{group\_by(Date)} to create a new variable \texttt{FP} that
starts from each date and goes four periods into the future.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{U2 }\OtherTok{\textless{}{-}}\NormalTok{ UNEMP }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{UNRATE=}\FunctionTok{lead}\NormalTok{(UNEMP1,}\DecValTok{1}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(Date, UNRATE, }\FunctionTok{num\_range}\NormalTok{(}\StringTok{"UNEMP"}\NormalTok{, }\DecValTok{2}\SpecialCharTok{:}\DecValTok{6}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename\_with}\NormalTok{(}\SpecialCharTok{\textasciitilde{}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"UNEMP"}\NormalTok{, }\DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{), }\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"UNEMP"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{starts\_with}\NormalTok{(}\StringTok{"UNE"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Date) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{FP =} \FunctionTok{seq.Date}\NormalTok{(Date[}\DecValTok{1}\NormalTok{], }\AttributeTok{by=}\StringTok{"quarter"}\NormalTok{, }\AttributeTok{length.out=}\DecValTok{5}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

The following plots of the mean forecasts with the early estimate of
unemployment rate supplied:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(U2) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Date, }\AttributeTok{y=}\NormalTok{UNRATE), }\AttributeTok{colour=}\StringTok{"black"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{FP, }\AttributeTok{y=}\NormalTok{value, }\AttributeTok{group=}\NormalTok{Date, }\AttributeTok{colour=}\FunctionTok{as.factor}\NormalTok{(Date))) }\SpecialCharTok{+} 
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Mean forecasts"}\NormalTok{, }\AttributeTok{x=}\StringTok{""}\NormalTok{, }\AttributeTok{y=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Unemp_files/figure-pdf/p3-1.pdf}

}

\end{figure}

There are clear characteristics -- on the way up unemployment is on
average under-predicted, one the way down over-predicted. This gets
worse the further out the forecast horizon. And maybe this century
forecasting has got better. Recent events unsurprisingly look like an
huge outlier.

\hypertarget{underlying-forecasts}{%
\section{Underlying forecasts}\label{underlying-forecasts}}

We do the same for underlying data, at the ID level. Reading the data we
get:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{micro }\OtherTok{\textless{}{-}}\NormalTok{ readxl}\SpecialCharTok{::}\FunctionTok{read\_excel}\NormalTok{(}\StringTok{"spfmicrodata.xlsx"}\NormalTok{, }\AttributeTok{na=}\StringTok{"\#N/A"}\NormalTok{, }\AttributeTok{sheet=}\StringTok{"UNEMP"}\NormalTok{)  }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Date =} \FunctionTok{yq}\NormalTok{(}\FunctionTok{paste}\NormalTok{(YEAR, QUARTER))) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(Date, ID, }\AttributeTok{UNRATE=}\NormalTok{UNEMP1, }\FunctionTok{num\_range}\NormalTok{(}\StringTok{"UNEMP"}\NormalTok{, }\DecValTok{2}\SpecialCharTok{:}\DecValTok{6}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename\_with}\NormalTok{(}\SpecialCharTok{\textasciitilde{}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"UNEMP"}\NormalTok{, }\DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{), }\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"UNEMP"}\NormalTok{))}

\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(micro))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrrrrr@{}}
\toprule\noalign{}
Date & ID & UNRATE & UNEMP0 & UNEMP1 & UNEMP2 & UNEMP3 & UNEMP4 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1968-10-01 & 1 & 3.6 & 3.7 & 4.0 & 4.0 & 3.7 & 3.7 \\
1968-10-01 & 2 & 3.6 & 3.5 & 3.5 & 3.5 & 3.6 & 3.6 \\
1968-10-01 & 3 & 3.6 & 3.7 & 3.9 & 4.2 & 4.2 & 4.1 \\
1968-10-01 & 4 & 3.6 & 3.8 & 4.0 & 4.2 & 4.0 & 4.0 \\
1968-10-01 & 5 & 3.6 & 3.6 & 3.7 & 3.9 & 3.7 & 3.7 \\
1968-10-01 & 6 & 3.5 & 3.6 & 3.7 & 3.9 & 4.0 & 3.9 \\
\end{longtable}

What is apparent from this is that the data is actually at the two-digit
level, and that the historical unemployment rate used by the individual
forecasters isn't necessarily the same. So we can guess that the series
\texttt{UNRATE} in the mean levels file is the \emph{average}
unemployment rates in the previous period observed by the forecaster.
Turns out it is. (Have a look yourself.) We could look at the spread of
these conditioning values, if we want, but here instead is the
difference between the mean and the median value of the information.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{micro }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Date) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{u =} \FunctionTok{mean}\NormalTok{(UNRATE, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{), }\AttributeTok{v =} \FunctionTok{median}\NormalTok{(UNRATE, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Date, }\AttributeTok{y=}\NormalTok{u}\SpecialCharTok{{-}}\NormalTok{v), }\AttributeTok{color =} \StringTok{"pink"}\NormalTok{, }\AttributeTok{linewidth =} \FloatTok{1.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{""}\NormalTok{, }\AttributeTok{y=}\StringTok{""}\NormalTok{, }\AttributeTok{title=}\StringTok{"Mean minus median of all forecasters\textquotesingle{} conditioning data by forecast period"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Unemp_files/figure-pdf/vu-1.pdf}

}

\end{figure}

As we can see, usually this is nearly nothing, although in one case the
discrepancy is almost 0.1 percentage points. On average using the mean
seems fine as a measure of the contemporaneous estimate of the latest
unemployment rate, but perhaps there would be a case for using the
median.

We can do a lot with this data. Let's count and then plot how many
forecasters there are for each period.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{micro }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{starts\_with}\NormalTok{(}\StringTok{"UNEMP"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(value)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Date, name) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{year}\NormalTok{(Date) }\SpecialCharTok{\textgreater{}} \DecValTok{1979}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_col}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Date, }\AttributeTok{y=}\NormalTok{n, }\AttributeTok{group=}\NormalTok{name, }\AttributeTok{fill=}\FunctionTok{as.factor}\NormalTok{(Date)), }\AttributeTok{color=}\ConstantTok{NA}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"UNEMP: No. forecasters forecasting 0 to 4 steps ahead"}\NormalTok{, }\AttributeTok{x=}\StringTok{""}\NormalTok{, }\AttributeTok{y=}\StringTok{""}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position=}\StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ name, }\AttributeTok{ncol=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Unemp_files/figure-pdf/count-1.pdf}

}

\end{figure}

So not everybody does all the steps ahead forecasts but most do, as all
these graphs look about the same -- look at the right hand graph where
you can spot a few missing.

In the late 1980s the number of forecasters had really declined, but it
picked up in the 1990s and hasn't fallen since, although if anything
recessions seem to \emph{increase} the number of forecasters. Let's do
another check -- is this pattern special to \texttt{UNEMP}? We plot the
same for \texttt{INDPROD} and \texttt{NGDP} (we hide the code) and see
whilst clearly not everybody forecasts all the variables the patterns of
numbers of forecasters is very similar, implying for example that it
wasn't just that people stopped forecasting unemployment, they just
stopped forecasting.

\includegraphics{Unemp_files/figure-pdf/counti-1.pdf}

\includegraphics{Unemp_files/figure-pdf/counti-2.pdf}

Unemployment is clearly not the most popular series to forecast, but as
a UK resident I was surprised how relatively popular it was -- UK
forecasts of unemployment rates were rare before Mark Carney became
governor of the Bank of England.\footnote{This was for a number of
  reasons, but forecasters were probably quite pleased not to be judged
  on their performance in a difficult case.}

Next, we do the same as before with vintages of forecast and dates, and
we also create \texttt{IDVin} so we know the vintage of the forecast for
each ID. All of this is put into a long data set to facilitate what we
do next.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{micro\_long }\OtherTok{\textless{}{-}}\NormalTok{ micro }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{starts\_with}\NormalTok{(}\StringTok{"UNE"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{IDVin =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"ID"}\NormalTok{,ID,}\StringTok{"\_Vin:"}\NormalTok{,Date)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(ID, Date) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{FP =} \FunctionTok{seq.Date}\NormalTok{(Date[}\DecValTok{1}\NormalTok{], }\AttributeTok{by=}\StringTok{"quarter"}\NormalTok{, }\AttributeTok{length.out=}\DecValTok{5}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(value)) }
\end{Highlighting}
\end{Shaded}

\hypertarget{aggregating-the-forecasts}{%
\subsection{Aggregating the forecasts}\label{aggregating-the-forecasts}}

Can we recreate the previous graph of the mean forecasts? Turns out we
can and rather easily too. As we did to average the conditioning
unemployment rates, we can use the \texttt{summarise} function to take
the mean across the different IDs. We also redo the conditional rate too
but for the long data set. The plot looks like the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{av }\OtherTok{\textless{}{-}}\NormalTok{ micro\_long }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Date, FP, name) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{m =} \FunctionTok{mean}\NormalTok{(value, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{))}

\NormalTok{u }\OtherTok{\textless{}{-}}\NormalTok{ micro\_long }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Date) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{ur =} \FunctionTok{mean}\NormalTok{(UNRATE, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{ur =} \FunctionTok{lead}\NormalTok{(ur, }\DecValTok{1}\NormalTok{))}

\FunctionTok{ggplot}\NormalTok{(av) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{data =}\NormalTok{ u, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Date, }\AttributeTok{y=}\NormalTok{ur), }\AttributeTok{colour=}\StringTok{"black"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{FP, }\AttributeTok{y=}\NormalTok{m, }\AttributeTok{group=}\NormalTok{Date, }\AttributeTok{colour=}\FunctionTok{as.factor}\NormalTok{(Date))) }\SpecialCharTok{+} 
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Means of the dis{-}aggregate forecasts"}\NormalTok{, }\AttributeTok{x=}\StringTok{""}\NormalTok{, }\AttributeTok{y=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Unemp_files/figure-pdf/av-1.pdf}

}

\end{figure}

That looks pretty much like we had before.\footnote{Actually if you look
  at all the numbers properly there is the odd discrepancy, but it is
  not possible to say if the source of the differences lies in the
  disaggregate data or the mean levels. There are lots of good reasons
  for small discrepancies.} But we can also look at the dispersion, and
obtain various measures in the same way as for the mean.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\OtherTok{\textless{}{-}}\NormalTok{ micro\_long }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Date, FP, name) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{v =} \FunctionTok{var}\NormalTok{(value, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{), }
            \AttributeTok{a =} \FunctionTok{mad}\NormalTok{(value, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{), }
            \AttributeTok{i =} \FunctionTok{IQR}\NormalTok{(value, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{))}

\NormalTok{d }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{year}\NormalTok{(Date) }\SpecialCharTok{\textless{}} \DecValTok{2020} \SpecialCharTok{\&} \FunctionTok{year}\NormalTok{(Date) }\SpecialCharTok{\textgreater{}} \DecValTok{1979}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{FP, }\AttributeTok{y=}\FunctionTok{sqrt}\NormalTok{(v), }\AttributeTok{group=}\NormalTok{name), }\AttributeTok{color=}\StringTok{"red"}\NormalTok{, }\AttributeTok{alpha=}\NormalTok{.}\DecValTok{5}\NormalTok{, }\AttributeTok{linewidth=}\NormalTok{.}\DecValTok{8}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{FP, }\AttributeTok{y=}\NormalTok{a, }\AttributeTok{group=}\NormalTok{name), }\AttributeTok{color=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{alpha=}\NormalTok{.}\DecValTok{5}\NormalTok{, }\AttributeTok{size=}\FloatTok{1.2}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{FP, }\AttributeTok{y=}\NormalTok{i}\SpecialCharTok{/}\FloatTok{1.349}\NormalTok{, }\AttributeTok{group=}\NormalTok{name), }\AttributeTok{color=}\StringTok{"darkgreen"}\NormalTok{, }\AttributeTok{alpha=}\NormalTok{.}\DecValTok{5}\NormalTok{, }\AttributeTok{size=}\FloatTok{1.2}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Dispersion of the dis{-}aggregate forecasts"}\NormalTok{, }\AttributeTok{x=}\StringTok{""}\NormalTok{, }\AttributeTok{y=}\StringTok{""}\NormalTok{,}
       \AttributeTok{subtitle=}\StringTok{"Standard error (red); Mean Absolute Deviation (blue); IQR/1.349 (green)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ name, }\AttributeTok{ncol =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Unemp_files/figure-pdf/disp-1.pdf}

}

\end{figure}

So already, this looks useful. We can see that the dispersion measures
-- standard deviation, mean absolute deviation and inter-quartile
range\footnote{We deflate this by
  \href{https://en.wikipedia.org/wiki/Robust_measures_of_scale\#Estimation}{1.349}
  to convert it to an asymptotically valid (given normality) estimate of
  the standard error.} -- conform to type, getting larger the longer the
horizon, bigger at turning points etc.

\hypertarget{everything}{%
\subsection{Everything}\label{everything}}

What about plotting \textbf{all} the underlying forecasts?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(micro\_long) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{FP, }\AttributeTok{y=}\NormalTok{value, }\AttributeTok{group=}\NormalTok{IDVin, }\AttributeTok{colour=}\FunctionTok{as.factor}\NormalTok{(ID))) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"All unemployment forecasts"}\NormalTok{, }\AttributeTok{x=}\StringTok{""}\NormalTok{, }\AttributeTok{y=}\StringTok{"Rate (percentage points)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Unemp_files/figure-pdf/unnamed-chunk-1-1.pdf}

}

\end{figure}

\hypertarget{sub-groups-of-forecasters}{%
\subsection{Sub groups of forecasters}\label{sub-groups-of-forecasters}}

Recent forecasts by forecasters with earlier (i.e.~lower value) IDs
\emph{could} be the most experienced, but we are relying on the SPF
classification. So here's a rough split based on ID number, which shows
some differences that we might ascribe to experience.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{micro\_long }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Split\_ID =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    ID }\SpecialCharTok{\textless{}} \DecValTok{450} \SpecialCharTok{\textasciitilde{}} \StringTok{"ID 1{-}449"}\NormalTok{,}
\NormalTok{    ID }\SpecialCharTok{\textgreater{}} \DecValTok{550} \SpecialCharTok{\textasciitilde{}} \StringTok{"ID 551+"}\NormalTok{,}
    \ConstantTok{TRUE}     \SpecialCharTok{\textasciitilde{}} \StringTok{"ID 450{-}550"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{FP, }\AttributeTok{y=}\NormalTok{value, }\AttributeTok{group=}\NormalTok{IDVin, }\AttributeTok{colour=}\FunctionTok{as.factor}\NormalTok{(ID))) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ Split\_ID) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{""}\NormalTok{,}\AttributeTok{y=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Unemp_files/figure-pdf/pa-1.pdf}

}

\end{figure}

But we can be more accurate. We just work out when they did their first
forecast and then set the groups accordingly. Here, we detect those that
forecasted since the last millennium\footnote{And yes, 2000 was in the
  last millennium.}, since the onset of the last recession, or in
between. So are the new kids better -- or worse?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{micro\_long }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(ID) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Earliest =} \FunctionTok{min}\NormalTok{(Date)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Recent =} \FunctionTok{case\_when}\NormalTok{(}
    \FunctionTok{year}\NormalTok{(Earliest) }\SpecialCharTok{\textgreater{}} \DecValTok{2008} \SpecialCharTok{\textasciitilde{}} \StringTok{"3. First forecasted after 2008"}\NormalTok{,}
    \FunctionTok{year}\NormalTok{(Earliest) }\SpecialCharTok{\textless{}} \DecValTok{2001} \SpecialCharTok{\textasciitilde{}} \StringTok{"1. First forecasted before 2001"}\NormalTok{,}
    \ConstantTok{TRUE}                  \SpecialCharTok{\textasciitilde{}} \StringTok{"2. Started forecasting 2001{-}2008"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{year}\NormalTok{(FP) }\SpecialCharTok{\textgreater{}} \DecValTok{2000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{FP, }\AttributeTok{y=}\NormalTok{value, }\AttributeTok{group=}\NormalTok{IDVin, }\AttributeTok{colour=}\FunctionTok{as.factor}\NormalTok{(ID))) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Recent) }\SpecialCharTok{+} 
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"US unemployment forecasts by when forecaster first operated"}\NormalTok{, }\AttributeTok{x=}\StringTok{""}\NormalTok{, }\AttributeTok{y=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Unemp_files/figure-pdf/compare-1.pdf}

}

\end{figure}

Worse, right? Kids today, don't know they're born. Maybe, maybe
not\ldots. Here's a plot of some of the forecasts which reveal breaks:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{filter}\NormalTok{(micro, ID}\SpecialCharTok{\textgreater{}=}\DecValTok{400} \SpecialCharTok{\&}\NormalTok{ ID }\SpecialCharTok{\textless{}} \DecValTok{434}\NormalTok{)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Date, }\AttributeTok{y=}\NormalTok{UNEMP1, }\AttributeTok{group=}\NormalTok{ID, }\AttributeTok{color=}\FunctionTok{as.factor}\NormalTok{(ID)), }\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ ID) }\SpecialCharTok{+} 
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{""}\NormalTok{, }\AttributeTok{y=}\StringTok{""}\NormalTok{, }\AttributeTok{title=}\StringTok{"UNEMP1 for ID numbers in the early 400s"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Unemp_files/figure-pdf/IDs-1.pdf}

}

\end{figure}

This seems to imply that the
\href{https://www.philadelphiafed.org/-/media/research-and-data/real-time-center/survey-of-professional-forecasters/spf-caveats.pdf}{caveats
that some ID numbers may have been re-used when forecasters left the
panel} could be important -- did 422 go on a prolonged holiday? Did 429
figure unemployment was too hard to forecast in the great financial
crisis? Or were they (ironically) unemployed at that time?\ldots{}

\part{Quantiles}

\hypertarget{quantile-fan-charts}{%
\chapter{Quantile fan charts}\label{quantile-fan-charts}}

\hypertarget{quantile-regression}{%
\section{Quantile regression}\label{quantile-regression}}

QR is a powerful technique building on the insight that the predicted
value of a regression need not be the conditional mean. Instead QR
models a chosen conditional quantile of the distribution of the target
variable.

The predicted quantiles can be used individually, perhaps as a proxy for
risk, or used to approximate a complete predicted density. It is the
last of these that particularly concerns us.

Gaglianone and Lima (2012) estimate forecast densities for the U.S.
unemployment rate using the consensus forecast from the Survey of
Professional Forecasters (SPF). This generalizes the proposition of
Capistrán and Timmermann (2009), that we can efficiently combine
forecasts by regressing the consensus (usually mean) forecast on the
out-turn and use the resulting equation for point-forecasting
unemployment, to one where the density of the combined forecast is
predicted.

They suggest:

\begin{quote}
`The resulting density forecast is far from normal and is therefore able
to reflect the current increased risk of a higher unemployment rate in
the U.S. economy provoked by the recent subprime crisis.' Gaglianone and
Lima (2012), p.~1598
\end{quote}

This highlights the two useful features that makes this technique so
valuable. The estimated forecast density need not be normal and is
potentially state dependent. But it is difficult to tell how significant
this effect is from one graph, or whether the upward spread of the
density is associated with the subprime crisis. What happens if the
unemployment rate is radically different from the experience of 2010? As
the subprime crisis is no longer quite so recent we consider the
following: when and how are the QR-forecast densities for U.S.
unemployment asymmetric?

Capistrán and Timmermann (2009) suggested combining forecasts by
regressing the out-turn on the mean forecast. For unemployment forecasts
would be \[
   u_{t+k} = \beta_0+\beta_1 \hat u_{t,t+k}+\varepsilon_{t+k} 
\] where \(u_t\) is the quarterly unemployment rate expressed as a
percentage and \(\hat u_{t,t+k}\) is the SPF's mean forecast, \(k = 1\)
to 4 quarters ahead. They find this an effective way to combine
forecasts from different sources particularly with a non-stationary
panel of forecasters, characteristic of the SPF. This models the
conditional mean of unemployment as a `bias corrected' forecast from the
aggregated information.

Gaglianone and Lima (2012) use QR instead of OLS, and predict the
\(\alpha\)-quantile of \(u_{t+k}\) so that \[
  Q_\alpha(u_{t+k}) = \beta_0(\alpha) + \beta_1(\alpha) \hat u_{t,t+k} + \varepsilon{(\alpha)}_{t+k}
\] where \(Q_\alpha(\cdot)\) is the quantile function. This yields a
sequence of models, indexed both by \(\alpha\) and the forecast horizon.
It is a simple matter to derive the required forecast density from them,
typically smoothed using a kernel method.

\hypertarget{a-fan-chart}{%
\section{A fan chart}\label{a-fan-chart}}

Gaglianone and Lima (2012) don't actually produce a fan chart, they plot
the implied densities. We can do that, but the fan chart is more useful,
particularly for something as visually striking as unemployment
forecasts.

The following code does a lot. It read the SPF individual forecasts and
then averages them (you'll see why later). Then it does the QR,
calculates the points for the fans a plots a fan chart.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(readxl)}
\FunctionTok{library}\NormalTok{(quantreg)}
\FunctionTok{library}\NormalTok{(moments)}

\NormalTok{fls    }\OtherTok{\textless{}{-}} \StringTok{"spfmicrodata.xlsx"}
\NormalTok{UNRATE }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"UNRATE (1).csv"}\NormalTok{, }\AttributeTok{col\_types=}\FunctionTok{cols}\NormalTok{(}\AttributeTok{DATE=}\FunctionTok{col\_date}\NormalTok{(}\AttributeTok{format=}\StringTok{"\%d/\%m/\%Y"}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\AttributeTok{Date=}\NormalTok{DATE, UNRATE) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{UNRATE=}\FunctionTok{as.numeric}\NormalTok{(UNRATE)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{drop\_na}\NormalTok{()}

\NormalTok{mx }\OtherTok{\textless{}{-}} \FunctionTok{read\_excel}\NormalTok{(fls, }\AttributeTok{sheet=}\StringTok{"UNEMP"}\NormalTok{, }\AttributeTok{na=}\StringTok{"\#N/A"}\NormalTok{, }\AttributeTok{col\_types=}\StringTok{"numeric"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unite}\NormalTok{(Date, }\FunctionTok{c}\NormalTok{(YEAR, QUARTER), }\AttributeTok{sep=}\StringTok{" "}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Date =} \FunctionTok{yq}\NormalTok{(Date)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(Date, }\AttributeTok{UNEMP0=}\NormalTok{UNEMP2, }\AttributeTok{UNEMP1=}\NormalTok{UNEMP3, }\AttributeTok{UNEMP2=}\NormalTok{UNEMP4, }\AttributeTok{UNEMP3=}\NormalTok{UNEMP5, }\AttributeTok{UNEMP4=}\NormalTok{UNEMP6, ID) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols=}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(Date, ID), }\AttributeTok{names\_to=}\StringTok{"Horizon"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Date, Horizon) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{av   =} \FunctionTok{mean}\NormalTok{(value, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Horizon =} \FunctionTok{as.integer}\NormalTok{(}\FunctionTok{str\_sub}\NormalTok{(Horizon,}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{drop\_na}\NormalTok{() }

\CommentTok{\# Pivot wider to put each series in a column to merge with UNEMP data}
\NormalTok{mxb }\OtherTok{\textless{}{-}}\NormalTok{ mx }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(Date, Horizon), }\AttributeTok{names\_to =} \StringTok{"Variable"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unite}\NormalTok{(}\StringTok{"Names"}\NormalTok{, Variable}\SpecialCharTok{:}\NormalTok{Horizon, }\AttributeTok{sep=}\StringTok{""}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ Names) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{n}\NormalTok{())}

\NormalTok{UNEMP }\OtherTok{\textless{}{-}}\NormalTok{ UNRATE }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\#group\_by(paste(year(Date), quarter(Date))) \%\textgreater{}\% }
  \CommentTok{\#summarise(Date=min(Date), UNRATE=mean(UNRATE)) \%\textgreater{}\% }
  \CommentTok{\#select(Date, UNRATE) \%\textgreater{}\% }
  \FunctionTok{right\_join}\NormalTok{(mxb, }\AttributeTok{by=}\StringTok{"Date"}\NormalTok{) }

\CommentTok{\# QReg }
\NormalTok{sf }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(.}\DecValTok{05}\NormalTok{,.}\DecValTok{95}\NormalTok{,.}\DecValTok{15}\NormalTok{)[}\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{]}

\NormalTok{tail\_colour   }\OtherTok{\textless{}{-}} \StringTok{"grey95"}
\NormalTok{centre\_colour }\OtherTok{\textless{}{-}} \StringTok{"seagreen"}

\NormalTok{nq  }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(sf)}
\NormalTok{nv  }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(centre\_colour)}
\NormalTok{col }\OtherTok{\textless{}{-}} \FunctionTok{colorRampPalette}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(tail\_colour, centre\_colour), tail\_colour))(nv}\SpecialCharTok{*}\NormalTok{nq}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}

\NormalTok{ystart }\OtherTok{\textless{}{-}} \DecValTok{2017}
\NormalTok{ares   }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{) \{}
\NormalTok{          reg  }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"av"}\NormalTok{,i))}
\NormalTok{          eq   }\OtherTok{\textless{}{-}} \FunctionTok{formula}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"lead(UNRATE,"}\NormalTok{,(i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{),}\StringTok{") \textasciitilde{} "}\NormalTok{, reg))}
\NormalTok{          eqrq }\OtherTok{\textless{}{-}} \FunctionTok{rq}\NormalTok{(eq, }\AttributeTok{data=}\NormalTok{UNEMP, }\AttributeTok{tau=}\NormalTok{sf)}
\NormalTok{          res  }\OtherTok{\textless{}{-}}\NormalTok{ broom}\SpecialCharTok{::}\FunctionTok{tidy}\NormalTok{(eqrq) }\SpecialCharTok{\%\textgreater{}\%} 
            \FunctionTok{group\_by}\NormalTok{(tau) }\SpecialCharTok{\%\textgreater{}\%}
            \FunctionTok{mutate}\NormalTok{(}\AttributeTok{dta =} \FunctionTok{unlist}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FunctionTok{slice}\NormalTok{(}\FunctionTok{select}\NormalTok{(UNEMP, reg), }\FunctionTok{n}\NormalTok{())))) }\SpecialCharTok{\%\textgreater{}\%} 
            \FunctionTok{mutate}\NormalTok{(}\AttributeTok{q   =} \FunctionTok{sum}\NormalTok{(estimate}\SpecialCharTok{*}\NormalTok{dta)) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{            ungroup }\SpecialCharTok{\%\textgreater{}\%}
            \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Vintage =} \FunctionTok{max}\NormalTok{(UNEMP}\SpecialCharTok{$}\NormalTok{Date), }\AttributeTok{Horizon =}\NormalTok{ i) }
\NormalTok{          ares }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(ares, res)}
\NormalTok{\}}

\NormalTok{aresx }\OtherTok{\textless{}{-}}\NormalTok{ ares }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(tau, q, Vintage, Horizon) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{distinct}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{left\_join}\NormalTok{(}\FunctionTok{select}\NormalTok{(UNEMP, Date, UNRATE), }\AttributeTok{by=}\FunctionTok{c}\NormalTok{(}\StringTok{"Vintage"} \OtherTok{=} \StringTok{"Date"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{q =} \FunctionTok{if\_else}\NormalTok{(Horizon }\SpecialCharTok{==} \DecValTok{0}\NormalTok{, UNRATE, q)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ tau, }\AttributeTok{names\_prefix =} \StringTok{"tau="}\NormalTok{, }\AttributeTok{values\_from =}\NormalTok{ q) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Vintage) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{fdate =} \FunctionTok{seq.Date}\NormalTok{(}\AttributeTok{from =}\NormalTok{ Vintage[}\DecValTok{1}\NormalTok{], }\AttributeTok{by =} \StringTok{"quarter"}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{5}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"tau"}\NormalTok{), }\AttributeTok{names\_to =} \StringTok{"q"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Vals"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{qs =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"Q"}\NormalTok{, q)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{qs =} \FunctionTok{as\_factor}\NormalTok{(}\FunctionTok{desc}\NormalTok{(qs))) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{UNRATE)}

\NormalTok{bck }\OtherTok{\textless{}{-}}\NormalTok{ UNEMP }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(Date, UNRATE) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{year}\NormalTok{(Date) }\SpecialCharTok{\textgreater{}}\NormalTok{ ystart) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Vintage =} \FunctionTok{list}\NormalTok{(}\FunctionTok{unique}\NormalTok{(aresx}\SpecialCharTok{$}\NormalTok{Vintage)))  }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =}\NormalTok{ Vintage) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Vintage) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(Date }\SpecialCharTok{\textless{}=}\NormalTok{ Vintage) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(Vintage, Date)}

\FunctionTok{ggplot}\NormalTok{(aresx) }\SpecialCharTok{+}
  \FunctionTok{geom\_rect}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xmin=}\NormalTok{fdate, }\AttributeTok{xmax=}\FunctionTok{max}\NormalTok{(fdate), }\AttributeTok{ymin=}\SpecialCharTok{{-}}\ConstantTok{Inf}\NormalTok{, }\AttributeTok{ymax=}\ConstantTok{Inf}\NormalTok{, }\AttributeTok{group=}\NormalTok{q), }\AttributeTok{fill=}\NormalTok{tail\_colour) }\SpecialCharTok{+}
  \FunctionTok{geom\_area}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{fdate, }\AttributeTok{y=}\NormalTok{Vals, }\AttributeTok{group=}\NormalTok{qs, }\AttributeTok{fill=}\NormalTok{qs), }\AttributeTok{position=}\StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values=}\NormalTok{col) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_date}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{data=}\NormalTok{bck, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Date, }\AttributeTok{y=}\NormalTok{UNRATE), }\AttributeTok{colour=}\StringTok{"grey44"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{facet\_grid}\NormalTok{(Vintage }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\FunctionTok{paste}\NormalTok{(}\StringTok{"US unemployment rate and forecast fanchart"}\NormalTok{), }\AttributeTok{y=}\StringTok{""}\NormalTok{, }\AttributeTok{x=}\StringTok{""}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{FansQR_files/figure-pdf/unnamed-chunk-1-1.pdf}

}

\end{figure}

\hypertarget{different-horizons}{%
\section{Different horizons}\label{different-horizons}}

We compute recursive quantile regressions are calculated in pseudo-real
time (we don't take revisions to \(u_t\) into account, but it should be
noted these are relatively minor). We use the available SPF forecasts
from the last period used in the estimation to make the quantile
predictions.

Add we add a regressor -- a measure of uncertainty.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add some scale/skew measures}
\NormalTok{mx }\OtherTok{\textless{}{-}} \FunctionTok{read\_excel}\NormalTok{(fls, }\AttributeTok{sheet=}\StringTok{"UNEMP"}\NormalTok{, }\AttributeTok{na=}\StringTok{"\#N/A"}\NormalTok{, }\AttributeTok{col\_types=}\StringTok{"numeric"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unite}\NormalTok{(Date, }\FunctionTok{c}\NormalTok{(YEAR, QUARTER), }\AttributeTok{sep=}\StringTok{" "}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Date =} \FunctionTok{yq}\NormalTok{(Date)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(Date, }\AttributeTok{UNEMP0=}\NormalTok{UNEMP2, }\AttributeTok{UNEMP1=}\NormalTok{UNEMP3, }\AttributeTok{UNEMP2=}\NormalTok{UNEMP4, }\AttributeTok{UNEMP3=}\NormalTok{UNEMP5, }\AttributeTok{UNEMP4=}\NormalTok{UNEMP6, ID) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols=}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(Date, ID), }\AttributeTok{names\_to=}\StringTok{"Horizon"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Date, Horizon) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{n    =} \FunctionTok{n}\NormalTok{(),}
            \AttributeTok{av   =} \FunctionTok{mean}\NormalTok{(value, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{),}
            \AttributeTok{IQR  =} \FunctionTok{IQR}\NormalTok{(value,  }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)}\SpecialCharTok{/}\FloatTok{1.349}\NormalTok{,}
            \AttributeTok{Var  =} \FunctionTok{var}\NormalTok{(value,  }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{),}
            \AttributeTok{SE   =} \FunctionTok{sqrt}\NormalTok{(Var),}
            \AttributeTok{MAD  =} \FunctionTok{mad}\NormalTok{(value,  }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{),}
            \AttributeTok{Skew =} \FunctionTok{skewness}\NormalTok{(value, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Skew =} \FunctionTok{replace\_na}\NormalTok{(Skew, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Horizon =} \FunctionTok{as.integer}\NormalTok{(}\FunctionTok{str\_sub}\NormalTok{(Horizon,}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{drop\_na}\NormalTok{()}

\CommentTok{\# Pivot wider to put each series in a column to merge with UNEMP data}
\NormalTok{mxb }\OtherTok{\textless{}{-}}\NormalTok{ mx }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(Date, Horizon), }\AttributeTok{names\_to =} \StringTok{"Variable"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unite}\NormalTok{(}\StringTok{"name"}\NormalTok{, Variable}\SpecialCharTok{:}\NormalTok{Horizon, }\AttributeTok{sep=}\StringTok{""}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_wider}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{n}\NormalTok{())}

\NormalTok{UNEMP }\OtherTok{\textless{}{-}}\NormalTok{ UNRATE }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\#group\_by(paste(year(Date), quarter(Date))) \%\textgreater{}\%}
  \CommentTok{\#summarise(Date=min(Date), UNRATE=mean(UNRATE)) \%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(Date, UNRATE) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{right\_join}\NormalTok{(mxb, }\AttributeTok{by=}\StringTok{"Date"}\NormalTok{)}

\CommentTok{\# QReg }
\NormalTok{sf }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(.}\DecValTok{05}\NormalTok{,.}\DecValTok{95}\NormalTok{,.}\DecValTok{15}\NormalTok{)[}\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{]}

\NormalTok{tail\_colour   }\OtherTok{\textless{}{-}} \StringTok{"grey95"}
\NormalTok{centre\_colour }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"maroon4"}\NormalTok{,}\StringTok{"seagreen"}\NormalTok{)}

\NormalTok{nq  }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(sf)}
\NormalTok{nv  }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(centre\_colour)}
\NormalTok{col }\OtherTok{\textless{}{-}} \FunctionTok{colorRampPalette}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(tail\_colour, centre\_colour), tail\_colour))(nv}\SpecialCharTok{*}\NormalTok{nq}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}

\NormalTok{N   }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(UNEMP)}

\NormalTok{K   }\OtherTok{\textless{}{-}} \DecValTok{12}
\NormalTok{nt  }\OtherTok{\textless{}{-}} \DecValTok{6}

\NormalTok{ystart }\OtherTok{\textless{}{-}} \DecValTok{2018}
\NormalTok{meas   }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"None"}\NormalTok{, }\StringTok{"Var"}\NormalTok{)}
\NormalTok{setype }\OtherTok{\textless{}{-}} \StringTok{"rank"} \CommentTok{\# "rank" \# "boot" "nid"}

\NormalTok{ares }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\ControlFlowTok{for}\NormalTok{ (samp }\ControlFlowTok{in} \FunctionTok{seq}\NormalTok{(N}\SpecialCharTok{{-}}\NormalTok{K,N,nt)) \{}
\NormalTok{    UNEMPs }\OtherTok{\textless{}{-}} \FunctionTok{head}\NormalTok{(UNEMP, samp)}
    \ControlFlowTok{for}\NormalTok{ (nm }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(meas)) \{}
        \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{0}\SpecialCharTok{:}\DecValTok{4}\NormalTok{) \{}
\NormalTok{          reg  }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"av"}\NormalTok{,i))}
          \ControlFlowTok{if}\NormalTok{(nm }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{) reg }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(reg, }\FunctionTok{paste0}\NormalTok{(meas[nm],i))}
\NormalTok{          eq   }\OtherTok{\textless{}{-}} \FunctionTok{formula}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"lead(UNRATE,"}\NormalTok{,(i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{),}\StringTok{") \textasciitilde{} "}\NormalTok{, }\FunctionTok{paste}\NormalTok{(reg, }\AttributeTok{collapse =} \StringTok{" + "}\NormalTok{)))}
\NormalTok{          eqrq }\OtherTok{\textless{}{-}} \FunctionTok{rq}\NormalTok{(eq, }\AttributeTok{data=}\NormalTok{UNEMPs, }\AttributeTok{tau=}\NormalTok{sf)}
\NormalTok{          res  }\OtherTok{\textless{}{-}}\NormalTok{ broom}\SpecialCharTok{::}\FunctionTok{tidy}\NormalTok{(eqrq, }\AttributeTok{se.type=}\NormalTok{setype) }\SpecialCharTok{\%\textgreater{}\%} 
            \FunctionTok{group\_by}\NormalTok{(tau) }\SpecialCharTok{\%\textgreater{}\%}
            \FunctionTok{mutate}\NormalTok{(}\AttributeTok{dta =} \FunctionTok{unlist}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FunctionTok{slice}\NormalTok{(}\FunctionTok{select}\NormalTok{(UNEMPs, }\FunctionTok{all\_of}\NormalTok{(reg)), }\FunctionTok{n}\NormalTok{())))) }\SpecialCharTok{\%\textgreater{}\%} 
            \FunctionTok{mutate}\NormalTok{(}\AttributeTok{q   =} \FunctionTok{sum}\NormalTok{(estimate}\SpecialCharTok{*}\NormalTok{dta)) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{            ungroup }\SpecialCharTok{\%\textgreater{}\%}
            \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Vintage =} \FunctionTok{max}\NormalTok{(UNEMPs}\SpecialCharTok{$}\NormalTok{Date), }\AttributeTok{Horizon =}\NormalTok{ i, }\AttributeTok{Dispersion =}\NormalTok{ meas[nm]) }
\NormalTok{          ares }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(ares, res)}
\NormalTok{        \}}
\NormalTok{    \}}
\NormalTok{\}}

\NormalTok{aresx }\OtherTok{\textless{}{-}}\NormalTok{ ares }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(tau, q, Vintage, Horizon, Dispersion) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{distinct}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{left\_join}\NormalTok{(}\FunctionTok{select}\NormalTok{(UNEMP, Date, UNRATE), }\AttributeTok{by=}\FunctionTok{c}\NormalTok{(}\StringTok{"Vintage"} \OtherTok{=} \StringTok{"Date"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{q =} \FunctionTok{if\_else}\NormalTok{(Horizon }\SpecialCharTok{==} \DecValTok{0}\NormalTok{, UNRATE, q)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{names\_from =}\NormalTok{ tau, }\AttributeTok{names\_prefix =} \StringTok{"tau="}\NormalTok{, }\AttributeTok{values\_from =}\NormalTok{ q) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Dispersion, Vintage) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{fdate =} \FunctionTok{seq.Date}\NormalTok{(}\AttributeTok{from =}\NormalTok{ Vintage[}\DecValTok{1}\NormalTok{], }\AttributeTok{by =} \StringTok{"quarter"}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{5}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"tau"}\NormalTok{), }\AttributeTok{names\_to =} \StringTok{"q"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Vals"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{qs =} \FunctionTok{paste0}\NormalTok{(Dispersion, }\FunctionTok{gsub}\NormalTok{(}\StringTok{"tau="}\NormalTok{, }\StringTok{""}\NormalTok{, q))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{qs =} \FunctionTok{factor}\NormalTok{(}\FunctionTok{desc}\NormalTok{(qs))) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Dispersion =} \FunctionTok{as\_factor}\NormalTok{(Dispersion)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{UNRATE)}

\NormalTok{bck }\OtherTok{\textless{}{-}}\NormalTok{ UNEMP }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(Date, UNRATE) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{year}\NormalTok{(Date) }\SpecialCharTok{\textgreater{}}\NormalTok{ ystart) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Vintage =} \FunctionTok{list}\NormalTok{(}\FunctionTok{unique}\NormalTok{(aresx}\SpecialCharTok{$}\NormalTok{Vintage)))  }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =}\NormalTok{ Vintage) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Vintage) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(Date }\SpecialCharTok{\textless{}=}\NormalTok{ Vintage) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(Vintage, Date)}

\NormalTok{aresx }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_rect}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xmin=}\NormalTok{fdate, }\AttributeTok{xmax=}\NormalTok{Vintage }\SpecialCharTok{\%m+\%} \FunctionTok{months}\NormalTok{(}\DecValTok{3}\NormalTok{), }\AttributeTok{ymin=}\SpecialCharTok{{-}}\ConstantTok{Inf}\NormalTok{, }\AttributeTok{ymax=}\ConstantTok{Inf}\NormalTok{, }\AttributeTok{group=}\NormalTok{q), }\AttributeTok{fill=}\NormalTok{tail\_colour) }\SpecialCharTok{+}
  \FunctionTok{geom\_area}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{fdate, }\AttributeTok{y=}\NormalTok{Vals, }\AttributeTok{group=}\NormalTok{qs, }\AttributeTok{fill=}\NormalTok{qs), }\AttributeTok{position =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_manual}\NormalTok{(}\AttributeTok{values=}\NormalTok{col) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_date}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{expand =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{data=}\NormalTok{bck, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Date, }\AttributeTok{y=}\NormalTok{UNRATE), }\AttributeTok{colour=}\StringTok{"grey44"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{facet\_grid}\NormalTok{(Vintage }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Dispersion) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\FunctionTok{paste}\NormalTok{(}\StringTok{"US unemployment rate and forecast fanchart"}\NormalTok{), }\AttributeTok{y=}\StringTok{""}\NormalTok{, }\AttributeTok{x=}\StringTok{""}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{FansQR_files/figure-pdf/unnamed-chunk-2-1.pdf}

}

\end{figure}

\hypertarget{gdprisk}{%
\chapter{GDP@Risk}\label{gdprisk}}

\hypertarget{simple-implementation-of-gdp-at-risk-using-r}{%
\section{\texorpdfstring{Simple implementation of \emph{GDP-at-Risk}
using
R}{Simple implementation of GDP-at-Risk using R}}\label{simple-implementation-of-gdp-at-risk-using-r}}

A somewhat fashionable use of quantile regression is by Adrian,
Boyarchenko, and Giannone (2019), and their idea has become known as
\emph{GDP-at-risk}, rather like VaR. The idea is to use a simple
forecasting model that uses some financial indicator and find the ``at
risk'' value of growth. There are three elements:

\begin{itemize}
\tightlist
\item
  Estimate a ``forecasting model'' using quantile regression that
  depends on some forward-looking indicator.
\item
  Fit a skew Student-t model to the output of their quantile estimation
  procedure, and use this to find the ``at risk'' value.
\item
  We will plot the result as a \emph{ridgeline graph}, which are really
  quite cool.
\end{itemize}

This is computationally quite a bit harder than our other applications.

\hypertarget{prelims}{%
\section{Prelims}\label{prelims}}

We need a lot of libraries for this one, although we have relatively
little code!

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(jsonlite)          }\CommentTok{\# Read in UK data}
\FunctionTok{library}\NormalTok{(quantreg)          }\CommentTok{\# Quantile regression}
\FunctionTok{library}\NormalTok{(ggridges)          }\CommentTok{\# Ridgeline plots}
\FunctionTok{library}\NormalTok{(viridis)           }\CommentTok{\# Colours for graphs}
\FunctionTok{library}\NormalTok{(tidyverse)         }\CommentTok{\# Usual}
\FunctionTok{library}\NormalTok{(readxl)            }\CommentTok{\# And more...}
\FunctionTok{library}\NormalTok{(sn)                }\CommentTok{\# Skew{-}t distribution}
\end{Highlighting}
\end{Shaded}

\hypertarget{data}{%
\section{Data}\label{data}}

Downloaded data \emph{up until the repsent} for
\href{https://www.ons.gov.uk/economy/grossdomesticproductgdp/timeseries/ihyr}{UK
GDP growth} in
\href{https://www.ons.gov.uk/economy/grossdomesticproductgdp/timeseries/ihyr/data}{JSON
format} from ONS UK in a file called \texttt{ihyr.json}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{json  }\OtherTok{\textless{}{-}} \FunctionTok{fromJSON}\NormalTok{(}\StringTok{"ihyr.json"}\NormalTok{)  }\CommentTok{\# Use jsonlite to parse file}

\CommentTok{\# Retrieve quarterly data, dates etc and calculate lags}
\NormalTok{qdata }\OtherTok{\textless{}{-}}\NormalTok{ json}\SpecialCharTok{$}\NormalTok{quarters }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Date   =} \FunctionTok{yq}\NormalTok{(date), }
         \AttributeTok{Growth =} \FunctionTok{as.numeric}\NormalTok{(value)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(Date, Growth) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Growth\_1 =} \FunctionTok{lag}\NormalTok{(Growth, }\DecValTok{1}\NormalTok{), }
         \AttributeTok{Growth\_4 =} \FunctionTok{lag}\NormalTok{(Growth, }\DecValTok{4}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{drop\_na}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\href{https://www.bis.org/statistics/totcredit.htm}{BIS credit data} is
available
\href{https://www.bis.org/statistics/totcredit/totcredit.xlsx}{here} --
be warned there is a lot of it, and you need to get all of it to find
the bits you want.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{totcredit }\OtherTok{\textless{}{-}} \FunctionTok{read\_excel}\NormalTok{(}\StringTok{"totcredit.xlsx"}\NormalTok{, }
                        \AttributeTok{sheet =} \StringTok{"Quarterly Series"}\NormalTok{,}
                        \AttributeTok{col\_types =} \FunctionTok{c}\NormalTok{(}\StringTok{"date"}\NormalTok{, }\FunctionTok{rep}\NormalTok{(}\StringTok{"text"}\NormalTok{, }\DecValTok{1133}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%}   \CommentTok{\# This my need adjusting}
  \FunctionTok{select}\NormalTok{(}\AttributeTok{Date =} \StringTok{"Back to menu"}\NormalTok{, }\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"United K"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}            \CommentTok{\# Find UK}
  \FunctionTok{slice}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{contains}\NormalTok{(}\StringTok{"US Dollar"}\NormalTok{), }\SpecialCharTok{{-}}\FunctionTok{contains}\NormalTok{(}\StringTok{"Unadjusted"}\NormalTok{), }\SpecialCharTok{{-}}\FunctionTok{contains}\NormalTok{(}\StringTok{"Domestic currency"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Date =} \FunctionTok{ymd}\NormalTok{(Date)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: Expecting date in A4 / R4C1: got 'Period'
\end{verbatim}

Names in the BIS file are very long so kludge them to something a bit
more readable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nn  }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"United Kingdom {-} "}\NormalTok{, }\StringTok{""}\NormalTok{, }\FunctionTok{names}\NormalTok{(totcredit))}
\NormalTok{nn  }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{" {-} Adjusted for breaks"}\NormalTok{, }\StringTok{""}\NormalTok{, nn)}
\NormalTok{nn  }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{" {-} Percentage of GDP"}\NormalTok{, }\StringTok{""}\NormalTok{, nn)}
\NormalTok{nn  }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{" at Market value"}\NormalTok{, }\StringTok{""}\NormalTok{, nn)}

\NormalTok{totcredit }\OtherTok{\textless{}{-}}\NormalTok{ totcredit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename\_with}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nn)}
\end{Highlighting}
\end{Shaded}

\hypertarget{plots-of-pivoted-data}{%
\subsection{Plots of pivoted data}\label{plots-of-pivoted-data}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dd }\OtherTok{\textless{}{-}}\NormalTok{ totcredit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols=}\SpecialCharTok{{-}}\NormalTok{Date) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{value =} \FunctionTok{as.numeric}\NormalTok{(value)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(value))}

\FunctionTok{ggplot}\NormalTok{(dd) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Date, }\AttributeTok{y=}\NormalTok{value, }\AttributeTok{colour=}\NormalTok{name), }\AttributeTok{show.legend=}\ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ name, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{""}\NormalTok{, }\AttributeTok{y=}\StringTok{""}\NormalTok{, }\AttributeTok{title=}\StringTok{"Credit data; all as percentage of GDP"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{GDP_files/figure-pdf/plotd-1.pdf}

}

\end{figure}

\hypertarget{difference-data-at-required-interval}{%
\subsection{Difference data at required
interval}\label{difference-data-at-required-interval}}

We will use some measure of long run credit growth as a predictor of
financial fragility. Pick an interval -- we choose five years -- and
calculate the growth rate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lagv }\OtherTok{\textless{}{-}} \DecValTok{20}

\NormalTok{dd2 }\OtherTok{\textless{}{-}}\NormalTok{ dd }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(name) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{value =} \DecValTok{100}\SpecialCharTok{*}\NormalTok{(value}\SpecialCharTok{/}\FunctionTok{lag}\NormalTok{(value,lagv)}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{() }

\FunctionTok{ggplot}\NormalTok{(dd2) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Date, }\AttributeTok{y=}\NormalTok{value, }\AttributeTok{colour=}\NormalTok{name), }\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ name, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{""}\NormalTok{,}\AttributeTok{y=}\StringTok{""}\NormalTok{, }\AttributeTok{title=}\FunctionTok{paste}\NormalTok{(}\StringTok{"Credit data; Percentage difference over"}\NormalTok{, lagv, }\StringTok{"quarters"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{GDP_files/figure-pdf/diff-1.pdf}

}

\end{figure}

\hypertarget{choose-a-variable}{%
\subsection{Choose a variable}\label{choose-a-variable}}

We select the variable we want, plot it to check, and then create a data
set to use in the quantile regression. What are the variables?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Recall all the names are in nn}
\FunctionTok{print}\NormalTok{(nn)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Date"                                                          
[2] "Credit to Non financial sector from All sectors"               
[3] "Credit to General government from All sectors"                 
[4] "Credit to General government from All sectors at Nominal value"
[5] "Credit to Households and NPISHs from All sectors"              
[6] "Credit to Non-financial corporations from All sectors"         
[7] "Credit to Private non-financial sector from All sectors"       
[8] "Credit to Private non-financial sector from Banks, total"      
\end{verbatim}

Let's go for number 7.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dd2 }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(dd2, name }\SpecialCharTok{==}\NormalTok{ nn[}\DecValTok{7}\NormalTok{]) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(Date, value) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename\_with}\NormalTok{(}\SpecialCharTok{\textasciitilde{}} \FunctionTok{c}\NormalTok{(}\StringTok{"Date"}\NormalTok{, }\StringTok{"GCredit"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Date =} \FunctionTok{floor\_date}\NormalTok{(Date, }\AttributeTok{unit=}\StringTok{"quarter"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(Date)}

\CommentTok{\# Quick plot to check we have the right one}
\FunctionTok{ggplot}\NormalTok{(dd2) }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Date, }\AttributeTok{y=}\NormalTok{GCredit), }\AttributeTok{color =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{GDP_files/figure-pdf/clagb-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dataz }\OtherTok{\textless{}{-}} \FunctionTok{left\_join}\NormalTok{(qdata, dd2, }\AttributeTok{by=}\StringTok{"Date"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{GCredit\_1 =} \FunctionTok{lag}\NormalTok{(GCredit,}\DecValTok{1}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{GCredit\_4 =} \FunctionTok{lag}\NormalTok{(GCredit,}\DecValTok{4}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{drop\_na}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{year}\NormalTok{(Date)}\SpecialCharTok{\textless{}}\DecValTok{2021}\NormalTok{)}

\FunctionTok{head}\NormalTok{(dataz)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        Date Growth Growth_1 Growth_4    GCredit  GCredit_1  GCredit_4
1 1969-01-01    1.7      5.7      6.1 -0.8333333 14.9905123 11.1913357
2 1969-04-01    2.9      1.7      4.2  5.1107325 -0.8333333 17.3674589
3 1969-07-01    1.5      2.9      5.8  5.2173913  5.1107325 16.1410019
4 1969-10-01    1.6      1.5      5.7  5.6939502  5.2173913 14.9905123
5 1970-01-01    1.1      1.6      1.7 -7.4960128  5.6939502 -0.8333333
6 1970-04-01    2.8      1.1      2.9 -3.7277147 -7.4960128  5.1107325
\end{verbatim}

\hypertarget{equation-and-estimates}{%
\section{Equation and estimates}\label{equation-and-estimates}}

Run a single quantile regression. We will do just one, and then look at
in-sample predictions. Really we ought to do this recursively. A couple
of parameters let us choose bits of the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fcast }\OtherTok{\textless{}{-}} \DecValTok{4}
\NormalTok{inccg }\OtherTok{\textless{}{-}} \DecValTok{1}

\ControlFlowTok{if}\NormalTok{ (inccg }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) \{}
\NormalTok{  eqn.q }\OtherTok{\textless{}{-}} \FunctionTok{formula}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"Growth \textasciitilde{} Growth\_"}\NormalTok{, fcast, }\StringTok{" + GCredit\_"}\NormalTok{, fcast))}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
\NormalTok{  eqn.q }\OtherTok{\textless{}{-}} \FunctionTok{formula}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"Growth \textasciitilde{} Growth\_"}\NormalTok{, fcast))  }
\NormalTok{\}}

\NormalTok{qvals  }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(.}\DecValTok{05}\NormalTok{,.}\DecValTok{95}\NormalTok{,.}\DecValTok{025}\NormalTok{)}
\NormalTok{q.inst }\OtherTok{\textless{}{-}} \FunctionTok{rq}\NormalTok{(eqn.q, }\AttributeTok{data=}\NormalTok{dataz, }\AttributeTok{tau=}\NormalTok{qvals)}
\CommentTok{\# summary(q.inst)}
\NormalTok{q.inst}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:
rq(formula = eqn.q, tau = qvals, data = dataz)

Coefficients:
             tau= 0.050  tau= 0.075  tau= 0.100   tau= 0.125  tau= 0.150
(Intercept) -3.60730272 -2.65975882 -2.03373503 -0.283117717  0.08723187
Growth_4    -0.01349979 -0.01961265  0.14950306  0.326307071  0.29884421
GCredit_4    0.04892147  0.02966876  0.02880223 -0.006781692 -0.01380020
             tau= 0.175  tau= 0.200  tau= 0.225  tau= 0.250  tau= 0.275
(Intercept)  0.52998577  0.67211457  0.78265307  0.87493621  0.96717528
Growth_4     0.23938813  0.26226668  0.28785310  0.29513320  0.27729443
GCredit_4   -0.01400174 -0.01139684 -0.01308326 -0.01193709 -0.01211441
              tau= 0.300   tau= 0.325  tau= 0.350   tau= 0.375   tau= 0.400
(Intercept)  1.132454167  1.294438029  1.44292183  1.463810672  1.562583315
Growth_4     0.253116511  0.231861348  0.20268022  0.215751945  0.195380042
GCredit_4   -0.009604268 -0.006338726 -0.00567619 -0.002420902 -0.001360645
              tau= 0.425   tau= 0.450    tau= 0.475   tau= 0.500    tau= 0.525
(Intercept)  1.663627576 1.6862397478  1.9086308843  2.039651688  2.0709412090
Growth_4     0.177560201 0.1771913047  0.1460039958  0.158547435  0.1639409996
GCredit_4   -0.000887589 0.0004844992 -0.0009085638 -0.003658792 -0.0005871492
             tau= 0.550  tau= 0.575  tau= 0.600   tau= 0.625 tau= 0.650
(Intercept) 2.120989585 2.175379958 2.375690044  2.548885682  2.6555717
Growth_4    0.161292582 0.170752556 0.142450983  0.127591069  0.1586058
GCredit_4   0.002333639 0.000429499 0.002191878 -0.001498171 -0.0024793
              tau= 0.675   tau= 0.700   tau= 0.725  tau= 0.750  tau= 0.775
(Intercept)  2.691126204 2.7527507431  2.886669318 2.934754152 2.999541321
Growth_4     0.192269383 0.1886024512  0.178036916 0.170590019 0.160463858
GCredit_4   -0.000934141 0.0006228696 -0.002012307 0.001794157 0.001253084
              tau= 0.800   tau= 0.825   tau= 0.850   tau= 0.875  tau= 0.900
(Intercept)  3.130746672  3.433999347  3.672727571  3.840716859 4.009738583
Growth_4     0.217223769  0.195635922  0.189628209  0.242774436 0.274378382
GCredit_4   -0.003218895 -0.008257329 -0.008929732 -0.000484291 0.005502021
              tau= 0.925  tau= 0.950
(Intercept) 4.3225662301 4.475711862
Growth_4    0.3251010478 0.357499115
GCredit_4   0.0004444649 0.001945412

Degrees of freedom: 208 total; 205 residual
\end{verbatim}

\hypertarget{non-parametric-estimated-quantiles}{%
\subsection{Non-parametric estimated
quantiles}\label{non-parametric-estimated-quantiles}}

We can easily plot the estimated quantiles as ridgeline plots, see Wilke
(2020). First we retrieve and then organize the predicted values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{q.predict }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(}\FunctionTok{predict}\NormalTok{(q.inst)) }\SpecialCharTok{\%\textgreater{}\%}           \CommentTok{\# In{-}sample predictions}
  \FunctionTok{as\_tibble}\NormalTok{(}\AttributeTok{.name\_repair =} \SpecialCharTok{\textasciitilde{}} \FunctionTok{as.character}\NormalTok{(dataz}\SpecialCharTok{$}\NormalTok{Date)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{q =}\NormalTok{ qvals) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols=}\SpecialCharTok{{-}}\NormalTok{q, }\AttributeTok{names\_to=}\StringTok{"Date"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Date =} \FunctionTok{ymd}\NormalTok{(Date)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{year}\NormalTok{(Date) }\SpecialCharTok{\textgreater{}} \DecValTok{2012}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next we plot them as a non-parametric estimate of the cumulative density
in a ridgeline plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sc }\OtherTok{\textless{}{-}} \DecValTok{1000}

\NormalTok{q.predict }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_ridgeline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{value, }\AttributeTok{height=}\NormalTok{q, }\AttributeTok{y=}\NormalTok{Date, }\AttributeTok{group=}\NormalTok{Date, }\AttributeTok{scale=}\NormalTok{sc, }\AttributeTok{fill=}\FunctionTok{as.factor}\NormalTok{(Date)),  }
                          \AttributeTok{colour=}\ConstantTok{NA}\NormalTok{, }\AttributeTok{show.legend=}\ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_cyclical}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"orange"}\NormalTok{, }\StringTok{"yellow"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{theme\_ridges}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{""}\NormalTok{, }\AttributeTok{y=}\StringTok{""}\NormalTok{, }\AttributeTok{title =} \StringTok{"GDP@Risk: Non{-}parametric cumulative density estimates"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{GDP_files/figure-pdf/nonpara1-1.pdf}

}

\end{figure}

What about with the tail probabilities emphasized? Now can use
\texttt{geom\_ridgeline\_gradient}, where the fill is over the
continuous x-axis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{q.predict }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_ridgeline\_gradient}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{value, }\AttributeTok{height=}\NormalTok{q, }\AttributeTok{y=}\NormalTok{Date, }\AttributeTok{group=}\NormalTok{Date, }\AttributeTok{scale=}\NormalTok{sc, }
                              \AttributeTok{fill=}\FloatTok{0.5}\SpecialCharTok{{-}}\FunctionTok{abs}\NormalTok{(q}\FloatTok{{-}0.5}\NormalTok{)),  }
                          \AttributeTok{colour=}\StringTok{"grey77"}\NormalTok{, }\AttributeTok{show.legend=}\ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_viridis}\NormalTok{(}\AttributeTok{option=}\StringTok{"D"}\NormalTok{, }\AttributeTok{direction=}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{alpha=}\NormalTok{.}\DecValTok{67}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_ridges}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{""}\NormalTok{, }\AttributeTok{y=}\StringTok{""}\NormalTok{, }\AttributeTok{title =} \StringTok{"GDP@Risk: Non{-}parametric cumulative density estimates"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{GDP_files/figure-pdf/nonpara1x-1.pdf}

}

\end{figure}

There are important features. There's quite a long, messy left hand
tail, and the density looks often multi-modal, as the colors switch. A
way of tidying this up is to fit a parametric distribution and to treat
that as the actual distribution. This has implications of course. This
is what the original authors do.

\hypertarget{parametric-results}{%
\subsection{Parametric results}\label{parametric-results}}

We now fit a skew-t to the predicted quantiles, and then work with these
estimated densities afterwards.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitst }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(e, p, }\AttributeTok{q=}\NormalTok{qvals) \{}
  \FunctionTok{sum}\NormalTok{((p }\SpecialCharTok{{-}} \FunctionTok{qst}\NormalTok{(q, }\AttributeTok{xi=}\NormalTok{e[}\DecValTok{1}\NormalTok{], }\AttributeTok{omega=}\NormalTok{e[}\DecValTok{2}\NormalTok{], }\AttributeTok{alpha=}\NormalTok{e[}\DecValTok{3}\NormalTok{], }\AttributeTok{nu=}\NormalTok{(e[}\DecValTok{4}\NormalTok{])))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{  \}}

\CommentTok{\# fitsti \textless{}{-} function(e, p, q) \{}
\CommentTok{\#   ss \textless{}{-} rep(0, 30)}
\CommentTok{\#   for (i in 1:30) \{}
\CommentTok{\#     ss[i] \textless{}{-} optim(e, fitsti, gr=NULL, p, q)}
\CommentTok{\#   \}}
\CommentTok{\#   e[4] \textless{}{-} which(ss==min(ss))}
\CommentTok{\#   \}}

\NormalTok{dens }\OtherTok{\textless{}{-}} \ConstantTok{NULL} \CommentTok{\# Store densities}
\NormalTok{eall }\OtherTok{\textless{}{-}} \ConstantTok{NULL} \CommentTok{\# Store estimated parameters}

\NormalTok{x   }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{,}\DecValTok{7}\NormalTok{,}\FloatTok{0.05}\NormalTok{)                        }\CommentTok{\# Evaluate fitted density over this interval}

\CommentTok{\# sel \textless{}{-} c(0.05, 0.15, 0.25, 0.36, 0.45, 0.5, 0.55, 0.65, 0.75, 0.85, 0.95)}
\CommentTok{\# sel \textless{}{-} c(0.05, 0.25, 0.75, 0.95)}
\NormalTok{sel }\OtherTok{\textless{}{-}}\NormalTok{ qvals}

\NormalTok{dte  }\OtherTok{\textless{}{-}}\NormalTok{ q.predict}\SpecialCharTok{$}\NormalTok{Date[}\DecValTok{88}\NormalTok{]}
\NormalTok{kvar }\OtherTok{\textless{}{-}} \FloatTok{0.1}
\ControlFlowTok{for}\NormalTok{ (dte }\ControlFlowTok{in} \FunctionTok{unique}\NormalTok{(q.predict}\SpecialCharTok{$}\NormalTok{Date)) \{}

\NormalTok{  pp  }\OtherTok{\textless{}{-}}\NormalTok{ q.predict }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{filter}\NormalTok{(Date}\SpecialCharTok{==}\NormalTok{dte) }\CommentTok{\# Predicted vals for i}
  
\NormalTok{  p  }\OtherTok{\textless{}{-}}\NormalTok{ pp}\SpecialCharTok{$}\NormalTok{value}
\NormalTok{  q  }\OtherTok{\textless{}{-}}\NormalTok{ pp}\SpecialCharTok{$}\NormalTok{q}
\NormalTok{  e0 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(p[q}\SpecialCharTok{==}\FloatTok{0.5}\NormalTok{], }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
  
\NormalTok{  fst }\OtherTok{\textless{}{-}} \FunctionTok{optim}\NormalTok{(e0, fitst, }\AttributeTok{gr=}\ConstantTok{NULL}\NormalTok{, p[q }\SpecialCharTok{\%in\%}\NormalTok{ sel], sel, }
               \AttributeTok{method =} \StringTok{"L{-}BFGS{-}B"}\NormalTok{,}
               \AttributeTok{lower=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{20}\NormalTok{, }\DecValTok{0}\NormalTok{,  }\SpecialCharTok{{-}}\ConstantTok{Inf}\NormalTok{, }\DecValTok{1}\NormalTok{),}
               \AttributeTok{upper=}\FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{,  }\ConstantTok{Inf}\NormalTok{, }\ConstantTok{Inf}\NormalTok{, }\DecValTok{30}\NormalTok{),}
               \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{factr=}\FloatTok{1e4}\NormalTok{))}
\NormalTok{  e   }\OtherTok{\textless{}{-}}\NormalTok{ fst}\SpecialCharTok{$}\NormalTok{par                                                    }\CommentTok{\# Fitted values}
\NormalTok{  y   }\OtherTok{\textless{}{-}} \FunctionTok{dst}\NormalTok{(x,    }\AttributeTok{xi=}\NormalTok{e[}\DecValTok{1}\NormalTok{], }\AttributeTok{omega=}\NormalTok{e[}\DecValTok{2}\NormalTok{], }\AttributeTok{alpha=}\NormalTok{e[}\DecValTok{3}\NormalTok{], }\AttributeTok{nu=}\NormalTok{(e[}\DecValTok{4}\NormalTok{]))   }\CommentTok{\# Fitted density}
\NormalTok{  vr  }\OtherTok{\textless{}{-}} \FunctionTok{qst}\NormalTok{(kvar, }\AttributeTok{xi=}\NormalTok{e[}\DecValTok{1}\NormalTok{], }\AttributeTok{omega=}\NormalTok{e[}\DecValTok{2}\NormalTok{], }\AttributeTok{alpha=}\NormalTok{e[}\DecValTok{3}\NormalTok{], }\AttributeTok{nu=}\NormalTok{(e[}\DecValTok{4}\NormalTok{]))   }\CommentTok{\# k\% quantile}
\NormalTok{  dr  }\OtherTok{\textless{}{-}} \FunctionTok{dst}\NormalTok{(vr,   }\AttributeTok{xi=}\NormalTok{e[}\DecValTok{1}\NormalTok{], }\AttributeTok{omega=}\NormalTok{e[}\DecValTok{2}\NormalTok{], }\AttributeTok{alpha=}\NormalTok{e[}\DecValTok{3}\NormalTok{], }\AttributeTok{nu=}\NormalTok{(e[}\DecValTok{4}\NormalTok{]))   }\CommentTok{\# Density at that point}
  
\NormalTok{  dens }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(dens, }\FunctionTok{tibble}\NormalTok{(}\AttributeTok{x=}\NormalTok{x,   }
                                 \AttributeTok{y=}\NormalTok{y,  }
                                 \AttributeTok{Date=}\FunctionTok{as.Date}\NormalTok{(dte),}
                                 \AttributeTok{vr=}\NormalTok{vr, }
                                 \AttributeTok{dr=}\NormalTok{dr,}
                                 \AttributeTok{v=}\FunctionTok{as.numeric}\NormalTok{(x}\SpecialCharTok{\textgreater{}}\NormalTok{vr)))}
\NormalTok{  eall }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(eall, }\FunctionTok{tibble}\NormalTok{(}\AttributeTok{Date=}\FunctionTok{as.Date}\NormalTok{(dte), }\AttributeTok{xi=}\NormalTok{e[}\DecValTok{1}\NormalTok{], }\AttributeTok{omega=}\NormalTok{e[}\DecValTok{2}\NormalTok{], }\AttributeTok{alpha=}\NormalTok{e[}\DecValTok{3}\NormalTok{], }\AttributeTok{nu=}\NormalTok{(e[}\DecValTok{4}\NormalTok{])))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

The coefficients don't show an obvious pattern as we can see from the
plots below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eall }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Date) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Date, }\AttributeTok{y=}\NormalTok{value, }\AttributeTok{color=}\NormalTok{name), }\AttributeTok{show.legend=}\ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{""}\NormalTok{, }\AttributeTok{y=}\StringTok{""}\NormalTok{, }\AttributeTok{title=}\StringTok{"GDP@Risk: Fitted skew{-}t coefficients"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ name, }\AttributeTok{scales=}\StringTok{"free\_y"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{GDP_files/figure-pdf/eall-1.pdf}

}

\end{figure}

Plots of calculated densities is easy -- we need to scale them -- here
with the 10\% value as dots.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sc   }\OtherTok{\textless{}{-}} \DecValTok{1750}                                    \CommentTok{\# Scale factor}

\NormalTok{dens }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_ridgeline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{height=}\NormalTok{y, }\AttributeTok{y=}\NormalTok{Date, }\AttributeTok{group=}\NormalTok{Date), }\AttributeTok{colour=}\StringTok{"grey77"}\NormalTok{, }\AttributeTok{fill=}\StringTok{"slateblue1"}\NormalTok{, }\AttributeTok{scale=}\NormalTok{sc) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ . }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(Date, vr) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{unique}\NormalTok{(), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{vr,}\AttributeTok{y=}\NormalTok{Date), }\AttributeTok{color=}\StringTok{"red"}\NormalTok{, }\AttributeTok{size=}\FloatTok{1.1}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme\_ridges}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{""}\NormalTok{, }\AttributeTok{y=}\StringTok{""}\NormalTok{, }\AttributeTok{title =} \StringTok{"GDP@Risk: Fitted skew{-}t"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{GDP_files/figure-pdf/para2-1.pdf}

}

\end{figure}

We can do a lot better, and recreate the density shaded by quantiles
using \texttt{v}, calculated above. Note the variable \texttt{v} is zero
to the left of the 10\% value, and 1 otherwise. we can use this to shade
the areas using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cpt }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"Shade indicates "}\NormalTok{, kvar}\SpecialCharTok{*}\DecValTok{100}\NormalTok{,}\StringTok{"\%"}\NormalTok{)}

\NormalTok{dens }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_ridgeline\_gradient}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x, }\AttributeTok{height=}\NormalTok{y, }\AttributeTok{y=}\NormalTok{Date, }\AttributeTok{group=}\NormalTok{Date, }\AttributeTok{scale=}\NormalTok{sc, }\AttributeTok{fill=}\FunctionTok{factor}\NormalTok{(v)),  }
                          \AttributeTok{colour=}\StringTok{"grey77"}\NormalTok{, }\AttributeTok{show.legend=}\ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_viridis\_d}\NormalTok{(}\AttributeTok{option=}\StringTok{"E"}\NormalTok{, }\AttributeTok{direction=}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{alpha=}\NormalTok{.}\DecValTok{67}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_ridges}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{""}\NormalTok{, }\AttributeTok{y=}\StringTok{""}\NormalTok{, }\AttributeTok{title=}\StringTok{"GDP@Risk: Fitted skew{-}t"}\NormalTok{, }\AttributeTok{caption=}\NormalTok{cpt)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{GDP_files/figure-pdf/para3-1.pdf}

}

\end{figure}

Cool, huh?

\hypertarget{gibbs-sampling}{%
\chapter{Gibbs Sampling}\label{gibbs-sampling}}

\hypertarget{gibbs-sampling-in-a-table}{%
\section{Gibbs sampling in a table}\label{gibbs-sampling-in-a-table}}

Think about a the joint density of some parameters of interest for a
discrete distribution:

\begin{table}
\centering
\begin{tabular}{>{}l|>{}c|>{}c|>{}c|>{}c}
\hline
  & σ₁ & σ₂ & σ₃ & Marginal β\\
\hline
\cellcolor{white}{β₁} & \cellcolor{pink}{0.10} & \cellcolor{pink}{0.20} & \cellcolor{pink}{0.30} & \cellcolor{wheat}{0.6}\\
\hline
\cellcolor{white}{β₂} & \cellcolor{pink}{0.10} & \cellcolor{pink}{0.05} & \cellcolor{pink}{0.05} & \cellcolor{wheat}{0.2}\\
\hline
\cellcolor{white}{β₃} & \cellcolor{pink}{0.05} & \cellcolor{pink}{0.10} & \cellcolor{pink}{0.05} & \cellcolor{wheat}{0.2}\\
\hline
\cellcolor{white}{Marginal σ} & \cellcolor{wheat}{0.25} & \cellcolor{wheat}{0.35} & \cellcolor{wheat}{0.40} & \cellcolor{wheat}{1.0}\\
\hline
\end{tabular}
\end{table}

Table 1 Joint probabilities

For this discrete example, the sums across the columns and rows are the
\textbf{marginal} densities for the parameters: they don't depend on the
other parameter. Gibbs sampling estimates these sums by constructing
sequences from the \textbf{conditional} densities that have the right
joint density.

\hypertarget{the-gibbs-sampler}{%
\section{The Gibbs' Sampler}\label{the-gibbs-sampler}}

Suppose there are \(k\) variables \(\phi_i\) jointly distributed \[
   J(\phi_1,\phi_2,...,\phi_k)
\] For inferential purposes we are interested in the marginal
distributions denoted \[
    G(\phi_i),\quad i=1,...,k
\] Gibbs sampling is a technique that generates a sequence of values
that have the same distribution as the underlying marginals. It doesn't
use the joint density but instead a sequence of conditionals densities
\[
    H(\phi_i|\Phi_{j\ne i}),\quad i=1,...,k
\] where \(\Phi_{j\ne i}\) are all other parameters. We will look at the
procedure first and then at how it works in an example.

\hypertarget{gibbs-sampling-is-the-following-steps}{%
\section{Gibbs sampling is the following
steps}\label{gibbs-sampling-is-the-following-steps}}

\begin{itemize}
\item
  \textbf{Step 0} Set starting values for \(\phi_1,...,\phi_k\) \[
    \phi_1^0,\ \phi_2^0,\ ...,\ \phi_k^0
  \]
\item
  \textbf{Step 1} Sample \(\phi_1^1\) from \[
     H(\phi_1^1\ |\ \phi_2^0,\ \phi_3^0,\ ...,\ \phi_k^0) 
  \]
\item
  \textbf{Step 2} Sample \(\phi_2^1\) from \[    
    \begin{gather*}
    H(\phi_2^1\ |\ \phi_1^1,\ \phi_3^0,\ ...,\ \phi_k^0)  \\
    \vdots 
    \end{gather*}
  \]
\item
  \textbf{Step \(k\)} Sample \(\phi_k^1\) from \[
    H(\phi_k^1\ |\ \phi_1^1,\ \phi_2^1,\ ...,\ \phi_{k-1}^1) 
  \] to complete one iteration.
\end{itemize}

Repeat for \(n\) iterations and save the last \(n-p\) values of
\(\phi_i^j\) for every \(i=1,...,k\). As \(n \rightarrow \infty\) the
joint and marginal distributions of the simulated
\(\phi_1^j,\ ...,\ \phi_k^j\) converge at an exponential rate to the
joint and marginal distributions of \(\phi_1,\ ...,\ \phi_k\). Then the
joint and marginal distributions can be approximated by the empirical
distribution.

For example, the estimated mean of the marginal distribution of
\(\phi_i\) is \[
   \bar \phi_i = \frac{\sum_{j=p+1}^n \phi_i^j}{n-p}
\] where we discard the first \(p\) draws.

\hypertarget{example-linear-regression-model}{%
\chapter{Example: linear regression
model}\label{example-linear-regression-model}}

For the specific linear model \[
   y_t = \alpha + \beta_1 X_{1t} + \beta_2 X_{2t}+v_t\text{, }v_t\sim N(0,\sigma^2)
\]

\begin{itemize}
\item
  \textbf{Step 1} Set priors for \(\sigma^2\) and
  \(\beta=\{\alpha, \beta_1, \beta_2\}\) \[
  P(\beta) \sim N\left( \underset{\beta_{0}}{\left[ 
  \begin{array}{c}
  \alpha^0 \\ 
  \beta_1^0 \\ 
  \beta_2^0
  \end{array}
  \right],}\underset{\Sigma_0} {\left[ 
  \begin{array}{ccc}
  \Sigma_{\alpha} & 0 & 0 \\ 
  0 & \Sigma_{\beta_1} & 0 \\ 
  0 & 0 & \Sigma_{\beta_2}
  \end{array}
  \right] }\right)
  \] \[
  P(\sigma^2) \sim \Gamma^{-1}\left( \frac{T_{0}}{2},\frac{\theta_0}{2}\right)
  \] and set starting values for e.g.~\(\alpha=\beta_1=\beta_2=0\),
  \(\sigma^2=1\)
\item
  \textbf{Step 2} Given \(\sigma^2\) sample \(\beta\) from its
  conditional posterior distribution \[
   H(\beta|\sigma^2) \sim N(M^*, V^*)
  \] where \[
  \begin{align}
  M^* &= \left(\Sigma_0^{-1} + \frac{1}{\sigma^2} X'X\right)^{-1} \left(\Sigma_0^{-1} \beta_0+\frac{1}{\sigma^2}X'y\right)\\
  V^* &= \left(\Sigma_0^{-1}+\frac{1}{\sigma^2} X'X\right)^{-1}
  \end{align}
  \] and \(X_t = \{\alpha, X_{1t}, X_{2t}\}\)

  \begin{itemize}
  \tightlist
  \item
    To sample a \(k\times 1\) vector \(b \sim N(M^*,V^*)\), generate
    \(k\) numbers \(z \sim N(0,1)\), scale by the square root of \(V^*\)
    and add in the mean \[
     b = M^* + \mbox{chol}(V^*) \times z
    \] where \(E[(b-M^*)(b-M^*)'] = V^*\)
  \end{itemize}
\item
  \textbf{Step 3} Given a draw of \(\beta\) (and call it \(\beta^1\))
  draw \(\sigma^2\) from its conditional distribution: \[
   H(\sigma^2 | \beta) \sim \Gamma^{-1}\left( \frac{T_0+T}{2}, \frac{\theta_0 + (y-X\beta^1)'(y-X\beta^1)}{2}\right) 
  \]

  \begin{itemize}
  \tightlist
  \item
    Sample some value \(s\) from an Inverse Gamma distribution
    \(\Gamma^{-1}(\frac{\tau}{2},\frac{\delta}{2})\), either from a
    suitable \(\Gamma^{-1}\)-distributed random number generator or
    generate \(\tau\) standard Normal-distributed numbers
    \(\varepsilon \sim N(0,1)\) and calculate \[
    s = \frac{\delta}{\varepsilon'\varepsilon}
    \]
  \item
    Setting \(\tau = T_0+T\) and \(\delta = \theta_0 + e'e\) is directly
    the exact conditional posterior
  \end{itemize}
\item
  \textbf{Step 4} Repeat Steps 2 and 3 \(n\) times and compute the
  posterior means using the last \(m\) draws (e.g.~repeat 5000 times and
  save the last 1000 draws)
\end{itemize}

\hypertarget{estimating-a-model-for-us-inflation}{%
\section{Estimating a model for US
inflation}\label{estimating-a-model-for-us-inflation}}

\begin{figure}

{\centering \includegraphics{Gibbs_files/figure-pdf/unnamed-chunk-2-1.pdf}

}

\end{figure}

\begin{itemize}
\tightlist
\item
  Choose an \(AR(4)\) \[
   \pi_t = \alpha + \beta_1 \pi_{t-1} + \beta_2 \pi_{t-2} + \beta_3 \pi_{t-3} + \beta_4 \pi_{t-4} + v_t\text{, }v_t\sim N(0,\sigma^2)
  \] and write
  \(\beta' = [\alpha\ \beta_1\ \beta_2\ \beta_3\ \beta_4]'\)
\item
  Priors are \[
  P(\beta) \sim N\left( \underset{\beta_0} {\left[ 
  \begin{array}{c}
  0 \\ 
  1 \\ 
  0 \\
  0 \\ 
  0
  \end{array}
  \right],}\ \underset{\Sigma} {\eta\left[ 
  \begin{array}{ccc}
  1 & 0 & 0 & 0 & 0\\ 
  0 & 1 & 0 & 0 & 0\\ 
  0 & 0 & 1 & 0 & 0\\
  0 & 0 & 0 & 1 & 0\\
  0 & 0 & 0 & 0 & 1
  \end{array}
  \right] }\right), \qquad
  P(\sigma^2) \sim \Gamma^{-1}\left( \frac{1}{2},\frac{1}{2}\right)
  \] where \(\eta\) is a scalar we will use to adjust prior tightness
\item
  Prior model consistent with a random walk for inflation
\end{itemize}

\begin{table}

\caption{Regression results}
\centering
\begin{tabular}[t]{lrrr}
\toprule
Coefficient & Estimate & SE & t-stat\\
\midrule
INF\_1 & 1.309 & 0.064 & 20.599\\
INF\_2 & -0.335 & 0.105 & -3.179\\
INF\_3 & 0.100 & 0.105 & 0.946\\
INF\_4 & -0.137 & 0.064 & -2.140\\
constant & 0.247 & 0.075 & 3.268\\
\addlinespace
sigma & 0.683 & NA & NA\\
\bottomrule
\end{tabular}
\end{table}

Linear regression table (n.b.~no priors)

\hypertarget{gibbs-samples}{%
\subsection{Gibbs samples}\label{gibbs-samples}}

\begin{figure}

{\centering \includegraphics{Gibbs_files/figure-pdf/unnamed-chunk-4-1.pdf}

}

\caption{Sample output for 2000 replications, long burn in}

\end{figure}

\begin{table}

\caption{Bayesian estimates}
\centering
\begin{tabular}[t]{lrrrr}
\toprule
Parameter & Expected & SE & lower\_5 & upper\_95\\
\midrule
alpha & 0.265 & 0.043 & 0.200 & 0.337\\
beta[1] & 1.260 & 0.125 & 1.040 & 1.448\\
beta[2] & -0.329 & 0.140 & -0.553 & -0.093\\
beta[3] & 0.093 & 0.107 & -0.077 & 0.253\\
beta[4] & -0.162 & 0.063 & -0.268 & -0.069\\
\addlinespace
sigma & 0.805 & 0.760 & 0.430 & 1.840\\
\bottomrule
\end{tabular}
\end{table}

Calculate descriptive statistics from Gibbs Samples

\begin{figure}

{\centering \includegraphics{Gibbs_files/figure-pdf/unnamed-chunk-6-1.pdf}

}

\caption{Gibbs sampling generates estimates of the full marginal
distributions of the parameters}

\end{figure}

We can look at the impact of tighter priors and more samples. What
should we expect?

\begin{itemize}
\tightlist
\item
  More samples will generate smoother-looking density plots, increase
  precison (maybe not by as much as you think)
\item
  Tighter priors will move estimates towards the prior: we can, say,
  reduce \(\eta\) or increase degrees of freedom for \(\sigma\)
\end{itemize}

\begin{figure}

{\centering \includegraphics{Gibbs_files/figure-pdf/unnamed-chunk-7-1.pdf}

}

\caption{Histograms/density estimates for 7000 replications, long burn
in}

\end{figure}

\begin{figure}

{\centering \includegraphics{Gibbs_files/figure-pdf/unnamed-chunk-8-1.pdf}

}

\caption{Histograms/density estimates for 7000 samples, \(\eta=.025\),
long burn in}

\end{figure}

\hypertarget{carter-kohn}{%
\chapter{Carter-Kohn}\label{carter-kohn}}

\hypertarget{using-the-kalman-filter}{%
\section{Using the Kalman Filter}\label{using-the-kalman-filter}}

Establish the usefulness of the Kalman Filter (and not just for state
estimation). Refresh idea of maximum likelihood estimation in the
context of state space models.

\begin{itemize}
\tightlist
\item
  Introduce \emph{smoothing}
\item
  Develop Gibbs sampling by the Carter-Kohn method
\item
  All of these use the Kalman Filter to develop conceptually different
  tools
\end{itemize}

Follow Kim and Nelson (1999); also see Harvey (1989), J. D. Hamilton
(1994), Durbin and Koopman (2001)

\hypertarget{maximum-likelihood}{%
\section{Maximum likelihood}\label{maximum-likelihood}}

\hypertarget{classical-maximum-likelihood-estimation}{%
\subsection{Classical Maximum Likelihood
Estimation}\label{classical-maximum-likelihood-estimation}}

The principle of maximum likelihood is that the parameters should be
chosen so that the probability of observing a given sample is maximized.

For time series models the joint density of
\(\psi_T = \{y_T, y_{T-1},\ldots ,y_1 \}\) and parameters \(\theta\) in
conditional form is \begin{equation}
 p(\theta|\psi_T) = \prod\nolimits_{t=1}^T p (y_t|\psi_{t-1},\theta)
\end{equation} emphasizing the serial dependence of observations.

Interpret this as the likelihood for a particular sample. Assuming
(conditional) normality, the likelihood of any particular \(n\)-vector
of observations is \begin{equation}
p(y_t) = (2\pi)^{-\frac{n}{2}}|var(y_t)|^{-\frac{1}{2}}e^{\left\{ -\frac{1}{2}(y_t-\mu )' var(y_t)^{-1}(y_t-\mu)\right\} }
\end{equation} Notice this depends on the observed data \emph{and} the
values of the parameters. It can be multivariate and for any underlying
density. A maximum likelihood (ML) estimate of \(\theta\) maximizes the
likelihood of the parameter given an observed sample.

\hypertarget{poisson-example}{%
\subsection{Poisson example}\label{poisson-example}}

Th Poisson distribution is a nice one to consider as the maximum
likelihood estimate can be calculated easily -- essentially in your
head.\footnote{For example Greene (1997) constructs a Poisson
  distribution example where the chosen observations yield an exact
  estimate of the underlying parameter of the distribution. The example
  is to find the most likely value of \(\theta\) given observations
  \begin{equation}
  5,\ 0,\ 1,\ 1,\ 0,\ 3,\ 2,\ 3,\ 4,\ 1
  \end{equation} ten observations which sum to 20.}

The Poisson distribution is \begin{equation}
 p(y_i,\ \theta )=\frac{e^{-\theta }\theta ^{y_i}}{y_i!}
\end{equation} for \(y>0\), zero otherwise with the property
\(E[Y]=var(Y)=\theta\).

Count variables often modelled as a random Poisson process: numbers of
road traffic accidents, sales, telephone calls, electron emissions.
Greene's example is to find the most likely value of \(\theta\) given
observations. \begin{equation}
5,\ 0,\ 1,\ 1,\ 0,\ 3,\ 2,\ 3,\ 4,\ 1
\end{equation} For independent observations the joint density is
\begin{equation}
p(y,\ \theta) =\prod_{i=1}^{10}p(y_i,\ \theta )
  = \frac{e^{-10\theta}\theta^{\sum_i y_i}}{\prod_i (y_i!)}
  = \frac{e^{-10\theta}\theta^{20}}{207,360}
\end{equation} We can plot this function to see if it has a maximum

\begin{figure}

{\centering \includegraphics[width=0.85\textwidth,height=\textheight]{CK_files/figure-pdf/dens-1.pdf}

}

\end{figure}

We can also find this by calculus. As ever, because the log function is
monotonic it is convenient to take logs \begin{equation}
   \ln L(\theta) = -10\theta + 20\ln\theta -\ln(207,360)
\end{equation} First order conditions are \begin{equation}
  \frac{\partial \ln L(\theta )}{\partial\theta} = -10+\frac{20}{\theta}
\Rightarrow \theta =\frac{20}{10}=2
\end{equation} Check for maximum \begin{equation}
   \frac{\partial^2\ln L(\theta )}{\partial\theta^2} = -\frac{20}{\theta^2} < 0
\end{equation}

\hypertarget{poisson-example-1}{%
\section{Poisson example}\label{poisson-example-1}}

The Poisson density for each observation is \begin{equation}
   p(y_i, \theta) = \frac{e^{-\theta} \theta^{y_i}}{y_i!}
\end{equation} for \(y>0\), zero otherwise, with \(E[Y]=var(Y)=\theta\).
For \(n\) independent observations, joint density is \begin{equation}
P(y, \theta) = \prod_{i=1}^n p(y_i, \theta)
  = \frac{e^{-n\theta}\theta^{\sum_i y_i}}{\prod_i (y_i!)}
\end{equation} Interpret this as a likelihood function, i.e.~a
probability measure for \(\theta\) given some observed \(y\)
\begin{equation}
 L(\theta | y) = P(y,\theta)
\end{equation} As log function is monotonic \begin{equation}
\ln L(\theta | y) = -n\theta + {\textstyle{\sum_i} y_i} \ln\theta  - \ln\left(\textstyle{\prod_i} y_i! \right)
\end{equation} First order conditions are \begin{equation}
  \frac{\partial \ln L(\theta|y)}{\partial\theta}
     = -n + \frac{\sum_i y_i}{\theta}
     \Rightarrow
        \theta =\frac{\sum_i y_i}{n}
\end{equation} Finally, check for maximum \begin{equation}
 \frac{\partial^2\ln L(\theta)}{(\partial\theta)^2} = -\frac{\sum_i y_i}{\theta^2} < 0
\end{equation}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(scales)}

\NormalTok{reps   }\OtherTok{\textless{}{-}} \DecValTok{20}
\NormalTok{lambda }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{n      }\OtherTok{\textless{}{-}} \DecValTok{10}

\NormalTok{a }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{x=}\FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{3}\SpecialCharTok{*}\NormalTok{lambda),}
            \AttributeTok{p=}\NormalTok{(}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{lambda}\SpecialCharTok{*}\NormalTok{x)}\SpecialCharTok{*}\NormalTok{(lambda}\SpecialCharTok{\^{}}\NormalTok{x))}\SpecialCharTok{/}\FunctionTok{prod}\NormalTok{(}\FunctionTok{factorial}\NormalTok{(lambda)))}

\NormalTok{d }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rpois}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{reps,lambda), n, reps)}

\NormalTok{t }\OtherTok{\textless{}{-}} \FunctionTok{as\_tibble}\NormalTok{(}\FunctionTok{factorial}\NormalTok{(d), }\AttributeTok{.name\_repair =} \SpecialCharTok{\textasciitilde{}}\FunctionTok{paste0}\NormalTok{(}\StringTok{"Rep"}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\NormalTok{reps)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{everything}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(name) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{dd =} \FunctionTok{prod}\NormalTok{(value)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{sum  =} \FunctionTok{colSums}\NormalTok{(d),}
         \AttributeTok{ests =}\NormalTok{ sum}\SpecialCharTok{/}\NormalTok{n)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(t) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{ests), }\AttributeTok{fill=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{alpha=}\NormalTok{.}\DecValTok{33}\NormalTok{, }\AttributeTok{color=}\ConstantTok{NA}\NormalTok{, }\AttributeTok{bins=}\DecValTok{20}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.85\textwidth,height=\textheight]{CK_files/figure-pdf/ests-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theta }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{y     }\OtherTok{\textless{}{-}}\NormalTok{ (}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{lambda)}\SpecialCharTok{*}\NormalTok{lambda}\SpecialCharTok{\^{}}\NormalTok{(theta))}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{factorial}\NormalTok{(theta))}
\NormalTok{df    }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{theta =}\NormalTok{ theta) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{lambda == 1}\StringTok{\textasciigrave{}} \OtherTok{=}\NormalTok{ (}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}\SpecialCharTok{*}\DecValTok{1}\SpecialCharTok{\^{}}\NormalTok{(theta))}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{factorial}\NormalTok{(theta)),}
         \StringTok{\textasciigrave{}}\AttributeTok{lambda == 2}\StringTok{\textasciigrave{}} \OtherTok{=}\NormalTok{ (}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{*}\DecValTok{2}\SpecialCharTok{\^{}}\NormalTok{(theta))}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{factorial}\NormalTok{(theta)),}
         \StringTok{\textasciigrave{}}\AttributeTok{lambda == 3}\StringTok{\textasciigrave{}} \OtherTok{=}\NormalTok{ (}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{)}\SpecialCharTok{*}\DecValTok{3}\SpecialCharTok{\^{}}\NormalTok{(theta))}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{factorial}\NormalTok{(theta)))}

\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \SpecialCharTok{{-}}\NormalTok{theta, }\AttributeTok{names\_to =} \StringTok{"lambda"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{theta, }\AttributeTok{y=}\NormalTok{value, }\AttributeTok{color=}\NormalTok{lambda), }\AttributeTok{size=}\DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{theta, }\AttributeTok{y=}\NormalTok{value, }\AttributeTok{color=}\NormalTok{lambda)) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Poisson density example"}\NormalTok{, }\AttributeTok{x=}\StringTok{""}\NormalTok{, }\AttributeTok{y=}\StringTok{""}\NormalTok{,}
       \AttributeTok{color=}\FunctionTok{expression}\NormalTok{(lambda)) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{hjust =} \FloatTok{0.5}\NormalTok{),}
        \AttributeTok{plot.subtitle =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{hjust =} \FloatTok{0.5}\NormalTok{),}
        \AttributeTok{legend.position =} \FunctionTok{c}\NormalTok{(.}\DecValTok{8}\NormalTok{,.}\DecValTok{8}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{scale\_colour\_discrete}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{parse\_format}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=0.85\textwidth,height=\textheight]{CK_files/figure-pdf/ests-2.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{e }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\NormalTok{l }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{5}\NormalTok{,}\FloatTok{0.05}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ (}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{n}\SpecialCharTok{*}\NormalTok{l)}\SpecialCharTok{*}\NormalTok{l}\SpecialCharTok{\^{}}\NormalTok{(}\FunctionTok{sum}\NormalTok{(e)))}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{prod}\NormalTok{(}\FunctionTok{factorial}\NormalTok{(e)))}

\FunctionTok{tibble}\NormalTok{(}\AttributeTok{l=}\NormalTok{l, }\AttributeTok{y=}\NormalTok{y) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{l, }\AttributeTok{y=}\NormalTok{y), }\AttributeTok{color=}\StringTok{"red"}\NormalTok{, }\AttributeTok{linewidth=}\DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Poisson density example"}\NormalTok{,}
       \AttributeTok{x=}\FunctionTok{expression}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Value of "}\NormalTok{, lambda)),}
       \AttributeTok{y=}\StringTok{"Joint density"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{hjust =} \FloatTok{0.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{CK_files/figure-pdf/unnamed-chunk-2-1.pdf}

}

\end{figure}

Maintain the value of \(\lambda\) of 2. Now generate \(reps=20\)
replications of \(n=10\) observations and plot the empirical density of
each and the maximum likelihood estimate for each replication.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t2 }\OtherTok{\textless{}{-}}\NormalTok{ t }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(name) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{r =} \FunctionTok{list}\NormalTok{((}\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{n}\SpecialCharTok{*}\NormalTok{l)}\SpecialCharTok{*}\NormalTok{l}\SpecialCharTok{\^{}}\NormalTok{sum)}\SpecialCharTok{/}\NormalTok{dd))}

\NormalTok{nms }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"Rep\_"}\NormalTok{, }\FunctionTok{str\_pad}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{reps, }\DecValTok{2}\NormalTok{, }\AttributeTok{pad=}\StringTok{"0"}\NormalTok{))}

\NormalTok{t2}\SpecialCharTok{$}\NormalTok{r }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{bind\_cols}\NormalTok{(}\AttributeTok{.name\_repair =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ nms) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{l =}\NormalTok{ l) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols =} \SpecialCharTok{{-}}\NormalTok{l) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{l, }\AttributeTok{y=}\NormalTok{value, }\AttributeTok{color=}\NormalTok{name), }\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept=}\NormalTok{lambda), }\AttributeTok{color=}\StringTok{"red"}\NormalTok{, }\AttributeTok{linetype=}\DecValTok{3}\NormalTok{, }\AttributeTok{linewidth=}\FloatTok{0.75}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ name, }\AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title    =} \FunctionTok{paste}\NormalTok{(}\StringTok{"Poisson density, sample size"}\NormalTok{, n),}
       \AttributeTok{subtitle =} \FunctionTok{expression}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Dotted line: true value of "}\NormalTok{, lambda)),}
       \AttributeTok{x        =} \FunctionTok{expression}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Estimated value of "}\NormalTok{, lambda)),}
       \AttributeTok{y        =} \StringTok{"Joint density"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{plot.title =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{hjust =} \FloatTok{0.5}\NormalTok{),}
        \AttributeTok{plot.subtitle =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{hjust =} \FloatTok{0.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{CK_files/figure-pdf/unnamed-chunk-3-1.pdf}

}

\end{figure}

\hypertarget{ml-and-regression}{%
\subsection{ML and regression}\label{ml-and-regression}}

Linear regression problem is \begin{equation}
L(\beta|y,X) = \frac{1}{(2\pi \sigma^2)^{n/2}}\exp \left[ - \frac{1}{2\sigma^2} (y-X\beta)'(y-X\beta) \right]
\end{equation} The log-likelihood is \begin{equation}
   \ln L = -\frac{n}{2}\ln (2\pi)-\frac{n}{2}\ln (\sigma^2) - \frac{1}{2\sigma^2}(y-X\beta)'(y-X\beta)
\end{equation} As before, find the extremum by calculus; yields
\emph{likelihood equations} \begin{align}
\frac{\partial \ln L}{\partial \beta} & = - \frac{2}{2\sigma^2} (X'y-X'X\beta) = 0 \\
& \Rightarrow \hat{\beta}_{ml} = (X'X)^{-1}X'y
\end{align} and \begin{align}
\frac{\partial \ln L}{\partial \sigma^2} &=
   - \frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}   
     (y - X\beta)'(y - X\beta) = 0 \\
& \Rightarrow -n + \sigma^{-2} (\epsilon'\epsilon) = 0 \\
& \Rightarrow \hat{\sigma}_{ml}^2 = \frac{\hat{\epsilon}'\hat{\epsilon}}{n}
\end{align} ML estimate of \(\sigma^2\) divided by \(n\) (not \(n-k\))
so biased in small samples but not asymptotically

\hypertarget{kalman-filter-tricks}{%
\section{Kalman filter tricks}\label{kalman-filter-tricks}}

For some initial condition -- say \(\beta_0 \sim N(\mu_0,P_0))\) -- the
conditional log-likelihood for sample \(1\) to \(T\) \begin{align}
\log L(\psi_t|\theta) &= \sum\nolimits_{t=1}^T\log p(y_t|\psi_{t-1},\theta) \\
&\propto -\sum\nolimits_{t=1}^T\left( \log \left\vert f_{t|t-1}\right\vert +\eta_{t|t-1}' f_{t|t-1}^{-1}\eta_{t|t-1} |\ \theta \right)
\end{align} Note we could obtain \(\eta_{t|t-1}\) and \(f_{t|t-1}\) from
the Kalman filter, i.e.~ \begin{equation}
  f_{t|t-1} = (H_tP_{t|t-1}H_t' + Q) = \Sigma_{\eta\eta}
\end{equation} This is the \emph{prediction error decomposition} of the
log-likelihood. For a classical approach we estimate \(\theta\) by
numerically maximizing \(\log L(\psi_T|\theta)\).This gives a point
estimate for the value of \(\theta\) and we typically apply classical
inference using the estimated standard errors. Note to do this we need
to evaluate the best estimate of the state as well as maximize the
likelihood: the Kalman Filter is a key ingredient in both.

\begin{tcolorbox}[enhanced jigsaw, breakable, left=2mm, arc=.35mm, toptitle=1mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, bottomrule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Maximisation}, colframe=quarto-callout-note-color-frame, coltitle=black, titlerule=0mm, toprule=.15mm, bottomtitle=1mm, rightrule=.15mm, colback=white]

A suitable numerical maximization routine will (in principle) maximize
the likelihood straightforwardly. Often use
\href{http://www.princeton.edu/~sims/}{Chris Sims'}
\href{http://sims.princeton.edu/yftp/optimize/rfiles/csminwel.R}{\textbf{csminwel}}
in Matlab or R as well-suited to this type of problem.

\end{tcolorbox}

Can show (via Cramer-Rao) that \begin{equation}
\widehat{\theta}\sim N\left(\theta, -\frac{\partial^2 \log L(\psi_T|\theta)} {\partial\theta\partial\theta'} \right)
\end{equation}

\hypertarget{full-sample-estimates-of-beta_t}{%
\section{\texorpdfstring{Full sample estimates of
\(\beta_t\)}{Full sample estimates of \textbackslash beta\_t}}\label{full-sample-estimates-of-beta_t}}

\hypertarget{the-regression-lemma-again}{%
\subsection{The regression lemma
again}\label{the-regression-lemma-again}}

You may recall that for any \begin{equation}
\begin{bmatrix} z \\ y \\ \varepsilon \end{bmatrix}
\sim N\left(\begin{bmatrix} \mu_z \\ \mu_y \\ 0 \end{bmatrix},
\begin{bmatrix}
\Sigma_{zz} & \Sigma_{zy} & \Sigma_{z\varepsilon } \\
\Sigma_{yz} & \Sigma_{yy} & 0 \\
\Sigma_{\varepsilon z} & 0 & \Sigma_{\varepsilon\varepsilon}
\end{bmatrix}\right)
\end{equation} then it must be that \begin{align}
E[z|y,\varepsilon] &= \mu_z + \Sigma_{zy}\Sigma_{yy}^{-1}(y-\mu_y) + \Sigma_{z\varepsilon}\Sigma_{\varepsilon\varepsilon}^{-1}\varepsilon \\
&= E[z|y] + \Sigma_{z\varepsilon}\Sigma_{\varepsilon\varepsilon }^{-1}\varepsilon
\end{align} We use this to derive a recursive update to \emph{smooth}
our estimates. We will also derive an appropriate conditional
expectation which we can use in Gibbs sampling.

\hypertarget{smoothing}{%
\section{Smoothing}\label{smoothing}}

The Kalman filter estimates \(\beta_t\) recursively: it only uses
information available up until time \(t\). This means that the estimate
of \(\beta_{T|T}\) uses all available information, but any previous
estimate doesn't. Indeed there must be some values of \(\eta_{i|t-1}\)
\begin{equation}
   \beta_{t|T} = E(\beta_t | \psi_{t-1}, \eta_{t|t-1}, \eta_{t+1|t-1},...,\eta_{T|t-1})
\end{equation} where the `news' is relative to period \(t\).

We could update \(\beta_{t|t-1}\) using the (uncorrelated) future
innovations \begin{equation}
  \beta_{t|T}=\beta_{t|t}+\sum_{j=t}^T\Sigma_{\beta_t\eta_j}\Sigma_{\eta_j\eta_j}^{-1}\eta_{j|t-1}
\end{equation} and recalling
\(\beta_{t|t}=E(\beta_t|\psi_{t-1},\eta_{t|t-1})\).

This is a \emph{fixed interval smoother}; often used for full sample
estimates of \(\beta_t\). Remember we already have an estimate of
\(\beta_{T|T}\) from the Kalman filter so \emph{smoothers work
backwards}; we sketch a derivation here.

In the last but one period we have a different prediction error
\begin{equation}
   \varsigma_{T|T-1} = \beta_{T|T} - F\beta_{T-1|T-1} - \mu
\end{equation} which is the error in predicting \(\beta_T\) using
\(\psi_{T-1}\).

An `update' has to be of the form \begin{equation}
\beta_{T-1|T}=\beta_{T-1|T-1} + \Sigma_{\beta\varsigma}\Sigma_{\varsigma\varsigma}^{-1}\varsigma_{T|T-1}
\end{equation} where
\(\Sigma_{\varsigma\varsigma} = var\left[ \varsigma_T|\psi_{T-1}\right]\)
and
\(\Sigma_{\beta \varsigma}=cov\left[\beta_{T-1},\varsigma_T|\psi_{T-1}\right]\).

These are \begin{align}
\Sigma_{\varsigma\varsigma} &= var(\beta_T - F\beta_{T-1|T-1}-\mu) \\
  &= var\left(F(\beta_{T-1}-\beta_{T-1|T-1}) + e_t \right) \\
  &= F P_{T-1|T-1}F' + Q
\end{align} and \begin{align}
\Sigma_{\beta\varsigma} &= E\left[ (\beta_{T-1}-\beta_{T-1|T-1}) \left( \beta_T - F\beta_{T-1|T-1}-\mu \right)'
\right] \\
&= E\left[ (\beta_{T-1}-\beta_{T-1|T-1}) (\beta_T-\beta_{T-1|T-1})'\right] F' \\
&= P_{T-1|T-1}F'
\end{align} Plugging these definitions in gives us \begin{equation}
\beta_{T-1|T} = \beta_{T-1|T-1} + P_{T-1|T-1}F' P_{T|T-1}^{-1} (\beta_{T|T}-F\beta_{T-1|T-1}-\mu)
\end{equation} Applying the argument backward in time gives the
recursion \begin{align}
\beta_{t|T} &= \beta_{t|t}+P_{t|t} F'P_{t+1|t}^{-1} (\beta_{t+1|T}-F\beta_{t|t}-\mu) \\
&= \beta_{t|t} - K_{t|T} (\beta_{t+1|T}-F\beta_{t|t}-\mu) \tag{smooth}
\end{align} All these quantities are outputs of the Kalman filter so
smoothing is easy to implement.

The smoothed variance of \(\beta_{t|T}\) found by multiplying out
(smooth). To do this use \(\beta_{t+1|t} = \mu + F\beta_{t|t}\) so
rearranging gives \begin{equation}
  \widetilde{\beta}_{t|T} + K_{t|T}\beta_{t+1|T} = \widetilde{\beta}_{t|t}+K_{t|T}\beta_{t+1|t}
\end{equation} where \(\widetilde{\beta}_{t|t}=\beta_t-\beta_{t|t}\).
Now square both sides and take expectations \begin{equation}
  P_{t|T} + K_{t|T} E\left[\beta_{t+1|T}\beta_{t+1|T}' \right] K_{t|T}' = P_{t|t} + K_{t|T} E\left[ \beta_{t+1|t} \beta_{t+1|t}' \right] K_{t|T}'
\end{equation} Adding and subtracting \(E[\beta_{t+1}\beta_{t+1}']\) we
can show that \begin{equation}
-E\left[\beta_{t+1|T}\beta_{t+1|T}'\right] + E\left[\beta_{t+1|t}\beta_{t+1|t}'\right] = P_{t+1|T}-P_{t+1|t}
\end{equation} to obtain \begin{equation}
  P_{t|T} = P_{t|t} + K_{t|T} (P_{t+1|T}-P_{t+1|t}) K_{t|T}'.
\end{equation}

\hypertarget{kalman-filter-in-econometrics}{%
\section{Kalman filter in
econometrics}\label{kalman-filter-in-econometrics}}

\hypertarget{classical-approach}{%
\subsection{Classical approach}\label{classical-approach}}

The typical procedure is some variation on:

\begin{itemize}
\tightlist
\item
  Formulate state-space model
\item
  Estimate the model by maximum likelihood
\item
  Condition on the parameters to retrieve the (usually smoothed) state
  estimates and standard errors
\item
  Use Cramer-Rao to calculate the standard errors of any other parameter
  estimates
\end{itemize}

For this the Kalman filter is a useful tool, as it allows a great deal
of flexibility in the estimation of a variety of models, as is is an
appropriate tool for models with unobserved components. However, it must
be used with care: it is easy to try to estimate models that are
essentially unidentified.

Further useful tools

\begin{itemize}
\tightlist
\item
  The \emph{Extended Kalman filter} linearises the filter at every step
  and can be used for nonlinear models (such as ones where you need to
  estimate \(B_T\) and \(\theta\) simultaneously)
\item
  Increasingly non-Gaussian non-linear models are estimated using the
  \emph{particle filter}
\end{itemize}

\hypertarget{bayesian-approach}{%
\subsection{Bayesian approach}\label{bayesian-approach}}

Bayesian approach is to generate the entire distribution of the model
parameters.

\begin{itemize}
\tightlist
\item
  Now no longer just look for the point estimate obtained by maximum
  likelihood
\item
  Use Gibbs sampling or some other appropriate method applied to the
  state space model
\item
  In particular we treat the states and the parameters as jointly
  determined by the data
\item
  As the state is estimated we need a way to draw the states conditional
  on the other estimates to do Gibbs sampling
\item
  Seek a conditional updating algorithm that replicates the Gibbs
  sampling approach we have used before
\end{itemize}

We require a procedure such that:

\begin{itemize}
\tightlist
\item
  \textbf{Step 1} Conditional on \(\theta\) and the data, generate the
  sequence \(B_T = (\beta_1, \beta_2, \ldots, \beta_T)\)
\item
  \textbf{Step 2} Conditional on \(B_T\) and the data, generate values
  of \(\theta\)
\item
  \textbf{Step 3} Iterate previous two steps until convergence
\end{itemize}

In this way the joint distribution of the two can be obtained from the
resulting simulation.

\hypertarget{carter-kohn-algorithm}{%
\section{Carter-Kohn algorithm}\label{carter-kohn-algorithm}}

\begin{itemize}
\item
  \textbf{Step 2} above relatively easy but how do we generate a
  sequence of states?

  \begin{itemize}
  \tightlist
  \item
    The state estimates depend on the parameter value through the Kalman
    filter
  \end{itemize}
\item
  Appropriate algorithm designed by Carter and Kohn (1994)
\item
  Takes the form of a modified Kalman smoother
\item
  Known as \emph{multimove Gibbs sampling}
\item
  Similar to above, define
\end{itemize}

\begin{equation}
  B_t = \begin{bmatrix} \beta_1 & \beta_2 & \ldots & \beta_t \end{bmatrix}
\end{equation} so in particular \begin{equation}
B_{T-1} = \begin{bmatrix} \beta_1 & \beta_2 & \ldots & \beta_{T-1} \end{bmatrix}
\end{equation} consistent with out earlier definition of \(\psi_t\).

\begin{itemize}
\tightlist
\item
  Multimove Gibbs sampling generates the whole vector of states
  (\(B_T\)) at once
\item
  We therefore need to generate a realization of \(B_T\) given the
  probability distribution \(p( B_T|\psi_T)\)
\item
  We want to generate an appropriate conditional probability
  distribution \(p(\beta_t|B_{j\neq t}, \psi_T)\) to sample from for our
  Gibbs sampler
\item
  Just as for the Kalman smoother we use the outputs of the Kalman
  filter and a separate backward recursion to obtain the conditional
  distribution
\end{itemize}

\hypertarget{joint-distribution}{%
\section{Joint distribution}\label{joint-distribution}}

Deriving the appropriate distributions is easy if we know what to
condition on. The joint probability density function can be split into a
sequence of conditional distributions: \(p(B_T|\psi_T)\) can be written
recursively \begin{align}
p(B_T|\psi_T) &= p(\beta_T|\psi_T) \times p(B_{T-1}|\beta_T,\psi_T) \\
&= p(\beta_T|\psi_T) \times p(\beta_{T-1}|\beta_T,\psi_T) \times p(B_{T-2}|\beta_{T-1},\beta_T,\psi_T) \\
&= p(\beta_T|\psi_T) \times p(\beta_{T-1}|\beta_T,\psi_T) \times p(B_{T-2}|\beta_{T-1},\psi_T)
\end{align} Final simplification follows as the state vector is a Markov
chain so there is no information in \(\beta_T\) not contained in
\(\beta_{T-1}\) and \(\psi_T\). Further as soon as we know
\(\beta_{T-1}\) there is no information contained in \(\psi_T\) so we
can drop that, so \begin{align}
p(B_T|\psi_T) &= p(\beta_T|\psi_T)\times p(\beta_{T-1}|\beta_T, \psi_T) \times  p(B_{T-2}|\beta_{T-1},\psi_T) \\
&= p(\beta_T|\psi_T)\times p(\beta_{T-1}|\beta_T, \psi_{T-1}) \times p(B_{T-2}|\beta_{T-1},\psi_{T-2}) \\
&= p(\beta_T|\psi_T) \times \prod_{t=1}^{T-1} p(\beta_t|\beta_{t+1}, \psi_t)
\end{align}

\hypertarget{the-carter-kohn-equations}{%
\section{The Carter-Kohn equations}\label{the-carter-kohn-equations}}

The estimated \(\beta\) variables are distributed \begin{align}
\beta_{T|\psi_T} &\sim N(\beta_{T|T}, P_{T|T}) \\
\beta_{t|\psi_t, \beta_{t+1}} &\sim N(\beta_{t|t,\beta_{t+1}},P_{t|t,\beta_{t+1}})
\end{align} where \begin{align}
\beta_{t|t,\beta_{t+1}} &= E\left[\beta_t|\psi_t,\beta_{t+1}\right]
= E\left[\beta_t|\beta_{t|t},\beta_{t+1}\right] \\
P_{t|t,\beta_{t+1}} &= cov\left[\beta_t|\psi_t,\beta_{t+1}\right] = cov\left[\beta_t |{\beta_{t|t},\beta_{t+1}}\right]
\end{align} Carter-Kohn derive appropriate recursions so that, for
example, we update the state estimate conditioning on some known value
of \(\beta_{t+1}\) \begin{equation}
  \beta_{t|t,\beta_{t+1}} = \beta_{t|t}-K_{t|t+1} (\beta_{t+1}-F\beta_{t|t}-\mu)
\end{equation} Define \begin{equation}
   \varsigma_{t+1|t} = \beta_{t+1}-F\beta_{t|t}-\mu
\end{equation} as the `innovation' in predicted \(\beta_{t+1|t}\) where
we have some realized \(\beta_{t+1}\) drawn from its probability
distribution. The Carter-Kohn smoother comprises updates to the
conditional expectations that use this news. \begin{align}
E[\beta_t|\psi_t,\beta_{t+1}] &= E[\beta_t|\psi_t] + \Sigma_{\beta\varsigma} \Sigma_{\varsigma\varsigma}^{-1}\varsigma_{t+1|t} \\
&= \beta_{t|t} + \Sigma_{\beta\varsigma}\Sigma_{\varsigma\varsigma}^{-1}\varsigma_{t+1|t} \\
var[\beta_t|\psi_t,\beta_{t+1}] &= var[\beta_t|\psi_t] -\Sigma_{\beta\varsigma} \Sigma_{\varsigma\varsigma}^{-1} \Sigma_{\varsigma\beta} \\
&= P_{t|t} - \Sigma_{\beta\varsigma} \Sigma_{\varsigma\varsigma}^{-1} \Sigma_{\varsigma\beta} \\
&= P_{t|t,\beta_{t+1}}
\end{align} Both \(\beta_{t|t}\) and \(P_{t|t}\) are outputs of the
Kalman filter.

\hypertarget{deriving-sigma_betavarsigma-and-sigma_varsigmavarsigma}{%
\subsection{\texorpdfstring{Deriving \(\Sigma_{\beta\varsigma}\) and
\(\Sigma_{\varsigma\varsigma}\)}{Deriving \textbackslash Sigma\_\{\textbackslash beta\textbackslash varsigma\} and \textbackslash Sigma\_\{\textbackslash varsigma\textbackslash varsigma\}}}\label{deriving-sigma_betavarsigma-and-sigma_varsigmavarsigma}}

As before we just plug in the definitions so \begin{align}
\Sigma_{\varsigma\varsigma} &= var[\beta_{t+1}-F\beta_{t|t}-\mu] \\
&= var\left[ F\beta_t+\mu +v_{t+1}-F\beta_{t|t}-\mu \right] \\
&= var\left[ F(\beta_t-\beta_{t|t})+v_{t+1}\right] \\
&= F P_{t|t}F' + Q
\end{align} and \begin{align}
\Sigma_{\beta\varsigma} &= E\left[ (\beta_t - \beta_{t|t}) (\beta_{t+1}-F\beta_{t|t}-\mu)'\right] \\
&= E\left[ (\beta_t-\beta_{t|t})\left( F(\beta_t-\beta_{t|t})+v_{t+1}\right)'\right] \\
&= P_{t|t}F'
\end{align}

\hypertarget{kalman-gain-again}{%
\section{`Kalman gain' again}\label{kalman-gain-again}}

So using the definitions of the covariances and the regression lemma we
get \begin{align}
\beta_{t|t,\beta_{t+1}} &= \beta_{t|t} + \Sigma_{s\eta} \Sigma_{\eta\eta}^{-1}\varsigma_t \\
&= \beta_{t|t} + P_{t|t}F' \left( FP_{t|t}F' + Q\right)^{-1} (\beta_{t+1}-F\beta_{t|t}-\mu) \\
&= \beta_{t|t} - K_{t|t}(\beta_{t+1}-F\beta_{t|t}-\mu)
\end{align} where \begin{equation}
   K_{t|t+1} = - P_{t|t}F' (FP_{t|t}F' + Q)^{-1}
\end{equation} Like the Kalman smoother, this uses the filter's estimate
of \(P_{t|t}\) and updates \(\beta_t\) using the error in predicting
\(\beta_{t+1}\) not \(y_t\).

\hypertarget{conditional-mean-and-variance-of-the-state}{%
\subsection{Conditional mean and variance of the
state}\label{conditional-mean-and-variance-of-the-state}}

Updating equations for the state and variance obtained directly from the
regression lemma \begin{align}
\beta_{t|t,\beta_{t+1}} &= \beta_{t|t} - K_{t|t+1}\varsigma_{t+1|t} \\
    P_{t|t,\beta_{t+1}} &= P_{t|t}-P_{t|t}F'(FP_{t|t}F' + Q)^{-1}F P_{t|t}
\end{align} CK equations recursively evaluate these quantities
\emph{backwards} beginning from \(s_T\), \(P_T\) obtained from the
Kalman filter.

Generate appropriate conditional samples using \begin{equation}
  \beta_{t|t,\beta_{t+1}} \sim N\left(\beta_{t|t,\beta_{t+1}}, P_{t|t,\beta_{t+1}} \right)
\end{equation} to give \(B_T|\psi_T\). This is a conditional sample that
depends on a given parameter vector to use in a Gibbs sampling scheme
that draws those parameters in turn from distributions conditioned on
the states.

\hypertarget{state-space-gibbs-sampling-in-practice}{%
\section{State-space Gibbs sampling in
practice}\label{state-space-gibbs-sampling-in-practice}}

Above approach cannot be used explicitly if
\(\Sigma_{\varsigma\varsigma}\) is singular, for example if we have more
states than shocks (which is not uncommon). A simple modification given
in KN can deal with this; we treat only those states that are shocked as
observed.

In general we need conditional distributions for all the other
parameters to be estimated. Need to store the complete sequence of
states and covariances to implement the Gibbs sampler. We will
investigate the exact implementation of Gibbs sampling for state-space
models in the exercises.

\hypertarget{comparing-the-filters-and-smoothers}{%
\section{Comparing the filters and
smoothers}\label{comparing-the-filters-and-smoothers}}

\begin{alignat*}{3}
&\text{Filter} & &\text{Innovation} & & \text{Gain and state covariance}  \\
& && &&\\
&KF &\qquad\ &\eta_t = y_t-H_t\beta_{t|t-1} && K_{t|t} =-P_{t|t-1}H_t' (H_t P_{t|t-1} H_t' + R)^{-1} \\
& &&  && P_{t|t} = P_{t|t-1}-P_{t|t-1} H_t'(H_t P_{t|t-1} H_t' +R)^{-1} H_t P_{t|t-1} \\
&KS && \varsigma_t = \beta_{t+1|T}-F\beta_{t|t}-\mu & \qquad\ & K_{t|T}=-P_{t|t}F' P_{t+1|t}^{-1} \\
&   &&      && P_{t|T} = P_{t|t}+K_{t|T}(P_{t+1|T}-P_{t+1|t}) K_{t|T}' \\
&CK && \varsigma_t = \beta_{t+1}-F\beta_{t|t}-\mu && K_{t|t,\beta_{t+1}} = -P_{t|t}F' P_{t+1|t}^{-1} \\ 
&   &&   && P_{t|t,\beta_{t+1}} = P_{t|t}-P_{t|t}F'(FP_{t|t}F' + Q)^{-1}FP_{t|t}
\end{alignat*}

\hypertarget{metropolis-hastings}{%
\chapter{Metropolis-Hastings}\label{metropolis-hastings}}

\hypertarget{complicated-densities}{%
\section{Complicated densities}\label{complicated-densities}}

What if we have a posterior density that is too complicated to factor
into a full set of conditional densities? Recall that Gibbs Sampling is
a procedure that generates \emph{marginal densities} from
\emph{conditional densities}. What if we don't have conditional
densities?

\begin{tcolorbox}[enhanced jigsaw, breakable, left=2mm, arc=.35mm, toptitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, bottomrule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Why can't we use Gibbs for DSGEs?}, colframe=quarto-callout-tip-color-frame, coltitle=black, titlerule=0mm, toprule=.15mm, bottomtitle=1mm, rightrule=.15mm, colback=white]

Say a model only has two unknown parameters, \(\alpha\) and \(\gamma\).
Any predictions of that model conditional on them is: \begin{equation}
  y = f(X,\ \alpha,\ \gamma)
\end{equation} The likelihood of that model \begin{equation}
  \mathcal{L}(\alpha,\ \gamma\ | \ y,\ X)
\end{equation} is readily available but the conditional density of say,
\(\alpha\) \begin{equation}
  \mathcal{P}(\alpha\ | \ y,\ X,\ \gamma)
\end{equation} typically isn't. This is because the predictive model
\(f(\cdot)\) depends on all the parameters through the reduced form
solution used in DSGEs even if the underlying model is linear, i.e.~the
solution in (\textbf{BK1980?}). As \(f(X,\ \alpha,\ \gamma)\)
\textbf{\emph{always}} depends on both \(\alpha\) and \(\gamma\) it is
difficult to find an appropriate conditional likelihood which isolates
one of them.

\end{tcolorbox}

Can we derive a technique that generates marginal densities from a
\emph{joint density}? Turns out we (perhaps surprisingly) can, using the
\emph{Metropolis-Hastings algorithm}.

\hypertarget{target-density}{%
\subsection{Target density}\label{target-density}}

We'll derive the simplest posterior density that we can use in an
exercise, but we begin with a more general case. Consider a posterior
likelihood that is the product of a likelihood and \(k\) prior
densities, say \begin{equation}
 \mathcal{H}(\theta\ | \ y) = \mathcal{L}(\theta\ | \ y)\times \mathcal{P}_1(\theta_1)\times \mathcal{P}_2(\theta_2)\times \mathcal{P}_3(\theta_3)\times ...\times \mathcal{P}_k(\theta_k)
\end{equation} This is the \emph{target density}. How can we estimate
the densities of the underlying \(\theta_i\) from this posterior alone?
This (somewhat amazingly) turns out to be rather simple.

The trick is to simulate draws for all the elements of \(\theta\) from
the target density -- despite not having a generating function to draw
from the density -- and then estimating the marginal densities from the
resulting series.

\hypertarget{simplified-problem-estimating-known-prior}{%
\section{Simplified problem: estimating (known)
prior}\label{simplified-problem-estimating-known-prior}}

As the aim is to introduce MH in as simple a context as possible, we
will sample from a posterior distribution for which we don't have an
appropriate random number generator but for marginal distributions that
we do. This means we can compare analytical and estimated results.

We will try an estimate the marginal distributions for the priors alone;
essentially \(\mathcal{H}(\theta\ | \ y)\) where
\(\mathcal{L}(y\ |\ \theta)\) is flat for all values of the prior so
\begin{equation}
 \mathcal{H}(\theta\ | \ y) = \mathcal{P}_1(\theta_1)\times \mathcal{P}_2(\theta_2)\times \mathcal{P}_3(\theta_3)\times ...\times \mathcal{P}_k(\theta_k)
\end{equation} This means we know what the marginals should look like --
they are just the priors!

Assume there are \(k\) unknown parameters with a prior density set by
the investigator, such as \begin{equation}
  \mathcal{P}(\rho_1) \sim \text{Beta}(1.2,1.8)
\end{equation} subject to the arbitrary bounds that
\(0.001 < \rho_1 <0.999\).

We have \(k\) of these, so in any code we could specify this in a matrix
where each prior is specified in a row containing a name, a PDF type,
the parameters of the PDF, as well as a lower and upper bound.

\hypertarget{example-1}{%
\section{Example 1}\label{example-1}}

First example: target density is the product of six parameters densities
of four types: Normal, Gamma, Inverse Gamma and Beta. Any suitable
density could be used as a prior, so the Uniform, say, or the Inverse
Weibull or Log Gamma could be slotted in -- and we will later on. All
that is required is that the some function exists to evaluate the
density. Obviously we could generalize this to one or three or more
parameter distributions with appropriate code.

For example we could fit a Skew-\(t\) say, as long as we can evaluate
the (log) density for this 4 parameter distribution. See Klugman,
Panjer, and Willmot (2008) for a very comprehensive list of densities we
could use -- I cannot recommend this highly enough.

\begin{table}
\centering
\begin{tabular}{l|r|r|l|r|r}
\hline
name & lb & ub & PDF & p1 & p2\\
\hline
beta & 0.001 & 0.999 & beta & 2.3 & 1.200\\
\hline
rho[1] & 0.001 & 0.999 & beta & 1.2 & 1.800\\
\hline
kappa & 0.001 & 3.000 & gamma & 2.0 & 4.000\\
\hline
mu & -5.000 & 1.000 & norm & -2.0 & 0.550\\
\hline
sigma[1] & 0.001 & 5.000 & invgamma & 12.0 & 0.050\\
\hline
sigma[2] & 0.001 & 5.000 & invgamma & 9.0 & 0.075\\
\hline
\end{tabular}
\end{table}

Parameters of six densities used in Example 1

\hypertarget{analytic-densities}{%
\subsection{Analytic densities}\label{analytic-densities}}

\begin{figure}

{\centering \includegraphics{SMH_files/figure-pdf/theoretical-1.pdf}

}

\caption{Plots of the theoretical densities given parameters in Table}

\end{figure}

\hypertarget{random-draws}{%
\subsection{Random draws}\label{random-draws}}

\begin{figure}

{\centering \includegraphics{SMH_files/figure-pdf/unnamed-chunk-1-1.pdf}

}

\caption{True density known so we can draw from an appropriate random
number generator}

\end{figure}

\hypertarget{estimating-the-marginals-from-the-joint-density}{%
\section{Estimating the marginals from the joint
density}\label{estimating-the-marginals-from-the-joint-density}}

First we need to understand the estimation procedure. The MH algorithm
uses only information from the posterior to estimate the marginal
processes that generated it, in stark contrast with Gibbs Sampling that
uses conditional densities to approximate the unconditional one and then
back out the marginals.

\hypertarget{metropolis-hastings-1}{%
\subsection{Metropolis-Hastings}\label{metropolis-hastings-1}}

A thorough explanation can be found in (\textbf{Chib?}) or
(\textbf{BDA?}), and here we describe the procedure without proof. Our
aim is to draw samples from some distribution \[
 \mathcal{H}(\theta)
\] where a direct approach is not feasible, because we don't have a
random number generator. The Metropolis-Hastings algorithm requires that
we can evaluate this posterior density at some arbitrary points. As the
form of the marginals is (potentially) unknown, we draw values from some
arbitrary density and decide whether it looks like it came from the
marginals that generated the posterior. \(\mathcal{H}(\theta)\) is
typically the posterior density where this distribution is far too
complex to directly sample.

This indirect approach is to specify a \emph{candidate density} \[
  q(\theta^{k+1}|\theta^k)
\] from which we \emph{can} make candidate draws. Given some value for
the parameters \(\theta^k\), we can randomly generate new values, which
may or may not be independent of this draws.

The MH algorithm requires that we are able to evaluate
\(\frac{H(\theta^{k+1})}{H(\theta^k)}\), and then draw a
\emph{candidate} value \(\theta^{k+1}\) from
\(q(\theta^{k+1}|\theta^k)\). We then accept this candidate value with
the probability \[
  \alpha = \min \left(\frac{H(\theta^{k+1})/q(\theta^{k+1}|\theta^k)}{H(\theta^k)/q(\theta^k|\theta^{k+1})}, 1\right)
\] Practically, this requires we compute \(\alpha\) and draw a number
\(u\) from \(U(0,1)\), and if \(u<\alpha\) accept \(\theta^{k+1}\)
otherwise keep \(\theta^k\).

\hypertarget{simplification}{%
\subsection{Simplification}\label{simplification}}

The \emph{random walk} version of the algorithm takes the specific
candidate density \(q(\theta^{k+1}|\theta^k)\) as\\
\[
 \theta^{k+1} = \theta^k + \epsilon_t
\] where \(\epsilon_t\sim N(0,\Sigma)\) for some \(\Sigma\) which we
need to choose. This is a simple vector-random walk. Let \(\theta^k\) be
some existing draw and \(\theta^{k+1}\) be a new draw. We can write \[
  \epsilon_t = \theta^{k+1}-\theta^k
\] then \[
  P(\epsilon_t) = P(\theta^{k+1}-\theta^k)
\]

Because this is a normal density (which is symmetric) then\\
\[
  P(\epsilon_t) = P(-\epsilon_t)
\] Symmetry implies an acceptance probability of\\
\[
  \frac{H(\theta^{k+1})}{H(\theta^k)}
\] as \(q(\theta^{k+1}|\theta^k) = q(\theta^k|\theta^{k+1})\) so these
terms cancel.

\hypertarget{algorithm}{%
\subsubsection{Algorithm}\label{algorithm}}

\textbf{Step 1} Draw a \emph{candidate} value \(\theta^{G+1}\) from
\(q(\theta^{k+1}|\theta^k)\), specifically
\(\theta^{k+1} = \theta^k + \epsilon_t\) where
\(\epsilon_t\sim N(0,\Sigma)\)

\textbf{Step 2} Compute the acceptance probability \[
  \alpha = \min \left(\frac{H(\theta^{k+1})}{H(\theta^k)}, 1\right)
\]

\textbf{Step 3} If \(u\sim U(0,1)\) is less than \(\alpha\), keep
\(\theta^{k+1}\), else repeat \(\theta^k\) and discard the new draw

\begin{tcolorbox}[enhanced jigsaw, breakable, left=2mm, arc=.35mm, toptitle=1mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, bottomrule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, coltitle=black, titlerule=0mm, toprule=.15mm, bottomtitle=1mm, rightrule=.15mm, colback=white]

\begin{itemize}
\tightlist
\item
  The density \(\mathcal{H}(\theta)\) will usually be a posterior,
  combining the priors and likelihood information;
\item
  At present we have no likelihood information, so all we need is a
  function to evaluate the (log) joint prior.
\end{itemize}

\end{tcolorbox}

\hypertarget{a-simpler-problem}{%
\section{A simpler problem}\label{a-simpler-problem}}

We have no data (or indeed model) to pass to the posterior function (as
there is no likelihood). Generalizing this to incorporate likelihood
information is straightforward.

We need to specify the scale of the random walk: assume \[
  \Sigma = sI
\] We should choose a value of \(s\) to ensure that the whole parameter
space is explored as

\begin{itemize}
\tightlist
\item
  if \(s\) too small we don't walk far enough, and stay too close to
  potentially only local maxima;
\item
  if \(s\) too large may jump over highest density points at every step
  and take a long time to converge.
\end{itemize}

We check if it is a suitable value by monitoring the acceptance rate:
between about 1/5 and 2/5 fine.

\hypertarget{estimating-example-1}{%
\subsection{Estimating example 1}\label{estimating-example-1}}

Choose some arbitrary initial values at which we can evaluate our
posterior likelihood (remembering for this example this only the joint
prior). These are 0.9, 0.2, 0.4, -2, 1.5, 1.5. As we have chosen values
close to the highest density this evaluates as 0.425433. We choose
\(s=0.25\) and do 100000 iterations and discard the first half. For the
run that generates the graphs below we get the message:

\begin{verbatim}
[1] "Acceptance ratio: 0.32642"
\end{verbatim}

The acceptance ratio is good, so we plot the draws:

\begin{figure}

{\centering \includegraphics{SMH_files/figure-pdf/unnamed-chunk-2-1.pdf}

}

\caption{Plots of the 50,000 draws}

\end{figure}

or just the first few to better see the algorithm in action:

\begin{figure}

{\centering \includegraphics{SMH_files/figure-pdf/unnamed-chunk-3-1.pdf}

}

\caption{Plots of first 333 draws}

\end{figure}

Clearly there are some repeat values. What do the estimated densiites
from each of these sequences look like?

\begin{figure}

{\centering \includegraphics{SMH_files/figure-pdf/unnamed-chunk-4-1.pdf}

}

\caption{Histograms of draws and theoretical priors}

\end{figure}

Pretty good! Compare this with the sequence of draws we got from the
correct random number generators for each density.

\hypertarget{example-2}{%
\subsection{Example 2}\label{example-2}}

\begin{table}
\centering
\begin{tabular}{l|r|r|l|r|r}
\hline
name & lb & ub & PDF & p1 & p2\\
\hline
eta & 0.250 & 5.000 & invweibull & 2.3 & 1.2\\
\hline
zeta[1] & 0.001 & 6.000 & paralogis & 1.0 & 4.0\\
\hline
zeta[2] & 0.001 & 3.000 & paralogis & 2.0 & 3.0\\
\hline
delta & 0.001 & 5.000 & invpareto & 2.0 & 0.3\\
\hline
alpha[1] & 1.001 & 5.000 & lgamma & 2.0 & 5.0\\
\hline
alpha[2] & 1.001 & 5.000 & lgamma & 3.0 & 4.0\\
\hline
upsilon[1] & 0.001 & 0.999 & unif & 0.3 & 0.7\\
\hline
upsilon[2] & 0.001 & 0.999 & unif & 0.5 & 1.0\\
\hline
\end{tabular}
\end{table}

All-different distributions -- Inverse Weibull, Paralogistic, Inverse
Pareto, Log Gamma and Uniform

\begin{figure}

{\centering \includegraphics{SMH_files/figure-pdf/unnamed-chunk-5-1.pdf}

}

\caption{Plots of the theoretical densities given parameters in Table 2}

\end{figure}

\begin{figure}

{\centering \includegraphics{SMH_files/figure-pdf/MHcall2-1.pdf}

}

\caption{Walk a little less \(s=0.125\) (could iterate a little more?)}

\end{figure}

\part{Networks}

\hypertarget{causal-inference}{%
\chapter{Causal Inference}\label{causal-inference}}

Outline how to solve the Pearl, Glymour, and Jewell (2016) exercises in
R.

\hypertarget{study-question-1.3.2}{%
\section{Study question 1.3.2}\label{study-question-1.3.2}}

Data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{ed }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{Gender =} \FunctionTok{c}\NormalTok{(}\StringTok{"M"}\NormalTok{,}\StringTok{"M"}\NormalTok{,}\StringTok{"M"}\NormalTok{,}\StringTok{"M"}\NormalTok{,}\StringTok{"F"}\NormalTok{,}\StringTok{"F"}\NormalTok{,}\StringTok{"F"}\NormalTok{,}\StringTok{"F"}\NormalTok{),}
             \AttributeTok{eLevel =} \FunctionTok{c}\NormalTok{(}\StringTok{"U"}\NormalTok{,}\StringTok{"H"}\NormalTok{,}\StringTok{"C"}\NormalTok{,}\StringTok{"G"}\NormalTok{,}\StringTok{"U"}\NormalTok{,}\StringTok{"H"}\NormalTok{,}\StringTok{"C"}\NormalTok{,}\StringTok{"G"}\NormalTok{),}
             \AttributeTok{num    =} \FunctionTok{c}\NormalTok{(}\DecValTok{112}\NormalTok{,}\DecValTok{231}\NormalTok{,}\DecValTok{595}\NormalTok{,}\DecValTok{242}\NormalTok{,}\DecValTok{136}\NormalTok{,}\DecValTok{189}\NormalTok{,}\DecValTok{763}\NormalTok{,}\DecValTok{172}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{total =} \FunctionTok{sum}\NormalTok{(num))}
\end{Highlighting}
\end{Shaded}

which we tabulate as

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ed }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llrr@{}}
\toprule\noalign{}
Gender & eLevel & num & total \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
M & U & 112 & 2440 \\
M & H & 231 & 2440 \\
M & C & 595 & 2440 \\
M & G & 242 & 2440 \\
F & U & 136 & 2440 \\
F & H & 189 & 2440 \\
F & C & 763 & 2440 \\
F & G & 172 & 2440 \\
\end{longtable}

\hypertarget{exercises-and-answers}{%
\section{Exercises and answers}\label{exercises-and-answers}}

\hypertarget{find-pelevel-h}{%
\subsection{\texorpdfstring{Find
\(P(eLevel = H)\)}{Find P(eLevel = H)}}\label{find-pelevel-h}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ed }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(eLevel }\SpecialCharTok{==} \StringTok{"H"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{p\_H =} \FunctionTok{sum}\NormalTok{(num)}\SpecialCharTok{/}\NormalTok{total) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llrrr@{}}
\toprule\noalign{}
Gender & eLevel & num & total & p\_H \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
M & H & 231 & 2440 & 0.1721311 \\
F & H & 189 & 2440 & 0.1721311 \\
\end{longtable}

\hypertarget{find-pelevel-h-vee-gender-f}{%
\subsection{\texorpdfstring{Find
\(P(eLevel = H\ \vee \ Gender = F)\)}{Find P(eLevel = H\textbackslash{} \textbackslash vee \textbackslash{} Gender = F)}}\label{find-pelevel-h-vee-gender-f}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ed }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(Gender }\SpecialCharTok{==} \StringTok{"F"} \SpecialCharTok{|}\NormalTok{ eLevel }\SpecialCharTok{==} \StringTok{"H"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{p\_HorF =} \FunctionTok{sum}\NormalTok{(num)}\SpecialCharTok{/}\NormalTok{total) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llrrr@{}}
\toprule\noalign{}
Gender & eLevel & num & total & p\_HorF \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
M & H & 231 & 2440 & 0.6110656 \\
F & U & 136 & 2440 & 0.6110656 \\
F & H & 189 & 2440 & 0.6110656 \\
F & C & 763 & 2440 & 0.6110656 \\
F & G & 172 & 2440 & 0.6110656 \\
\end{longtable}

\hypertarget{find-pelevel-h-gender-f}{%
\subsection{\texorpdfstring{Find
\(P(eLevel = H\ |\ Gender = F)\)}{Find P(eLevel = H\textbackslash{} \textbar\textbackslash{} Gender = F)}}\label{find-pelevel-h-gender-f}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ed }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(Gender }\SpecialCharTok{==} \StringTok{"F"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{tcond =} \FunctionTok{sum}\NormalTok{(num)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(eLevel }\SpecialCharTok{==} \StringTok{"H"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{p\_HgivenF =} \FunctionTok{sum}\NormalTok{(num)}\SpecialCharTok{/}\NormalTok{tcond) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llrrrr@{}}
\toprule\noalign{}
Gender & eLevel & num & total & tcond & p\_HgivenF \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
F & H & 189 & 2440 & 1260 & 0.15 \\
\end{longtable}

\hypertarget{find-pgender-f-elevel-h}{%
\subsection{\texorpdfstring{Find
\(P(Gender = F\ | \ eLevel = H)\)}{Find P(Gender = F\textbackslash{} \textbar{} \textbackslash{} eLevel = H)}}\label{find-pgender-f-elevel-h}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ed }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(eLevel }\SpecialCharTok{==} \StringTok{"H"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{tcond =} \FunctionTok{sum}\NormalTok{(num)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(Gender }\SpecialCharTok{==} \StringTok{"F"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{p\_FgivenH =} \FunctionTok{sum}\NormalTok{(num)}\SpecialCharTok{/}\NormalTok{tcond) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}llrrrr@{}}
\toprule\noalign{}
Gender & eLevel & num & total & tcond & p\_FgivenH \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
F & H & 189 & 2440 & 420 & 0.45 \\
\end{longtable}

\hypertarget{mapping-regional-house-price-inflation}{%
\chapter{Mapping regional house price
inflation}\label{mapping-regional-house-price-inflation}}

\hypertarget{how-heterogenous-is-uk-house-price-inflation}{%
\section{How heterogenous is UK house price
inflation?}\label{how-heterogenous-is-uk-house-price-inflation}}

A simple enough question, and one that Bahaj, Foulis, and Pinter
(2020)\index{Bahaj et al.|textbf} thought was best answered with a map
-- actually a referee asked for one. As I know how to draw a map in R
they asked me if I could do it. Well yes, but there are some particular
difficulties.

\begin{itemize}
\tightlist
\item
  The UK (actually Great Britain) is an awkward (but not too awkward)
  shape.
\item
  Population in the UK is heavily concentrated in a small number of
  centres, such as London or Manchester.
\item
  There are three different periods to compare.
\item
  It has to be in grayscale.
\end{itemize}

Before all of this we need some data, with boundaries that correspond to
areas that we have data for. The regional inflation data is available at
the level of the
\href{https://www.gov.uk/government/organisations/land-registry}{Land
Registry}, which almost by local authority but amalgamates a number of
the areas. So a map at Local Authority level would be fine as long as we
can amalgamate some of the regions.

The map data used here is available from the UK's
\href{https://geoportal.statistics.gov.uk/}{ONS
geoportal}\index{ONS!geoportal}, with a lot of administrative data
available including local authority boundaries. The Local Authority data
is specifically available from
\href{https://geoportal.statistics.gov.uk/maps/lad-dec-2015-generalised-clipped-boundaries-gb}{here},
where I use the clipped full extent version. There are a number of
possibilities, but in general high water mark, and enough but not too
much detail is needed.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(readxl)}
\FunctionTok{library}\NormalTok{(sf)}
\end{Highlighting}
\end{Shaded}

The information in the map file is comprehensive, and by Local Authority
as of December 2015.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fle }\OtherTok{\textless{}{-}} \StringTok{"LAD\_Dec\_2015\_GCB\_GB"}
\NormalTok{shape }\OtherTok{\textless{}{-}} \FunctionTok{read\_sf}\NormalTok{(}\AttributeTok{dsn=}\StringTok{"."}\NormalTok{, }\AttributeTok{layer=}\NormalTok{fle)}
\end{Highlighting}
\end{Shaded}

We can look at the attributes using \texttt{summary}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(shape)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   lad15cd            lad15nm            lad15nmw           GlobalID        
 Length:380         Length:380         Length:380         Length:380        
 Class :character   Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character   Mode  :character  
          geometry  
 MULTIPOLYGON :380  
 epsg:27700   :  0  
 +proj=tmer...:  0  
\end{verbatim}

This can be plotted straightforwardly using \texttt{ggplot}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{shape }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{geometry=}\NormalTok{geometry, }\AttributeTok{fill=}\NormalTok{lad15nm), }
          \AttributeTok{color=}\ConstantTok{NA}\NormalTok{, }\AttributeTok{alpha=}\NormalTok{.}\DecValTok{66}\NormalTok{, }\AttributeTok{show.legend=}\ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_void}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Maps_files/figure-pdf/map0a-1.pdf}

}

\end{figure}

Looking at the read-out above, each of the 380 regions have some
metadata associated, which are contained in each of the listed
attributes. It should be obvious that \texttt{objectid} is just a
sequence from 1 to 380. \texttt{lad15nm} turns out to be a list of names
of the regions -- I suspect \texttt{lad} for Local Authority District,
\texttt{15} for 2015 and \texttt{nm} for name -- and it is easy to
specify this as the name to use for the region when using \texttt{tidy}.

Now this can be plotted using \texttt{ggplot}, using \texttt{geometry}
for the \(x\) and \(y\) coordinates. The choice of fill colour is
determined by \texttt{fill} and we can set the colour of the lines by
\texttt{colour} (or \texttt{color}). The two extra arguments are for a
suitable blank style and to impose an appropriate ratio of height to
width.

Immediately, the awkward shape of the British Isles is apparent. (Note
this is a plot of Great Britain, and there is no Northern Ireland.) The
islands to the far north are somewhat unnecessary, although quite
rightly the inhabitants get a bit tired of being left off maps!
Nonetheless I'll do exactly the same by filtering out the polygons
associated with \texttt{Orkney\ Islands} and \texttt{Shetland\ Islands}.

Fewer Scottish Islands makes the graphs a lot clearer with little loss
of information, paticularly given the tiny number of transactions in the
Orkneys and the Shetlands, very far to the north.

In what follows we filter out the islands using

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{shape }\OtherTok{\textless{}{-}} \FunctionTok{read\_sf}\NormalTok{(}\AttributeTok{dsn=}\StringTok{"."}\NormalTok{, }\AttributeTok{layer=}\NormalTok{fle) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{lad15nm }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"Shetland Islands"}\NormalTok{,}\StringTok{"Orkney Islands"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Country=}\FunctionTok{str\_sub}\NormalTok{(lad15cd, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{.after=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

where we also create an indicator of country using the first letter of
the code string.

So the final country map is

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{shape }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Country) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill=}\NormalTok{Country), }\AttributeTok{color=}\StringTok{"grey77"}\NormalTok{, }\AttributeTok{linewidth=}\NormalTok{.}\DecValTok{25}\NormalTok{, }\AttributeTok{alpha=}\NormalTok{.}\DecValTok{66}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_void}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Maps_files/figure-pdf/map0d-1.pdf}

}

\end{figure}

\begin{tcolorbox}[enhanced jigsaw, breakable, left=2mm, arc=.35mm, toptitle=1mm, colbacktitle=quarto-callout-tip-color!10!white, opacityback=0, bottomrule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{\texttt{group} and \texttt{summarise} can join geographical areas}, colframe=quarto-callout-tip-color-frame, coltitle=black, titlerule=0mm, toprule=.15mm, bottomtitle=1mm, rightrule=.15mm, colback=white]

Note the really nice feature -- if we group by something, in this case
country, we can summarise to amalgamate the geometries!

\end{tcolorbox}

You may have noticed, one thing that that's missing on the LA graphs is
the boundaries. They aren't, they're just invisible. That's because I
set \texttt{colour\ =\ NA}, so I can fix that by choosing a colour and
making the lines very thin so they don't swamp the map, as in the
country one.

One further amendment, the \texttt{fill} is moved inside the
\texttt{aes()} specification and made conditional. \texttt{R} now
chooses unique colours for each of the regions.

Two things now need to be done to get the map colours right to
illustrate regional inflation rates. First we need to amalgamate some of
the Local Authority boundaries to the Land Registry definitions, and
second we need to assign the inflation rate to each area.

\hypertarget{inflation-data-and-regions}{%
\section{Inflation data and regions}\label{inflation-data-and-regions}}

We have a map, and we have that data in a form that is easy to
understand. If we can suitably attach an inflation rate to each area
then we can fill the individual areas with a colour unique to each
individual inflation rates.

Recall that the Land Registry areas aren't quite what we have, and will
need amalgamating. Bahaj, Foulis, and Pinter (2020) supplied me the
areas that needed amalgamating (and the inflation rates) using the
\href{https://en.wikipedia.org/wiki/ONS_coding_system}{ONS codes}. This
is contained in the metadata \texttt{lad15cd} above.

The data is structured in `wide' format with one row for each Land
Registry region. The details aren't very important for us now, but what
it means is I can manipulate it to get

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Price data by Land Registry region, converted to long format}
\NormalTok{hp\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_excel}\NormalTok{(}\StringTok{"house\_price\_data\_figure\_1.xls"}\NormalTok{)  }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\StringTok{"land\_reg\_region"}\NormalTok{, }\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"e\_"}\NormalTok{), }\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"av\_"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{names\_to  =} \StringTok{"name"}\NormalTok{, }
               \AttributeTok{values\_to =} \StringTok{"lad15cd"}\NormalTok{, }
               \AttributeTok{cols      =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{land\_reg\_region, }\SpecialCharTok{{-}}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"av\_"}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{drop\_na}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(land\_reg\_region, lad15cd, }\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"av\_"}\NormalTok{)) }

\NormalTok{codes }\OtherTok{\textless{}{-}}\NormalTok{ hp\_data }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(lad15cd, land\_reg\_region) }
\end{Highlighting}
\end{Shaded}

The important thing that the \texttt{pivot\_longer} achieves is that for
every \texttt{land\_reg\_region} I get a list of all the ONS codes that
makes up the Local Authority level. So if I look at
\texttt{buckinghamshire} as an example there are four ONS codes now
associated with it.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{filter}\NormalTok{(codes, land\_reg\_region }\SpecialCharTok{==} \StringTok{"buckinghamshire"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 2
  lad15cd   land_reg_region
  <chr>     <chr>          
1 E07000004 buckinghamshire
2 E07000005 buckinghamshire
3 E07000006 buckinghamshire
4 E07000007 buckinghamshire
\end{verbatim}

Join these together

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Join polygons defined by Land Registry regions}
\NormalTok{gg }\OtherTok{\textless{}{-}}\NormalTok{ shape }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"lad"}\NormalTok{,}\StringTok{"C"}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{left\_join}\NormalTok{(codes, }\AttributeTok{by=}\StringTok{"lad15cd"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(land\_reg\_region) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

which produces a match between the Land Registry and the Local Authority
areas, plus the inflation rates.

\hypertarget{inflation-in-grayscale}{%
\subsection{Inflation in grayscale}\label{inflation-in-grayscale}}

All the information required to plot the Land Registry-based regional
inflation rates is now available. As you can see from the
\texttt{buckinghamshire} data above, there are three average rates in
three different periods, so I'll focus on one, 2002-2007 to begin with.

First, augment the geographic data with the inflation data, and call
them something better.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gg }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{geometry=}\NormalTok{geometry, }\AttributeTok{fill=}\NormalTok{land\_reg\_region), }
          \AttributeTok{color=}\ConstantTok{NA}\NormalTok{, }\AttributeTok{alpha=}\NormalTok{.}\DecValTok{66}\NormalTok{, }\AttributeTok{show.legend =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_void}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Maps_files/figure-pdf/unnamed-chunk-4-1.pdf}

}

\end{figure}

Then specify gray and put the legend at the bottom.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nms }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"av\_hp\_growth"}\NormalTok{, }\StringTok{"HPI"}\NormalTok{, }\FunctionTok{colnames}\NormalTok{(hp\_data))}

\NormalTok{hp\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename\_all}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ nms) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(land\_reg\_region, }\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"HPI"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{distinct}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{left\_join}\NormalTok{(gg) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{geometry=}\NormalTok{geometry, }\AttributeTok{fill=}\NormalTok{HPI\_02\_07), }
          \AttributeTok{color=}\ConstantTok{NA}\NormalTok{, }\AttributeTok{alpha=}\NormalTok{.}\DecValTok{66}\NormalTok{, }\AttributeTok{show.legend =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_void}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_gradient}\NormalTok{(}\AttributeTok{low=}\FunctionTok{grey}\NormalTok{(}\FloatTok{0.9}\NormalTok{), }\AttributeTok{high=}\FunctionTok{grey}\NormalTok{(}\FloatTok{0.05}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.direction =} \StringTok{"horizontal"}\NormalTok{, }
        \AttributeTok{legend.position  =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.75}\NormalTok{,}\FloatTok{0.05}\NormalTok{),}
        \AttributeTok{legend.title     =} \FunctionTok{element\_blank}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining with `by = join_by(land_reg_region)`
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{Maps_files/figure-pdf/unnamed-chunk-5-1.pdf}

}

\end{figure}

\part{Time}

\hypertarget{linear-rational-expectations-models}{%
\chapter{Linear rational expectations
models}\label{linear-rational-expectations-models}}

\hypertarget{introduction-2}{%
\section{Introduction}\label{introduction-2}}

How do we solve rational expectations models? What does that even mean?
Here I show how to implement versions of the Blanchard and Kahn
(1980)\index{Blanchard, O.}\index{Kahn, C.} and Klein (2000) solutions
to linear rational expectations models in R. The implementation is
fairly general, and copes with singular models. It is a very transparent
implementation, with all the necessary code, and also shows how to
calculate and plot impulse responses.

\hypertarget{model}{%
\section{Model}\label{model}}

We take a simple New Keynesian model \begin{align}
y_t    &= y_{t+1}^e-\frac{1}{\sigma} (i_t - \pi_{t+1}^e) + e_t^1 \\
\pi_t  &= \beta \pi_{t+1}^e + \kappa y_t + e_t^2 \\
i_t    &= \gamma i_{t-1} + (1-\gamma) \delta \pi_t + \varepsilon_t^3 \\ 
e_t^1  &= \rho_1 e_{t-1}^1 + \varepsilon_t^1 \\ 
e_t^2  &= \rho_2 e_{t-1}^2 + \varepsilon_t^2 
\end{align} The model comprises a dynamic IS curve, a Phillips Curve and
a policy rule with smoothing. There are three shocks, two of which are
persistent. This we need to write in the general algebraic linear
state-space form: \[
E\begin{bmatrix} z_t \\ x_{t+1}^e \end{bmatrix} = A \begin{bmatrix} z_{t-1} \\ x_t \end{bmatrix} + B \varepsilon_t  
\] We map our variables to their algebraic equivalent as (\(z_t\),
\(x_t\)) \(=\) ((\(e^1_t\), \(e^2_t\), \(i_t\)), (\(y_t\), \(\pi_t\))).
Then the model in state-space form but including the matrix \(E\) is \[
\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\ 
                0 & 1 & 0 & 0 & 0 \\ 
                0 & 0 & 1 & 0 & 0 \\ 
                1 & 0 & -\frac{1}{\sigma} & 1 & \frac{1}{\sigma} \\ 
                0 & 1 & 0 & 0 & \beta
\end{bmatrix}
\begin{bmatrix} e^1_t \\ e^2_t \\ i_t \\ y^e_{t+1} \\ \pi^e_{t+1} \end{bmatrix} 
   = 
   \begin{bmatrix} \rho_1 & 0 & 0 & 0 & 0 \\ 
                0 & \rho_2 & 0 & 0 & 0 \\ 
                0 & 0 & \gamma & 0 & (1-\gamma)\delta \\ 
                0 & 0 & 0 & 1 & 0 \\ 
                0 & 0 & 0 & -\kappa & 1
   \end{bmatrix}
\begin{bmatrix} e^1_{t-1} \\ e^2_{t-1} \\ i_{t-1} \\ y_t \\ \pi_t \end{bmatrix}    
   + 
      \begin{bmatrix} 
                1 & 0 & 0  \\ 
                0 & 1 & 0 \\ 
                0 & 0 & 1 \\ 
                0 & 0 & 0 \\ 
                0 & 0 & 0 
   \end{bmatrix}
   \begin{bmatrix} \varepsilon^1_t \\ \varepsilon^2_t \\ \varepsilon^3_t \end{bmatrix}    
\] Anyone wanting to code up solutions should familiarize themselves
with this before continuing.

\hypertarget{coding-the-model}{%
\subsection{Coding the model}\label{coding-the-model}}

Before we begin coding this in R, load the \texttt{tidyverse} libraries
so we can do impulse responses with our usual tool kit and then we can
forget about it.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

Set the model parameters

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nf    }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{ns    }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{ne    }\OtherTok{\textless{}{-}} \DecValTok{3}
\NormalTok{np    }\OtherTok{\textless{}{-}}\NormalTok{ ns}\SpecialCharTok{{-}}\NormalTok{nf}

\NormalTok{beta  }\OtherTok{\textless{}{-}} \FloatTok{0.99}   \CommentTok{\# Discount factor }
\NormalTok{sigma }\OtherTok{\textless{}{-}} \FloatTok{2.0}    \CommentTok{\# Elas. substitution}
\NormalTok{kappa }\OtherTok{\textless{}{-}} \FloatTok{0.075}  \CommentTok{\# Slope PC}
\NormalTok{delta }\OtherTok{\textless{}{-}} \FloatTok{1.5}    \CommentTok{\# Inflation feedback}
\NormalTok{gamma }\OtherTok{\textless{}{-}} \FloatTok{0.75}   \CommentTok{\# Smoothing}
\NormalTok{rho\_1 }\OtherTok{\textless{}{-}} \FloatTok{0.9}    \CommentTok{\# AR1}
\NormalTok{rho\_2 }\OtherTok{\textless{}{-}} \FloatTok{0.8}    \CommentTok{\# AR1}
\NormalTok{Omega }\OtherTok{\textless{}{-}} \FunctionTok{diag}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.33}\NormalTok{,}\FloatTok{0.33}\NormalTok{,}\FloatTok{0.33}\NormalTok{)) }\CommentTok{\# SE of 3 shocks}
\end{Highlighting}
\end{Shaded}

Now define the model matrices `long hand' and some variable names, which
we put in \texttt{labels}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{labels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"e\^{}1"}\NormalTok{,}\StringTok{"e\^{}2"}\NormalTok{,}\StringTok{"i"}\NormalTok{,}\StringTok{"y"}\NormalTok{,}\StringTok{"pi"}\NormalTok{)}

\NormalTok{E }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{,ns,ns)}
\NormalTok{A }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{,ns,ns)}
\NormalTok{B }\OtherTok{\textless{}{-}} \FunctionTok{diag}\NormalTok{(}\DecValTok{1}\NormalTok{,ns,ne)}

\CommentTok{\# Now put the equations in matrix form}
\FunctionTok{diag}\NormalTok{(E[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }\OtherTok{\textless{}{-}} \DecValTok{1}
\FunctionTok{diag}\NormalTok{(A[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(rho\_1, rho\_2)}

\NormalTok{E[}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{]             }\OtherTok{\textless{}{-}} \DecValTok{1} 
\NormalTok{E[}\DecValTok{4}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{)] }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\SpecialCharTok{/}\NormalTok{sigma, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\SpecialCharTok{/}\NormalTok{sigma)}
\NormalTok{E[}\DecValTok{5}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{)]       }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, beta)}

\NormalTok{A[}\DecValTok{3}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{)]       }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(gamma, (}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{gamma)}\SpecialCharTok{*}\NormalTok{delta)}
\NormalTok{A[}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{]             }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{A[}\DecValTok{5}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{)]        }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{kappa, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

where for example, \(E\) and \(A\) are \begin{equation}
  E = \left[\begin{matrix}1 &0 &0 &0 &0 \\0 &1 &0 &0 &0 \\0 &0 &1 &0 &0 \\1 &0 &-0.5 &1 &0.5 \\0 &1 &0 &0 &0.99 \\\end{matrix}\right] 
\end{equation} \begin{equation}  
  A = \left[\begin{matrix}0.9 &0 &0 &0 &0 \\0 &0.8 &0 &0 &0 \\0 &0 &0.75 &0 &0.375 \\0 &0 &0 &1 &0 \\0 &0 &0 &-0.075 &1 \\\end{matrix}\right] 
\end{equation} Calculate the reduced form state-space model
\begin{equation}
\begin{bmatrix} z_t \\ x_{t+1}^e \end{bmatrix} = C \begin{bmatrix} z_{t-1} \\ x_t \end{bmatrix} + D \varepsilon_t  
\end{equation} which is done in R very simply as

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{C }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(E,A)}
\NormalTok{D }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(E,B)}
\end{Highlighting}
\end{Shaded}

Why can't we solve this for impulse responses?

The following function simulates the impulse responses of a model in a
loop within a loop\footnote{Sometimes a loop is the right way to do
  something.} and returns the time series in a suitably organised data
frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{impulse\_responses }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(P, Q, Omega, labels, T) \{}
\NormalTok{  s   }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{ncol}\NormalTok{(Q), }\DecValTok{1}\NormalTok{)}
\NormalTok{  z   }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(Q), T)}
  \FunctionTok{rownames}\NormalTok{(z) }\OtherTok{\textless{}{-}}\NormalTok{ labels}
\NormalTok{  dza }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(Q)) \{}
\NormalTok{    s[j]  }\OtherTok{\textless{}{-}}\NormalTok{ Omega[j,j]}
\NormalTok{    z[,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ Q }\SpecialCharTok{\%*\%}\NormalTok{ s}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{(T}\DecValTok{{-}1}\NormalTok{)) \{}
\NormalTok{      z[,i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}}\NormalTok{ P }\SpecialCharTok{\%*\%}\NormalTok{ z[,i]}
\NormalTok{    \}}
\NormalTok{    s[j] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{    dz }\OtherTok{\textless{}{-}} \FunctionTok{as\_tibble}\NormalTok{(}\FunctionTok{t}\NormalTok{(z)) }\SpecialCharTok{\%\textgreater{}\%} 
      \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Period =} \DecValTok{1}\SpecialCharTok{:}\NormalTok{T, }\AttributeTok{Shock =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"epsilon\^{}"}\NormalTok{,j))}
\NormalTok{    dza }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(dza,dz)}
\NormalTok{  \}}
  \FunctionTok{return}\NormalTok{(dza)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

A function to plot the impulses will be useful, so we create one.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{response\_plot }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(series, title) \{}
  \FunctionTok{return}\NormalTok{(}\FunctionTok{pivot\_longer}\NormalTok{(series, }\AttributeTok{cols =} \SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(Period,Shock), }\AttributeTok{names\_to=}\StringTok{"Var"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Val"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
           \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
           \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Period, }\AttributeTok{y=}\NormalTok{Val, }\AttributeTok{group=}\NormalTok{Shock, }\AttributeTok{colour=}\NormalTok{Var), }\AttributeTok{show.legend=}\ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
           \FunctionTok{facet\_grid}\NormalTok{(Shock}\SpecialCharTok{\textasciitilde{}}\NormalTok{Var, }\AttributeTok{scales=}\StringTok{"free"}\NormalTok{, }\AttributeTok{labeller=}\NormalTok{label\_parsed) }\SpecialCharTok{+}
           \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{expand=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)) }\SpecialCharTok{+}
           \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
           \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\NormalTok{title, }\AttributeTok{x=}\StringTok{""}\NormalTok{,}\AttributeTok{y=}\StringTok{""}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Call the impulse response function using the model \(C\) and \(D\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{T }\OtherTok{\textless{}{-}} \DecValTok{25}
\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{impulse\_responses}\NormalTok{(C, D, Omega, labels, T)}
\end{Highlighting}
\end{Shaded}

and plot

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{response\_plot}\NormalTok{(z, }\StringTok{"Impulse responses: Taylor rule"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{BK_files/figure-pdf/unnamed-chunk-4-1.pdf}

}

\end{figure}

Oh! That's not looking good. Let's try a few more periods.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{T }\OtherTok{\textless{}{-}} \DecValTok{150}
\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{impulse\_responses}\NormalTok{(C, D, Omega, labels, T)}
\FunctionTok{response\_plot}\NormalTok{(z, }\StringTok{"Impulse responses: Taylor rule"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{BK_files/figure-pdf/unnamed-chunk-5-1.pdf}

}

\end{figure}

This is clearly exploding. But it's rational -- we're solving forward so
expectations are always fulfilled. This is a key the insight of the
early rational expectations modellers -- rational isn't enough,
non-explosive is necessary too. Fortunately we know how to find this.

\hypertarget{bk80}{%
\section{Blanchard and Kahn (1980)}\label{bk80}}

To solve this model to give a unique \emph{stable} rational expectations
equilibrium, we appeal to the following. Consider the eigenvalue
decomposition \[
  MC=\Lambda M
\] where \(\Lambda\) is a diagonal matrix of \emph{eigenvalues} in
increasing absolute value and \(M\) is a non-singular matrix of
\emph{left eigenvectors}. Note that computer routines (including the one
in R) usually calculate \emph{right eigenvectors} such that
\(CV=V\Lambda\) and that \(M=V^{-1}\), so be aware of this in what
follows.

We can \emph{diagonalise} \(C\) and write it as \(C=M^{-1}\Lambda M\).
So pre-multiplying the reduced form model by \(M\) gives \[ 
M \begin{bmatrix} z_t \\ x_{t+1}^e \end{bmatrix} = \Lambda M \begin{bmatrix} z_{t-1} \\ x_t \end{bmatrix} + M D \varepsilon_t
\] Blanchard and Kahn (1980)\index{Blanchard-Kahn!conditions} (following
Vaughan (1970)) show uniqueness requires as many unstable eigenvalues as
jump variables. To see this, define \[
\begin{bmatrix} \xi_{t-1}^{s} \\  \xi_t^{u} \end{bmatrix}
  =  \begin{bmatrix} M_{11} & M_{12} \\  M_{21} & M_{22} \end{bmatrix}
      \begin{bmatrix} z_{t-1} \\  x_t \end{bmatrix}
\] Write the normalized model as \[
\begin{bmatrix} \xi_t^s \\ \xi_{t+1}^u \end{bmatrix}
 = \begin{bmatrix} \Lambda_s & 0 \\ 0 & \Lambda_u \end{bmatrix} 
\begin{bmatrix} \xi_{t-1}^s \\ \xi_t^u \end{bmatrix} +
\begin{bmatrix} M_1 \\ M_2 \end{bmatrix} D\varepsilon_t
\] where the eigenvalues are split into stable (\(\Lambda_s\)) and
unstable (\(\Lambda_u\)). If we ignore the stochastic bit for a moment
\[
\begin{bmatrix} \xi_t^s \\ \xi_{t+1}^u \end{bmatrix}
 = \begin{bmatrix} \Lambda_s & 0 \\ 0 & \Lambda_u \end{bmatrix} 
\begin{bmatrix} \xi_{t-1}^s \\ \xi_t^u \end{bmatrix}
\]

We seek a non-explosive solution, and this turns out to be easy to find
using the following

\begin{itemize}
\tightlist
\item
  The dynamics of \(\xi_t^u\) are determined by \(\Lambda_u\) and
  nothing else;
\item
  If they don't start at \(0\) they must explode;
\item
  This implies they must start at \(0\) and are always \(0\).
\end{itemize}

Thus the definition of the canonical variables necessarily implies \[
\begin{bmatrix} \xi_{t-1}^s \\  0 \end{bmatrix}
  =  \begin{bmatrix} M_{11} & M_{12} \\  M_{21} & M_{22} \end{bmatrix}
      \begin{bmatrix} z_{t-1} \\  x_t \end{bmatrix}
\]

From this it is clear that the jump variables themselves are only on the
saddle path if \[
   M_{21} z_{t-1} + M_{22} x_t = 0
\]

The rational solution implies that the jump variables are linearly
related to the predetermined ones through \begin{align}
x_t &= -M_{22}^{-1} M_{21}z_{t-1} \\
    &= N z_{t-1}
\end{align} We'll deal with the shocks in a moment.

How do we do this in R? First, find the eigenvalue decomposition of
\(C\) using

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m }\OtherTok{\textless{}{-}} \FunctionTok{eigen}\NormalTok{(C, }\AttributeTok{symmetric=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

which yields

\begin{verbatim}
eigen() decomposition
$values
[1] 1.0715518+0.092734i 1.0715518-0.092734i 0.9000000+0.000000i
[4] 0.8000000+0.000000i 0.6548762+0.000000i

$vectors
                      [,1]                  [,2]         [,3]           [,4]
[1,]  0.0000000+0.0000000i  0.0000000+0.0000000i 0.2854942+0i  0.00000000+0i
[2,]  0.0000000+0.0000000i  0.0000000+0.0000000i 0.0000000+0i  0.09783896+0i
[3,]  0.1599159-0.5089425i  0.1599159+0.5089425i 0.7830500+0i  0.49622464+0i
[4,] -0.6991064+0.0000000i -0.6991064+0.0000000i 0.4552131+0i -0.86012270+0i
[5,]  0.2629802-0.3968579i  0.2629802+0.3968579i 0.3132200+0i  0.06616328+0i
              [,5]
[1,]  0.0000000+0i
[2,]  0.0000000+0i
[3,]  0.6351203+0i
[4,] -0.7554249+0i
[5,] -0.1611069+0i
\end{verbatim}

However this calculates \emph{right} eigenvectors. We will need to
invert it for left ones. Given the number of jump variables in the model
satisfies the Blanchard-Kahn conditions\index{Blanchard-Kahn!conditions}
of as many unstable roots (1.072+0.093i, 1.072-0.093i) as jump variables
(2) we can calculate the reaction function from the eigenvectors

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iz }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\NormalTok{np}
\NormalTok{ix }\OtherTok{\textless{}{-}}\NormalTok{ (np}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{ns}
\NormalTok{M  }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(m}\SpecialCharTok{$}\NormalTok{vectors[,ns}\SpecialCharTok{:}\DecValTok{1}\NormalTok{])        }\CommentTok{\# Invert \& reverse order for increasing abs value}
\NormalTok{N  }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\FunctionTok{Re}\NormalTok{(}\FunctionTok{solve}\NormalTok{(M[ix,ix], M[ix,iz])) }\CommentTok{\# Drop tiny complex bits (if any)}
\end{Highlighting}
\end{Shaded}

where \texttt{iz} are the indices of the first \texttt{np} variables and
\texttt{ix} those of the remaining \texttt{nf} ones.

\hypertarget{stochastic-part}{%
\subsection{Stochastic part}\label{stochastic-part}}

What about the shocks? Assume the stochastic reaction function is \[
  x_t = N z_{t-1} + G \varepsilon_t 
\] Following Andrew P. Blake (2004), note that \(x_{t+1}^e = N z_t\) as
the expected value of \(\varepsilon_{t+1}=0\), meaning we can write \[
 Nz_t = C_{21}z_{t-1} + C_{22} x_t + D_2 \varepsilon_t
\] or \[
 N\left( C_{11}z_{t-1} + C_{12}x_t + D_1 \varepsilon_t\right) = C_{21}z_{t-1} + C_{22} x_t + D_2 \varepsilon_t
\] Gathering terms we obtain \[
  (C_{22} - N C_{12}) x_t = (NC_{11} - C_{21}) z_{t-1} + (N D_1 - D_2) \varepsilon_t
\] which implies \[
G=(C_{22} - N C_{12})^{-1}(N D_1 - D_2) 
\] Notice it also implies
\(N = (C_{22} - N C_{12})^{-1}(NC_{11} - C_{21})\). It is this fixed
point nature of the solution for \(N\) -- which in turn implies the
quadratic matrix equation \(C_{21} = NC_{11} - C_{22}N + N C_{12}N\) --
that means we need to use the Blanchard and Kahn
(1980)\index{Blanchard-Kahn!method} method in the first place.

All of this means that

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{G }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{((C[ix,ix] }\SpecialCharTok{{-}}\NormalTok{ N }\SpecialCharTok{\%*\%}\NormalTok{ C[iz,ix]), (N }\SpecialCharTok{\%*\%}\NormalTok{ D[iz,]}\SpecialCharTok{{-}}\NormalTok{ D[ix,]))}
\end{Highlighting}
\end{Shaded}

so for our model and parameters \(N\) and \(G\) are

\begin{equation}  
  N = \left[\begin{matrix}4.8568 &-2.7586 \\-1.1894 &1.7929 \\1.9628 &-0.2537 \\\end{matrix}\right] 
\end{equation} \begin{equation}
  G = \left[\begin{matrix}5.3964 &-3.4483 \\-1.5859 &1.9921 \\2.4535 &-0.3382 \\\end{matrix}\right] 
\end{equation}

The `fixed point' check is that the following should be the same as
\(N\)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{solve}\NormalTok{((C[ix,ix] }\SpecialCharTok{{-}}\NormalTok{ N }\SpecialCharTok{\%*\%}\NormalTok{ C[iz,ix]), (N }\SpecialCharTok{\%*\%}\NormalTok{ C[iz,iz]}\SpecialCharTok{{-}}\NormalTok{ C[ix,iz]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        [,1]      [,2]       [,3]
[1,] 4.85680 -2.758647 -1.1894200
[2,] 1.79286  1.962790 -0.2536635
\end{verbatim}

which it is.

The solved model is finally \begin{align}
\begin{bmatrix} z_t \\ x_t \end{bmatrix} &= \begin{bmatrix} C_{11}+C_{12}N & 0 \\ N & 0 \end{bmatrix} \begin{bmatrix} z_{t-1} \\ x_{t-1} \end{bmatrix} + \begin{bmatrix} D_1+C_{12}G \\ G \end{bmatrix} \varepsilon_t \\
 &= P \begin{bmatrix} z_{t-1} \\ x_{t-1} \end{bmatrix} + Q \varepsilon_t
\end{align} which can be coded as

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{P  }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\FunctionTok{rbind}\NormalTok{((C[iz,iz] }\SpecialCharTok{+}\NormalTok{ C[iz,ix] }\SpecialCharTok{\%*\%}\NormalTok{ N), N), }\FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{,ns,nf))}
\NormalTok{Q  }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(D[iz,] }\SpecialCharTok{+}\NormalTok{ C[iz,ix] }\SpecialCharTok{\%*\%}\NormalTok{ G, G)}
\end{Highlighting}
\end{Shaded}

\hypertarget{digression-right-eigenvector-version}{%
\subsection{Digression -- right eigenvector
version}\label{digression-right-eigenvector-version}}

It turns out that we could use the output from the standard
eigenvalue/vector routine directly by exploiting the following. This
time, let \(M\) be the matrix of \emph{right eigenvectors} so \[
  C M = M \Lambda \text{  or  } C = M\Lambda M^{-1}
\]\\
and \[
\begin{bmatrix} M_{11} & M_{12} \\ M_{21} & M_{22} \end{bmatrix}
\begin{bmatrix} \xi_{t-1}^s \\ \xi_t^u \end{bmatrix}
 = 
\begin{bmatrix} z_{t-1} \\ x_t \end{bmatrix}
\]

Written this way around, if \(\xi_t^{u}=0\) \(\forall\ t\) then (again
ignoring stochastics)

\[
  M_{11} \xi_{t-1}^s = z_{t-1}, \ M_{21}\xi_t^s = x_t 
\]

\[
   \Rightarrow x_t = M_{21} M_{11}^{-1} z_t
\] so

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{M }\OtherTok{\textless{}{-}}\NormalTok{ m}\SpecialCharTok{$}\NormalTok{vectors[,ns}\SpecialCharTok{:}\DecValTok{1}\NormalTok{]            }\CommentTok{\# Don\textquotesingle{}t invert as already right vectors, but reorder}
\FunctionTok{Re}\NormalTok{(M[ix,iz] }\SpecialCharTok{\%*\%} \FunctionTok{solve}\NormalTok{(M[iz,iz])) }\CommentTok{\# Again, drop tiny complex bits}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        [,1]      [,2]       [,3]
[1,] 4.85680 -2.758647 -1.1894200
[2,] 1.79286  1.962790 -0.2536635
\end{verbatim}

The result is identical. This method is particularly useful if there are
fewer predetermined variables than jumps as the matrix we need to invert
is of the same dimension as the predetermined variables this way round.

\hypertarget{impulse-responses}{%
\subsection{Impulse responses}\label{impulse-responses}}

We now call the impulse response function using the model solved for
rational expectations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{T }\OtherTok{\textless{}{-}} \DecValTok{25}
\NormalTok{z }\OtherTok{\textless{}{-}} \FunctionTok{impulse\_responses}\NormalTok{(P, Q, Omega, labels, T)}
\end{Highlighting}
\end{Shaded}

Now plot these responses

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{response\_plot}\NormalTok{(z, }\StringTok{"Impulse responses: Taylor rule"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{BK_files/figure-pdf/unnamed-chunk-13-1.pdf}

}

\end{figure}

Now, that looks better! It is no longer explosive. It also makes
complete economic sense, which you can verify by going through the
dynamics of the different demand, supply and monetary shocks.

\hypertarget{generalized-solution}{%
\section{Generalized solution}\label{generalized-solution}}

Sometimes for a model \(E\) is singular. A more general solution was
proposed by Klein (2000), that doesn't require \(E\) to be non-singular.
This uses a generalized Schur decomposition instead of an eigenvalue one
and is applied to the structural model represented by the matrix pencil
\((A,E)\), and is considered much more numerically stable (see Pappas,
Laub, and Sandell (1980)). The generalized Schur form of \((A,E)\) is
\((QTZ', QSZ')\), so we can write the model as \[
E \begin{bmatrix} z_t \\ x_{t+1}^e \end{bmatrix} \equiv QTZ' \begin{bmatrix} z_t \\ x_{t+1}^e \end{bmatrix} \equiv QT \begin{bmatrix} \xi_t^s \\ \xi_{t+1}^u \end{bmatrix} 
\] and \[
A \begin{bmatrix} z_{t-1} \\ x_t \end{bmatrix} \equiv QSZ' \begin{bmatrix} z_{t-1} \\ x_t \end{bmatrix} \equiv QS\begin{bmatrix} \xi_{t-1}^s \\ \xi_t^u \end{bmatrix} 
\] so the model pre-multiplied by \(Q'\) is \[
T \begin{bmatrix} \xi_{t+1}^s \\ \xi_{t+1}^u \end{bmatrix} = S \begin{bmatrix} \xi_t^s \\ \xi_t^u \end{bmatrix} + Q'B\varepsilon_t
\]

We use the function \texttt{gqz} from the library \texttt{geigen} for
this

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\OtherTok{\textless{}{-}}\NormalTok{ geigen}\SpecialCharTok{::}\FunctionTok{gqz}\NormalTok{(A, E, }\AttributeTok{sort=}\StringTok{"S"}\NormalTok{) }\CommentTok{\# Option "S" puts the stable roots first}
\end{Highlighting}
\end{Shaded}

We can check that this is actually saddle path using \texttt{gevalues()}
to get all the eigenvalues from the generalized Schur decomposition, and
the unstable ones are

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{e }\OtherTok{\textless{}{-}}\NormalTok{ geigen}\SpecialCharTok{::}\FunctionTok{gevalues}\NormalTok{(d)}
\NormalTok{e[}\FunctionTok{abs}\NormalTok{(e) }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.071552+0.092734i 1.071552-0.092734i
\end{verbatim}

The number of \emph{stable} roots is returned in \texttt{d\$sdim} which
is 3.

We then modify our solution function to calculate \texttt{Ns} and
\texttt{Gs} using the matrix \texttt{Z} and a generalized version of the
formula for \(G\) and calculate the reduced form model \texttt{Ps} and
`Q which are \begin{align}
    N_s &= Z_{21} Z_{11}^{-1} \\
    H   &= (E_{11} + E_{12} N_s)^{-1} \\
    W   &= (E_{21} + E_{22} N_s) H\\
    G_s &= (A_{22} - W A_{12})^{-1} (W B_1 - B_2) \\
    P_s &= H (A_{11} + A_{12} N_s) \\
    Q_s &= H (B_1 + A_{12} G_s)
\end{align} Verify this yourself with a bit of matrix algebra!

The R code for this is

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{solveGenBK }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(E,A,B,n) \{}
\NormalTok{  d  }\OtherTok{\textless{}{-}}\NormalTok{ geigen}\SpecialCharTok{::}\FunctionTok{gqz}\NormalTok{(A, E, }\AttributeTok{sort=}\StringTok{"S"}\NormalTok{) }
\NormalTok{  np }\OtherTok{\textless{}{-}}\NormalTok{ d}\SpecialCharTok{$}\NormalTok{sdim}
\NormalTok{  ns }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(E)}
  \FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Number of unstable roots is"}\NormalTok{, ns}\SpecialCharTok{{-}}\NormalTok{np))}
  \ControlFlowTok{if}\NormalTok{ (n }\SpecialCharTok{==}\NormalTok{ np) \{}
\NormalTok{    iz }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n}
\NormalTok{    ix }\OtherTok{\textless{}{-}}\NormalTok{ (n}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{ns}
\NormalTok{    Ns }\OtherTok{\textless{}{-}}\NormalTok{ d}\SpecialCharTok{$}\NormalTok{Z[ix,iz] }\SpecialCharTok{\%*\%} \FunctionTok{solve}\NormalTok{(d}\SpecialCharTok{$}\NormalTok{Z[iz,iz])}
\NormalTok{    H  }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(E[iz,iz] }\SpecialCharTok{+}\NormalTok{ E[iz,ix] }\SpecialCharTok{\%*\%}\NormalTok{ Ns)}
\NormalTok{    W  }\OtherTok{\textless{}{-}}\NormalTok{ (E[ix,iz] }\SpecialCharTok{+}\NormalTok{ E[ix,ix] }\SpecialCharTok{\%*\%}\NormalTok{ Ns) }\SpecialCharTok{\%*\%}\NormalTok{ H}
\NormalTok{    Gs }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{((A[ix,ix] }\SpecialCharTok{{-}}\NormalTok{ W }\SpecialCharTok{\%*\%}\NormalTok{ A[iz,ix]), (W }\SpecialCharTok{\%*\%}\NormalTok{ B[iz,] }\SpecialCharTok{{-}}\NormalTok{ B[ix,]))}
\NormalTok{    As }\OtherTok{\textless{}{-}}\NormalTok{ H }\SpecialCharTok{\%*\%}\NormalTok{ (A[iz,iz] }\SpecialCharTok{+}\NormalTok{ A[iz,ix] }\SpecialCharTok{\%*\%}\NormalTok{ Ns)}
\NormalTok{    Bs }\OtherTok{\textless{}{-}}\NormalTok{ H }\SpecialCharTok{\%*\%}\NormalTok{ (B[iz,] }\SpecialCharTok{+}\NormalTok{ A[iz,ix] }\SpecialCharTok{\%*\%}\NormalTok{ Gs)}
    \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{P=}\FunctionTok{cbind}\NormalTok{(}\FunctionTok{rbind}\NormalTok{(As,Ns),}\FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{,ns,ns}\SpecialCharTok{{-}}\NormalTok{n)), }\AttributeTok{Q=}\FunctionTok{rbind}\NormalTok{(Bs, Gs)))}
\NormalTok{    \} }
  \ControlFlowTok{else}\NormalTok{ \{ }
    \FunctionTok{return}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{) }
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Using this on our original model gives

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{S  }\OtherTok{\textless{}{-}} \FunctionTok{solveGenBK}\NormalTok{(E,A,B,np)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Number of unstable roots is 2"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Ps }\OtherTok{\textless{}{-}}\NormalTok{ S}\SpecialCharTok{$}\NormalTok{P}
\NormalTok{Qs }\OtherTok{\textless{}{-}}\NormalTok{ S}\SpecialCharTok{$}\NormalTok{Q}
\end{Highlighting}
\end{Shaded}

and comparing \texttt{Ps} and \texttt{Qs} with \texttt{P} and \texttt{Q}
obtained using Blanchard-Kahn\index{Blanchard-Kahn!method} we find

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{max}\NormalTok{(}\FunctionTok{abs}\NormalTok{(P}\SpecialCharTok{{-}}\NormalTok{Ps), }\FunctionTok{abs}\NormalTok{(Q}\SpecialCharTok{{-}}\NormalTok{Qs)), }\DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0
\end{verbatim}

They are, as expected, the same -- at least up to 12 decimal places,
which should be enough.

\hypertarget{singular-models-optimal-policy}{%
\section{Singular models: optimal
policy}\label{singular-models-optimal-policy}}

However, this is an easy test. What we need is to use a model that can't
be solved using the BK method.\index{Blanchard-Kahn!method} Under
optimal policy, the interest rate instrument rule is replaced with a
targeting rule, so that \[
  \pi_t = -\mu \Delta y_t - \varepsilon^3_t
\] for some value of \(\mu\) that reflects the optimal trade-off between
output (gap) growth and inflation, and we've included a disturbance
which we can loosely describe as a monetary policy shock. We modify the
model above by dropping the Taylor rule\index{Taylor!rule} in favour of
the targeting rule. This requires a lagged value of \(y\) to be created.
The following does the trick

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nf }\OtherTok{\textless{}{-}} \DecValTok{2}
\NormalTok{ne }\OtherTok{\textless{}{-}} \DecValTok{3}
\NormalTok{ns }\OtherTok{\textless{}{-}} \DecValTok{6}      \CommentTok{\# One extra state}
\NormalTok{np }\OtherTok{\textless{}{-}}\NormalTok{ ns}\SpecialCharTok{{-}}\NormalTok{nf}
\NormalTok{mu }\OtherTok{\textless{}{-}} \FloatTok{0.75}   \CommentTok{\# Representative trade{-}off}

\NormalTok{labels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"e\^{}1"}\NormalTok{,}\StringTok{"e\^{}2"}\NormalTok{,}\StringTok{"ylag"}\NormalTok{,}\StringTok{"i"}\NormalTok{,}\StringTok{"y"}\NormalTok{,}\StringTok{"pi"}\NormalTok{) }\CommentTok{\# New variable order}

\NormalTok{E }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{,ns,ns)}
\NormalTok{A }\OtherTok{\textless{}{-}}\NormalTok{ E}
\NormalTok{B }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{,ns,ne)}
\NormalTok{B[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{B[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{B[}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{1}

\FunctionTok{diag}\NormalTok{(E[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]) }\OtherTok{\textless{}{-}} \DecValTok{1}
\FunctionTok{diag}\NormalTok{(A[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(rho\_1, rho\_2)}
\NormalTok{A[}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{]           }\OtherTok{\textless{}{-}} \DecValTok{1}

\NormalTok{E[}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{]           }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{A[}\DecValTok{4}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{6}\NormalTok{)]     }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\SpecialCharTok{/}\NormalTok{mu)}

\NormalTok{E[}\DecValTok{5}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{)] }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\SpecialCharTok{/}\NormalTok{sigma, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\SpecialCharTok{/}\NormalTok{sigma)}
\NormalTok{A[}\DecValTok{5}\NormalTok{,}\DecValTok{5}\NormalTok{]           }\OtherTok{\textless{}{-}} \DecValTok{1}

\NormalTok{E[}\DecValTok{6}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{)]     }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, beta)}
\NormalTok{A[}\DecValTok{6}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{)]     }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{kappa, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The new \(E\) and \(A\) system matrices are then \[  
E = \left[\begin{matrix}1 &0 &0 &0 &0 &0 \\0 &1 &0 &0 &0 &0 \\0 &0 &1 &0 &0 &0 \\0 &0 &1 &0 &0 &0 \\1 &0 &0 &-0.5 &1 &0.5 \\0 &1 &0 &0 &0 &0.99 \\\end{matrix}\right] 
\] \[  
A = \left[\begin{matrix}0.9 &0 &0 &0 &0 &0 \\0 &0.8 &0 &0 &0 &0 \\0 &0 &0 &0 &1 &0 \\0 &0 &1 &0 &0 &-1.333 \\0 &0 &0 &0 &1 &0 \\0 &0 &0 &0 &-0.075 &1 \\\end{matrix}\right] 
\] Now we have a singular model. The matrix \(E\) is clearly singular as
rows 3 and 4 are identical. But we have a problem using the code above.
To use it we need the matrices \(H\) and \((A_{22} - W A_{21})\) to be
non-singular. What to do?

There are two ways out. Klein (2000) gives a solution that depends on
the decomposed matrix pencil, which is what is typically implemented,
but you don't actually need it although it is easiest. Instead, all you
need to do is reorder the equations.

The real problem is that with a targeting rule that doesn't include the
interest rate, and the interest rate is now only determined by the IS
curve. But we can swap the location of any two rows of the model
arbitrarily. If we swap the positions of the equations for the IS curve
and the targeting rule (rows 4 and 5) using the following

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{E[}\DecValTok{4}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,] }\OtherTok{\textless{}{-}}\NormalTok{ E[}\DecValTok{5}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,]}
\NormalTok{A[}\DecValTok{4}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,] }\OtherTok{\textless{}{-}}\NormalTok{ A[}\DecValTok{5}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,]}
\NormalTok{B[}\DecValTok{4}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,] }\OtherTok{\textless{}{-}}\NormalTok{ B[}\DecValTok{5}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

then the model is unchanged but now we have \[  
E_{11} = \left[\begin{matrix}1 &0 &0 &0 \\0 &1 &0 &0 \\0 &0 &1 &0 \\1 &0 &0 &-0.5 \\\end{matrix}\right] 
\] so \(E_{11} + E_{12}N\) is likely non-singular (it is). Also, note
after the re-ordering \(A_{22}\) is \[  
A_{22} = \left[\begin{matrix}0 &-1.33 \\-0.07 &1 \\\end{matrix}\right] 
\] which is guaranteed non-singular for zero \(W\). We can now proceed
as before. First, check for saddle path stability

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{e }\OtherTok{\textless{}{-}}\NormalTok{ geigen}\SpecialCharTok{::}\FunctionTok{gevalues}\NormalTok{(geigen}\SpecialCharTok{::}\FunctionTok{gqz}\NormalTok{(A, E, }\AttributeTok{sort=}\StringTok{"S"}\NormalTok{))}
\NormalTok{e[}\FunctionTok{abs}\NormalTok{(e) }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.378195     -Inf
\end{verbatim}

which confirms that it has a unique saddle path stable solution. This is

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{So }\OtherTok{\textless{}{-}} \FunctionTok{solveGenBK}\NormalTok{(E,A,B,np)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Number of unstable roots is 2"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Po }\OtherTok{\textless{}{-}}\NormalTok{ So}\SpecialCharTok{$}\NormalTok{P}
\NormalTok{Qo }\OtherTok{\textless{}{-}}\NormalTok{ So}\SpecialCharTok{$}\NormalTok{Q}
\end{Highlighting}
\end{Shaded}

The solved model is then

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Po}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     [,1]      [,2]       [,3] [,4] [,5] [,6]
[1,]  0.9  0.000000  0.0000000    0    0    0
[2,]  0.0  0.800000  0.0000000    0    0    0
[3,]  0.0 -1.863455  0.7329156    0    0    0
[4,]  1.8 -1.241330 -0.2446879    0    0    0
[5,]  0.0 -1.863455  0.7329156    0    0    0
[6,]  0.0  1.397591  0.2003133    0    0    0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Qo}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     [,1]      [,2]       [,3]
[1,]    1  0.000000  0.0000000
[2,]    0  1.000000  0.0000000
[3,]    0 -2.329318 -0.7329156
[4,]    2 -1.551663  0.2446879
[5,]    0 -2.329318 -0.7329156
[6,]    0  1.746989 -0.2003133
\end{verbatim}

\hypertarget{optimal-impulse-responses}{%
\subsection{Optimal impulse responses}\label{optimal-impulse-responses}}

We can now simulate the model under optimal policy and plot using

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{zo }\OtherTok{\textless{}{-}} \FunctionTok{impulse\_responses}\NormalTok{(Po, Qo, Omega, labels, T) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{ylag) }\CommentTok{\# Drop duplicate series}
\FunctionTok{response\_plot}\NormalTok{(zo, }\StringTok{"Impulse responses: Optimal policy"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{BK_files/figure-pdf/unnamed-chunk-24-1.pdf}

}

\end{figure}

\hypertarget{dummy-jumps}{%
\section{Dummy jumps}\label{dummy-jumps}}

But this isn't the only way to get this to work. Effectively what we
just did was create an extra predetermined variable and reorder the
system to give us non-singularity. What if instead of including an
unused \(i_{t-1}\) on the right hand side, we instead include an unused
\(i^e_{t+1}\) on the left hand side? So we swap to having one more jump
variable, one less predetermined one?

Compare the following to the previous model. When we pick out the
interest rate we do so on the right hand side of the matrix equation,
not the left as before.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ns }\OtherTok{\textless{}{-}} \DecValTok{6}      \CommentTok{\# One extra state}
\NormalTok{nf }\OtherTok{\textless{}{-}} \DecValTok{3}      \CommentTok{\# And one extra jump}
\NormalTok{np }\OtherTok{\textless{}{-}}\NormalTok{ ns}\SpecialCharTok{{-}}\NormalTok{nf}
\NormalTok{labels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"e\^{}1"}\NormalTok{,}\StringTok{"e\^{}2"}\NormalTok{,}\StringTok{"ylag"}\NormalTok{,}\StringTok{"i"}\NormalTok{,}\StringTok{"y"}\NormalTok{,}\StringTok{"pi"}\NormalTok{) }\CommentTok{\# New variable order}

\NormalTok{E }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{,ns,ns)}
\NormalTok{A }\OtherTok{\textless{}{-}}\NormalTok{ E}
\NormalTok{B }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{,ns,ne)}
\NormalTok{B[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{B[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{B[}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{1}

\FunctionTok{diag}\NormalTok{(E[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]) }\OtherTok{\textless{}{-}} \DecValTok{1}
\FunctionTok{diag}\NormalTok{(A[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(rho\_1, rho\_2)}
\NormalTok{A[}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{]           }\OtherTok{\textless{}{-}} \DecValTok{1}

\NormalTok{E[}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{]           }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{A[}\DecValTok{4}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{6}\NormalTok{)]     }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\SpecialCharTok{/}\NormalTok{mu)}

\NormalTok{E[}\DecValTok{5}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{)]  }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\SpecialCharTok{/}\NormalTok{sigma) }\CommentTok{\# One less coefficient}
\NormalTok{A[}\DecValTok{5}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{)]     }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\NormalTok{sigma, }\DecValTok{1}\NormalTok{)    }\CommentTok{\# One more {-} nothing else changes}

\NormalTok{E[}\DecValTok{6}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{6}\NormalTok{)]     }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, beta)}
\NormalTok{A[}\DecValTok{6}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{)]     }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{kappa, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This is still a singular model, as we can see from \[  
E = \left[\begin{matrix}1 &0 &0 &0 &0 &0 \\0 &1 &0 &0 &0 &0 \\0 &0 &1 &0 &0 &0 \\0 &0 &1 &0 &0 &0 \\1 &0 &0 &0 &1 &0.5 \\0 &1 &0 &0 &0 &0.99 \\\end{matrix}\right] 
\] with column 4 all zeros. Is \emph{this} model saddle path stable?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{e }\OtherTok{\textless{}{-}}\NormalTok{ geigen}\SpecialCharTok{::}\FunctionTok{gevalues}\NormalTok{(geigen}\SpecialCharTok{::}\FunctionTok{gqz}\NormalTok{(A, E, }\AttributeTok{sort=}\StringTok{"S"}\NormalTok{) )}
\NormalTok{e[}\FunctionTok{abs}\NormalTok{(e) }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4.787695e+15 1.378195e+00         -Inf
\end{verbatim}

Again, it is with an extra unstable root for the extra jump variable. We
could simplify the solution. As that top left 3 by 3 block, \(E_{11}\),
is the identity matrix and \(E_{12}\) is all zeros this \texttt{Ei} is
always an identity matrix. However, here we simply re-use
\texttt{solveGenBG}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{So2 }\OtherTok{\textless{}{-}} \FunctionTok{solveGenBK}\NormalTok{(E,A,B,np)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Number of unstable roots is 3"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Po2 }\OtherTok{\textless{}{-}}\NormalTok{ So2}\SpecialCharTok{$}\NormalTok{P}
\NormalTok{Qo2 }\OtherTok{\textless{}{-}}\NormalTok{ So2}\SpecialCharTok{$}\NormalTok{Q}
\end{Highlighting}
\end{Shaded}

Now the solved model is

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Po2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     [,1]      [,2]       [,3] [,4] [,5] [,6]
[1,]  0.9  0.000000  0.0000000    0    0    0
[2,]  0.0  0.800000  0.0000000    0    0    0
[3,]  0.0 -1.863455  0.7329156    0    0    0
[4,]  1.8 -1.241330 -0.2446879    0    0    0
[5,]  0.0 -1.863455  0.7329156    0    0    0
[6,]  0.0  1.397591  0.2003133    0    0    0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Qo2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     [,1]      [,2]       [,3]
[1,]    1  0.000000  0.0000000
[2,]    0  1.000000  0.0000000
[3,]    0 -2.329318 -0.7329156
[4,]    2 -1.551663  0.2446879
[5,]    0 -2.329318 -0.7329156
[6,]    0  1.746989 -0.2003133
\end{verbatim}

which is actually identical to our previous solution. This is because I
have preserved the order of the solved-out variables, and shows that the
swap from a predetermined to a jump variable is completely arbitrary.

\hypertarget{substituting-out}{%
\section{Substituting out}\label{substituting-out}}

But even this doesn't exhaust the possible re-parametrisations of the
model. We can reduce the number of jump variables to 1 and find the same
solution. There exist formal methods for reducing models (see King and
Watson (2002)) but there is an obvious way to proceed here. From the
targeting rule, it must be that \[
  y^e_{t+1} = y_t - \frac{1}{\mu}\pi^e_{t+1}
\] as the expected shock is zero. This means the IS curve can be
rewritten \[
y_t = y_t - \frac{1}{\mu}\pi^e_{t+1} - \frac{1}{\sigma} \left (i_t - \pi_{t+1}^e \right ) + e_t^1
\] implying \[
i_t =  \left (1 - \frac{\sigma}{\mu} \right )\pi_{t+1}^e + \sigma e_t^1
\] This is the required interest rate consistent with the targeting rule
holding. Now the only jump variable is the inflation rate as we have
eliminated the expected output gap. \begin{align}
y_t    &= y_{t-1} -\frac{1}{\mu} \pi_t  + \frac{1}{\mu} \varepsilon^3_t \\
\pi_t  &= \beta \pi_{t+1}^e + \kappa y_t + e_t^2 \\
i_t    &= \left (1 - \frac{\sigma}{\mu} \right ) \pi_{t+1}^e + \sigma e_t^1 \\
e_t^1  &= \rho_1 e_{t-1}^1 + \varepsilon_t^1 \\ 
e_t^2  &= \rho_2 e_{t-1}^2 + \varepsilon_t^2 
\end{align}

We can code this

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ns }\OtherTok{\textless{}{-}} \DecValTok{5}      \CommentTok{\# Back to 5 states}
\NormalTok{nf }\OtherTok{\textless{}{-}} \DecValTok{1}      \CommentTok{\# Now only one jump}
\NormalTok{np }\OtherTok{\textless{}{-}}\NormalTok{ ns}\SpecialCharTok{{-}}\NormalTok{nf}

\NormalTok{labels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"e\^{}1"}\NormalTok{,}\StringTok{"e\^{}2"}\NormalTok{,}\StringTok{"i"}\NormalTok{,}\StringTok{"y"}\NormalTok{,}\StringTok{"pi"}\NormalTok{) }\CommentTok{\# Lose a y}

\NormalTok{E }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{,ns,ns)}
\NormalTok{A }\OtherTok{\textless{}{-}}\NormalTok{ E}
\NormalTok{B }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{,ns,ne)}
\NormalTok{B[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{B[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{B[}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{1}

\FunctionTok{diag}\NormalTok{(E[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]) }\OtherTok{\textless{}{-}} \DecValTok{1}
\FunctionTok{diag}\NormalTok{(A[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(rho\_1, rho\_2)}

\NormalTok{E[}\DecValTok{3}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{)]  }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{sigma, }\DecValTok{1}\NormalTok{, sigma}\SpecialCharTok{/}\NormalTok{mu}\DecValTok{{-}1}\NormalTok{)}

\NormalTok{A[}\DecValTok{4}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{)]      }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\SpecialCharTok{/}\NormalTok{mu)}

\NormalTok{E[}\DecValTok{5}\NormalTok{,}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{)]  }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, kappa, beta)}
\NormalTok{A[}\DecValTok{5}\NormalTok{,}\DecValTok{5}\NormalTok{]           }\OtherTok{\textless{}{-}} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

and solve it using

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Ss }\OtherTok{\textless{}{-}} \FunctionTok{solveGenBK}\NormalTok{(E,A,B,np)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Number of unstable roots is 1"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Ps }\OtherTok{\textless{}{-}}\NormalTok{ Ss}\SpecialCharTok{$}\NormalTok{P}
\NormalTok{Qs }\OtherTok{\textless{}{-}}\NormalTok{ Ss}\SpecialCharTok{$}\NormalTok{Q}
\end{Highlighting}
\end{Shaded}

Compare the realized of \texttt{Ps}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Ps}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     [,1]      [,2] [,3]       [,4] [,5]
[1,]  0.9  0.000000    0  0.0000000    0
[2,]  0.0  0.800000    0  0.0000000    0
[3,]  1.8 -1.241330    0 -0.2446879    0
[4,]  0.0 -1.863455    0  0.7329156    0
[5,]  0.0  1.397591    0  0.2003133    0
\end{verbatim}

with \texttt{Po} above, say. This is the most `efficient' way of
programming the model, in that we have only five states, and indeed the
repeated behavioural equations we had before have disappeared in the
reduced form solution. Just to confirm this, simulating and plotting
this version gives

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{response\_plot}\NormalTok{(}\FunctionTok{impulse\_responses}\NormalTok{(Ps,Qs,Omega,labels,T), }\StringTok{"Optimal, substituted out"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{BK_files/figure-pdf/unnamed-chunk-32-1.pdf}

}

\end{figure}

which are identical results to those above. But of course \(E\) is now
invertible so we could solve this using the simplest
Blanchard-Kahn\index{Blanchard-Kahn!method} variant. Try it!

\hypertarget{bvar-with-dummies}{%
\chapter{BVAR with dummies}\label{bvar-with-dummies}}

\hypertarget{estimating-bvars-using-us-data}{%
\section{Estimating BVARs using US
data}\label{estimating-bvars-using-us-data}}

We will the Fed Funds rate, annual GDP growth and annual CPI inflation
data from FRED\index{FRED}, retrieved 2023-07-01. These are:

\begin{figure}

{\centering \includegraphics{BVAR_files/figure-pdf/unnamed-chunk-2-1.pdf}

}

\end{figure}

We will build a variety and two and three variable BVARs. More details
on the data are given below.

\hypertarget{bvars-with-dummy-variable-priors}{%
\section{BVARs with dummy variable
priors}\label{bvars-with-dummy-variable-priors}}

Rather than combine a prior distribution with a likelihood and draw from
the resulting joint posterior distribution there is another convenient
way of parameterizing the problem. We can instead add some `dummy
variables' that have the same properties of the prior so we have a
single modified likelihood that incorporates the prior information. This
approach was most obviously adopted by Banbura, Giannone, and Reichlin
(2010). Further discussion of this can be found in Giannone, Lenza, and
Primiceri (2015). In general this is a version of the Theil and
Goldberger (1961) \textbf{\emph{mixed estimator}} given a Bayesian
interpretation.

\hypertarget{var-model}{%
\subsection{VAR model}\label{var-model}}

Simple bi-variate two-lag VAR model:

\begin{align}
  \left[\begin{matrix}g_t \cr \pi_t\end{matrix}\right] &=
  \left[\begin{matrix}c_1 \cr c_2\end{matrix}\right] +
  \left[\begin{matrix}b_{11} & b_{12} \cr
    b_{21} & b_{22}\end{matrix}\right]
\left[\begin{matrix}g_{t-1} \cr \pi_{t-1} \end{matrix}\right] +
   \left[\begin{matrix}d_{11} & d_{12} \cr
    d_{21} & d_{22}\end{matrix}\right]
\left[\begin{matrix}g_{t-2} \cr \pi_{t-2} \end{matrix}\right] +
  \left[\begin{matrix}\nu_{g,t} \cr \nu_{\pi,t}\end{matrix}\right] \\
  \left[\begin{matrix}\nu_{g,t} \cr \nu_{\pi,t}\end{matrix}\right] &\sim N(0, \Sigma)
\end{align}

\hypertarget{bvar-hyperparameters}{%
\section{BVAR hyperparameters}\label{bvar-hyperparameters}}

We will (similarly to the straightforward Minnesota prior) need some
control parameters:

\begin{itemize}
\tightlist
\item
  \(\tau\) controls the overall tightness of the prior for the AR
  coefficients
\item
  \(d\) controls the prior on higher lags;
\item
  \(\lambda\) controls the prior on constants;
\item
  \(\gamma\) controls the prior on the sum of coefficients;
\item
  \(\delta\) controls the cointegration prior;
\end{itemize}

where

\begin{itemize}
\tightlist
\item
  \(\sigma_i\) standard deviation of error terms from individual OLS
  regressions;
\item
  \(\mu_i\) sample means of the data.
\end{itemize}

\hypertarget{first-lag}{%
\subsubsection{First lag}\label{first-lag}}

Now consider the following artificial data for the \textbf{first lag}.
We construct some dummy observations of the dependent and explanatory
variables that look like: \begin{equation}
  Y_{D,1} = \left[\begin{matrix}\frac{1}{\tau}\sigma_1 & 0 \cr
    0 & \frac{1}{\tau}\sigma_2\end{matrix}\right]
\end{equation}

and

\begin{equation}
  X_{D,1} = \left [ \begin{matrix}0 & \frac{1}{\tau}\sigma_1 & 0 & 0 & 0\cr
    0 & 0 & \frac{1}{\tau}\sigma_2 & 0 & 0\end{matrix}\right]
\end{equation}

Intuition:

\begin{equation}
  \left[\begin{matrix} \frac{\sigma_1}{\tau} & 0 \cr
    0 & \frac{\sigma_2}{\tau}\end{matrix}\right] =
  \left[\begin{matrix} 0 & \frac{\sigma_1}{\tau} & 0 & 0 & 0\cr
   0 & 0 & \frac{\sigma_2}{\tau} & 0 & 0\end{matrix} \right]
\left[\begin{matrix}c_1 & c_2 \cr
  b_{11} & b_{21} \cr
  b_{12} & b_{22} \cr
  d_{11} & d_{21} \cr
  d_{12} & d_{22}\end{matrix}\right]
+
  \left[\begin{matrix}\xi_{11} & \xi_{12} \cr \xi_{21} & \xi_{22} \end{matrix}\right]
\end{equation}

Multiplying out we get:

\begin{equation}
  \left[\begin{matrix}\frac{\sigma_1}{\tau} & 0 \cr
    0 & \frac{\sigma_2}{\tau}\end{matrix}\right]
=
  \left[\begin{matrix}\frac{\sigma_1}{\tau}b_{11} & \frac{\sigma_1}{\tau}b_{21}\cr
  \frac{\sigma_2}{\tau}b_{12} &  \frac{\sigma_2}{\tau}b_{22}\end{matrix} \right]
+
  \left[\begin{matrix}\xi_{11} & \xi_{12} \cr \xi_{21} & \xi_{22} \end{matrix}\right]
\end{equation}

Concentrating on the first row, notice:

\begin{equation}
  \frac{\sigma_1}{\tau} = \frac{\sigma_1}{\tau}b_{11} + \xi_{11}
\end{equation}

implying:

\begin{equation}
  b_{11} = 1 - \frac{\tau}{\sigma_1}\xi_{11}
\end{equation}

so we can write:

\begin{equation}
  b_{11} \sim N\left(1, \frac{\tau^2var(\xi_{11})}{\sigma^2_1}\right)
\end{equation}

as \(E[b_{11}] = 1 - \frac{\tau}{\sigma_1}E[\xi_{11}] = 1\) and the
variance is easily derived. Similarly: \begin{equation}
  b_{12} = - \frac{\tau}{\sigma_1}\xi_{12}
\end{equation} which is clearly zero in expectation.

\hypertarget{further-priors}{%
\subsection{Further priors}\label{further-priors}}

\hypertarget{higher-lags}{%
\subsubsection{Higher lags}\label{higher-lags}}

Rather than derive the implications we state the rest of the dummy
priors. Consider the following artificial data for the \textbf{second}
lag:

\begin{align*}
  Y_{D,2} &= \left[\begin{matrix}0 & 0 \cr 0 & 0\end{matrix}\right] \\
  X_{D,2} &= \left[\begin{matrix}0 & 0 & 0 & \frac{\sigma_1 2^d}{\tau} & 0 \cr
    0 & 0 & 0 & 0 & \frac{\sigma_2 2^d}{\tau} \end{matrix}\right]
\end{align*}

We can multiply these out and check the properties, in particular we can
verify in the same way as for the first lag that:

\begin{equation}
  b_{ji} \sim N\left(0, \frac{1}{4}\frac{\tau^2var(\xi_{ji})} {2^d\sigma^2_j}\right)
\end{equation}

for \(j=1,...N\), \(i=1,...l\).

\hypertarget{constant}{%
\subsubsection{Constant}\label{constant}}

Consider the following artificial data for the \textbf{constant}:

\begin{align*}
  Y_{D,3} &= \left[\begin{matrix}0 & 0 \end{matrix}\right] \\
  X_{D,3} &= \left[\begin{matrix}\lambda & 0 & 0 & 0 & 0 \end{matrix}\right]
\end{align*}

so \(\lambda c_1 = \varepsilon_1\) and \(\lambda c_2 = \varepsilon_2\).
As \(\lambda \rightarrow \infty\) the prior is implemented more tightly.

\hypertarget{covariances-1}{%
\subsubsection{Covariances}\label{covariances-1}}

Dummy observations to implement the prior on the error covariance matrix
are: \begin{align*}
  Y_{D,4} &= \left[\begin{matrix}\sigma_1 & 0 \cr 0 & \sigma_2\end{matrix}\right] \\
  X_{D,4} &= \left[\begin{matrix}0 & 0 & 0 & 0 & 0 \cr
    0 & 0 & 0 & 0 & 0 \end{matrix}\right]
\end{align*}

with the magnitude of the diagonal elements of \(\Sigma\) controlled by
the scale of the diagonal elements of \(Y_{D,4}\), as larger diagonal
elements implement the prior belief that the variance of \(\nu_1\) and
\(\nu_2\) is larger.

Banbura, Giannone, and Reichlin (2010) stop here, but there are
additional priors that could be added.

\hypertarget{sum-of-coefficients}{%
\subsubsection{Sum of coefficients}\label{sum-of-coefficients}}

We could add a prior that reflects the belief that the \textbf{sum of
coefficients} on `own' lags add up to 1. This is an additional `unit
root'-style prior. Consider:

\begin{align*}
  Y_{D,5} &= \left[\begin{matrix} \gamma\mu_1 & 0\cr 0 & \gamma\mu_2\end{matrix}\right] \\
  X_{D,5} &= \left [ \begin{matrix} 0 & \gamma\mu_1 & 0 & \gamma\mu_1 & 0\cr
    0 & 0 & \gamma\mu_2 & 0 & \gamma\mu_2\end{matrix}\right]
\end{align*}

where \(\mu_1\) is the sample mean of \(y_t\) and \(\mu_2\) is the
sample mean of \(x_t\). Note that these dummy observations imply prior
means of the form \(b_{ii} + d_{ii} = 1\) where \(i = 1, 2\) and
\(\gamma\) controls the tightness of the prior. As
\(\gamma \rightarrow \infty\) the prior is implemented more tightly.
Forecast growth rates eventually converge to their sample averages.

\hypertarget{trends}{%
\subsubsection{Trends}\label{trends}}

We can also specify \textbf{common stochastic trend} dummies:

\begin{align*}
  Y_{D,6} &= \left[\begin{matrix}\delta\mu_1 & \delta\mu_2 \end{matrix}\right] \\
  X_{D,6} &= \left [\begin{matrix}\delta & \delta\mu_1 & \delta\mu_2 & \delta\mu_1 & \delta\mu_2 \end{matrix}\right]
\end{align*}

where this imposes that the coefficients are consistent with limiting
the amount of drift between the predictions at their average values.

\hypertarget{implementation}{%
\subsection{Implementation}\label{implementation}}

The data and the artificial data are now stacked:

\begin{equation}
  Y^* = \left[\begin{matrix} g_3 & \pi_3 \cr
    \vdots & \vdots \cr
    g_T & \pi_T \cr
    \frac{1}{\tau}\sigma_1 & 0 \cr
    0 & \frac{1}{\tau}\sigma_2\cr
    0 & 0 \cr
    0 & 0 \cr
    0 & 0 \cr
    \sigma_1 & 0 \cr
    0 & \sigma_2 \cr
    \gamma\mu_1 & 0 \cr
    0 & \gamma\mu_2 \cr
    \delta\mu_1 & \delta\mu_2 \end{matrix}\right], \quad
X^* = \left [ \begin{matrix}1 & g_2 & \pi_2 & g_1 & \pi_1 \cr
  \vdots & \vdots & \vdots & \vdots & \vdots \cr
  1 & g_{T-1} & \pi_{T-1} & g_{T-2} & \pi_{T-2} \cr
  0 & \frac{1}{\tau}\sigma_1 & 0 & 0 & 0\cr
  0 & 0 & \frac{1}{\tau}\sigma_2 & 0 & 0\cr
  0 & 0 & 0 & \frac{\sigma_1 2^d}{\tau} & 0 \cr
  0 & 0 & 0 & 0 & \frac{\sigma_2 2^d}{\tau} \cr
  \lambda & 0 & 0 & 0 & 0 \cr
  0 & 0 & 0 & 0 & 0 \cr
  0 & 0 & 0 & 0 & 0 \cr
  0 & \gamma\mu_1 & 0 & \gamma\mu_1 & 0 \cr
  0 & 0 & \gamma\mu_2 & 0 & \gamma\mu_2 \cr
  \delta & \delta\mu_1 & \delta\mu_2 & \delta\mu_1 & \delta\mu_2 \end{matrix}\right]
\end{equation}

Estimation via Gibbs sampling now proceeds in a very straightforward
way. There is no need to draw for the prior separately.

\hypertarget{examples-1}{%
\section{Examples}\label{examples-1}}

First we use quarterly US Growth (FRED series
\href{https://fred.stlouisfed.org/series/A191RO1Q156NBEA}{A191RO1Q156NBEA})
and CPI (FRED series
\href{https://fred.stlouisfed.org/series/CPALTT01USQ661S}{CPALTT01USQ661S})
expressed as the annual inflation rate from 1961-01-01 to 2023-01-01 in
a bi-variate BVAR. The last ten observations are:

\begin{longtable}[]{@{}lrr@{}}
\toprule\noalign{}
Date & Growth & Inflation \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
2020-10-01 & -1.5 & 1.224176 \\
2021-01-01 & 1.2 & 1.905310 \\
2021-04-01 & 12.5 & 4.776278 \\
2021-07-01 & 5.0 & 5.264633 \\
2021-10-01 & 5.7 & 6.765892 \\
2022-01-01 & 3.7 & 8.023109 \\
2022-04-01 & 1.8 & 8.556077 \\
2022-07-01 & 1.9 & 8.284860 \\
2022-10-01 & 0.9 & 7.110821 \\
2023-01-01 & 1.8 & 5.769521 \\
\end{longtable}

We specify a VAR with two lags, and use it to forecast 12 periods ahead.
The BVAR are specified using the names above, with only \texttt{tau}
particularly binding in this case. We set the total number of iterations
in each case to 20000 and discard the first half. The parameter
\texttt{nb} is used to set how much back data should appear in a fan
chart.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\#\#\#\#\#\#\#\#}
\CommentTok{\# Options}
\DocumentationTok{\#\#\#\#\#\#\#\#\#}
\NormalTok{nf }\OtherTok{\textless{}{-}} \DecValTok{12} \CommentTok{\# Max forecast horizon}
\NormalTok{nb }\OtherTok{\textless{}{-}} \DecValTok{21} \CommentTok{\# No. back periods plotted in graphs}
\NormalTok{l  }\OtherTok{\textless{}{-}} \DecValTok{2}  \CommentTok{\# Number of lags in VAR}

\CommentTok{\# specify parameters of the Minnesota{-}type prior}
\NormalTok{tau    }\OtherTok{\textless{}{-}}\NormalTok{ .}\DecValTok{1}   \CommentTok{\# controls prior on own 1st lags (1 makes wibbly)}
\NormalTok{d      }\OtherTok{\textless{}{-}} \DecValTok{1}    \CommentTok{\# decay for higher lags}
\NormalTok{lambda }\OtherTok{\textless{}{-}} \DecValTok{1}    \CommentTok{\# prior for the constant}
\NormalTok{gamma  }\OtherTok{\textless{}{-}} \DecValTok{1}    \CommentTok{\# sum of coefficients unit roots}
\NormalTok{delta  }\OtherTok{\textless{}{-}} \DecValTok{1}    \CommentTok{\# cointegration prior}

\CommentTok{\# Gibbs control}
\NormalTok{reps }\OtherTok{\textless{}{-}} \DecValTok{20000} \CommentTok{\# total numbers of Gibbs iterations}
\NormalTok{burn }\OtherTok{\textless{}{-}} \DecValTok{10000} \CommentTok{\# number of burn{-}in iterations}
\end{Highlighting}
\end{Shaded}

In what follows we vary \texttt{tau} and the lag length to illustrate
their effects. To do this we create the augmented data and then run the
Gibbs sampler, using:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create augmented data}
\NormalTok{Yplus }\OtherTok{\textless{}{-}} \FunctionTok{augmentData}\NormalTok{(Y, l, tau, d, lambda, gamma, delta)}

\CommentTok{\# Run Gibbs sampler}
\NormalTok{out   }\OtherTok{\textless{}{-}} \FunctionTok{Gibbs\_estimate}\NormalTok{(Yplus[[}\DecValTok{1}\NormalTok{]], Yplus[[}\DecValTok{2}\NormalTok{]], reps, burn, }\DecValTok{1}\NormalTok{, nf)}
\end{Highlighting}
\end{Shaded}

where \texttt{Y} contains the data in a dataframe/tibble with the date
in the first column as in the data example above. The code strips out
the date and then uses the remaining \(N\) columns in the BVAR. See the
Code Appendix for the details of the functions.

The output contains any \textbf{\emph{forecast}} draws from the Gibbs
sampler in the third list element from the \texttt{Gibbs\_estimate()}
function. The first two elements are coefficient draws. Two further
functions plots the fan charts using the Gibbs draws:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# String to put in subtitle}
\NormalTok{controls }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"Lag length "}\NormalTok{, l, }\StringTok{": tau="}\NormalTok{, tau, }\StringTok{", d="}\NormalTok{, d,}
                   \StringTok{", lambda="}\NormalTok{, lambda, }\StringTok{", gamma="}\NormalTok{, gamma, }\StringTok{", delta="}\NormalTok{, delta)}

\CommentTok{\# Plots}
\FunctionTok{fan\_chart}\NormalTok{(Y, out[[}\DecValTok{3}\NormalTok{]], controls, nb)}
\NormalTok{p           }\OtherTok{\textless{}{-}} \FunctionTok{coeff\_plot}\NormalTok{(Y, l, out[[}\DecValTok{1}\NormalTok{]], out[[}\DecValTok{2}\NormalTok{]], }\DecValTok{333}\NormalTok{, controls)}
\NormalTok{pnum        }\OtherTok{\textless{}{-}}\NormalTok{ pnum}\SpecialCharTok{+}\DecValTok{1}
\NormalTok{pce[[pnum]] }\OtherTok{\textless{}{-}}\NormalTok{ p[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

where the string \texttt{controls} is put in the chart subtitle and the
coefficient densities. It can be anything but is a good place to remind
yourself of how you specified the model. Notice we save the coefficient
plots for later use.

\hypertarget{example-1-bvar2-with-tau0.1}{%
\subsection{\texorpdfstring{Example 1: BVAR(2) with
\(\tau=0.1\)}{Example 1: BVAR(2) with \textbackslash tau=0.1}}\label{example-1-bvar2-with-tau0.1}}

\begin{figure}

{\centering \includegraphics{BVAR_files/figure-pdf/estim-1.pdf}

}

\end{figure}

\hypertarget{example-2-bvar2-with-tau1}{%
\subsection{\texorpdfstring{Example 2: BVAR(2) with
\(\tau=1\)}{Example 2: BVAR(2) with \textbackslash tau=1}}\label{example-2-bvar2-with-tau1}}

\begin{figure}

{\centering \includegraphics{BVAR_files/figure-pdf/estim-1.pdf}

}

\end{figure}

\hypertarget{example-3-bvar6-with-tau0.1}{%
\subsection{\texorpdfstring{Example 3: BVAR(6) with
\(\tau=0.1\)}{Example 3: BVAR(6) with \textbackslash tau=0.1}}\label{example-3-bvar6-with-tau0.1}}

\begin{figure}

{\centering \includegraphics{BVAR_files/figure-pdf/estim-1.pdf}

}

\end{figure}

\hypertarget{example-4-bvar6-with-tau1}{%
\subsection{\texorpdfstring{Example 4: BVAR(6) with
\(\tau=1\)}{Example 4: BVAR(6) with \textbackslash tau=1}}\label{example-4-bvar6-with-tau1}}

\begin{figure}

{\centering \includegraphics{BVAR_files/figure-pdf/estim-1.pdf}

}

\end{figure}

\hypertarget{coefficient-estimates}{%
\subsection{Coefficient estimates}\label{coefficient-estimates}}

All of these have underlying parameters. Their estimated posterior
densities are:

\begin{figure}

{\centering \includegraphics{BVAR_files/figure-pdf/unnamed-chunk-9-1.pdf}

}

\end{figure}

\begin{figure}

{\centering \includegraphics{BVAR_files/figure-pdf/unnamed-chunk-9-2.pdf}

}

\end{figure}

\begin{figure}

{\centering \includegraphics{BVAR_files/figure-pdf/unnamed-chunk-9-3.pdf}

}

\end{figure}

\begin{figure}

{\centering \includegraphics{BVAR_files/figure-pdf/unnamed-chunk-9-4.pdf}

}

\end{figure}

\hypertarget{tri-variate-bvar}{%
\section{Tri-variate BVAR}\label{tri-variate-bvar}}

Now we add the FedFunds rate (FRED series
\href{https://fred.stlouisfed.org/series/FEDFUNDS}{FEDFUNDS}), so the
last ten periods of the data set is now:

\begin{longtable}[]{@{}lrrr@{}}
\toprule\noalign{}
Date & Growth & Inflation & FedFunds \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
2020-10-01 & -1.5 & 1.224176 & 0.09 \\
2021-01-01 & 1.2 & 1.905310 & 0.09 \\
2021-04-01 & 12.5 & 4.776278 & 0.07 \\
2021-07-01 & 5.0 & 5.264633 & 0.10 \\
2021-10-01 & 5.7 & 6.765892 & 0.08 \\
2022-01-01 & 3.7 & 8.023109 & 0.08 \\
2022-04-01 & 1.8 & 8.556077 & 0.33 \\
2022-07-01 & 1.9 & 8.284860 & 1.68 \\
2022-10-01 & 0.9 & 7.110821 & 3.08 \\
2023-01-01 & 1.8 & 5.769521 & 4.33 \\
\end{longtable}

Two more examples follow.

\hypertarget{example-5-bvar4-with-tau.1-3-variables}{%
\subsection{\texorpdfstring{Example 5: BVAR(4) with \(\tau=.1\), 3
variables}{Example 5: BVAR(4) with \textbackslash tau=.1, 3 variables}}\label{example-5-bvar4-with-tau.1-3-variables}}

\begin{figure}

{\centering \includegraphics{BVAR_files/figure-pdf/estim-1.pdf}

}

\end{figure}

\begin{figure}

{\centering \includegraphics{BVAR_files/figure-pdf/unnamed-chunk-12-1.pdf}

}

\end{figure}

\hypertarget{example-6-bvar6-with-tau1-3-variables}{%
\subsection{\texorpdfstring{Example 6: BVAR(6) with \(\tau=1\), 3
variables}{Example 6: BVAR(6) with \textbackslash tau=1, 3 variables}}\label{example-6-bvar6-with-tau1-3-variables}}

\begin{figure}

{\centering \includegraphics{BVAR_files/figure-pdf/estim-1.pdf}

}

\end{figure}

\begin{figure}

{\centering \includegraphics{BVAR_files/figure-pdf/unnamed-chunk-14-1.pdf}

}

\end{figure}

\hypertarget{code-appendix}{%
\section{Code appendix}\label{code-appendix}}

You can download the program and functions used for the estimates above
from the links below. Put them in the same directory and they should
recreate exactly (within sampling error) the same graphs as above.
Ensure you have all the libraries available that are loaded at the top
of \texttt{BVARdum.R}.

Main program:

Functions:

\hypertarget{dynare-basics}{%
\chapter{Dynare basics}\label{dynare-basics}}

\hypertarget{software-for-solving-models}{%
\section{Software for solving
models}\label{software-for-solving-models}}

Why do we need it? Any serious model will usually require a computer
package to, say, analyse policy options. Lots of Matlab code (and other
languages) is available to do this but it is convenient to use one of a
few packages that makes the tedious things easier to do. It is also nice
to be able to code up a model very quickly; we may need to marry that up
with data too. Dynare is one such `user friendly' program to we can use
to solve, simulate and estimate DSGE models, and other stuff besides.
Dynare is a \href{https://uk.mathworks.com/products/matlab.html}{Matlab}
program, so you need a licenced product\footnote{Actually
  \href{https://octave.org/}{Octave} will do, but is much slower.}. It
runs on a variety of OS/hardware platforms too, including Debian Linux.
It is available free from \url{http://www.dynare.org/} and there is
\href{http://www.dynare.org/wp-repo/dynarewp001.pdf}{extensive
documentation}.

\begin{tcolorbox}[enhanced jigsaw, breakable, left=2mm, arc=.35mm, toptitle=1mm, colbacktitle=quarto-callout-note-color!10!white, opacityback=0, bottomrule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{What is Dynare?}, colframe=quarto-callout-note-color-frame, coltitle=black, titlerule=0mm, toprule=.15mm, bottomtitle=1mm, rightrule=.15mm, colback=white]

Best answered by the authors themselves, from their website:

\emph{'Dynare \ldots{} handl{[}es{]} a wide class of economic models, in
particular dynamic stochastic general equilibrium (DSGE) and overlapping
generations (OLG) models. The models solved by Dynare include those
relying on the rational expectations hypothesis \ldots{} {[}but also{]}
models where expectations are formed differently {[}including{]} models
where agents have limited rationality or imperfect knowledge of the
state of the economy and, hence, form their expectations through a
learning process.}

\emph{'Dynare offers a user-friendly and intuitive way of describing
{[}\ldots{]} models. It is able to perform simulations of the model
given a calibration of the model parameters and is also able to estimate
these parameters given a dataset. In practice, the user will write a
text file containing the list of model variables, the dynamic equations
linking these variables together, the computing tasks to be performed
and the desired graphical or numerical outputs.}

From \url{http://www.dynare.org/}

\end{tcolorbox}

Although Dynare is frequently the tool of choice (particularly by
researchers) there are alternatives:
\href{https://iris.igpmn.org/}{IRIS},
\href{http://www.texlips.net/yada/}{YADA} etc are also available for
Matlab, with \href{https://gecon.r-forge.r-project.org/}{gEcon} for
\href{https://cran.r-project.org/}{R}, plus a number of commercial
products.

\begin{tcolorbox}[enhanced jigsaw, breakable, left=2mm, arc=.35mm, toptitle=1mm, colbacktitle=quarto-callout-caution-color!10!white, opacityback=0, bottomrule=.15mm, leftrule=.75mm, opacitybacktitle=0.6, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Notable characteristics of Dynare include:}, colframe=quarto-callout-caution-color-frame, coltitle=black, titlerule=0mm, toprule=.15mm, bottomtitle=1mm, rightrule=.15mm, colback=white]

\begin{itemize}
\tightlist
\item
  Their definition of user friendly isn't the same as mine.
\item
  The terminology that they use is at times a little strange; this
  partly reflects the history of the program rather than the methods
  being used.
\item
  Data handling is somewhat \emph{ad hoc}.
\item
  There are a lot of embedded methods (Kalman filter, MCMC, etc) and it
  is not always clear to the casual user how (or why) they are being
  implemented.
\item
  I wouldn't recommend Dynare as a forecast platform although you could
  use it. In my experience IRIS (for one) is better but the learning
  curve is steeper.
\end{itemize}

\end{tcolorbox}

\hypertarget{dynare-model-files}{%
\section{Dynare model files}\label{dynare-model-files}}

We begin with getting a representative DSGE model into Dynare, then
solve and simulate it. Each model in Dynare file requires a fairly
precisely structured text file to control both the specification of the
model (and any data) and what we wish to do with it.

\hypertarget{file-structure}{%
\subsection{File structure}\label{file-structure}}

The Dynare files have the extension \texttt{.mod} so we will often call
them mod files. The structure of a mod file has to be as follows :

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Preamble
\item
  Model
\item
  Steady-state/initial values
\item
  Shocks
\item
  Computation
\end{enumerate}

Many of these are specified as blocks; they have a begin/end form:

\begin{Shaded}
\begin{Highlighting}[]
 \OperatorTok{\textless{}} \VariableTok{Begin} \VariableTok{block} \OperatorTok{\textgreater{};}

    \OperatorTok{\textless{}} \VariableTok{commands} \OperatorTok{\textgreater{};}

 \KeywordTok{end}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\hypertarget{example-model}{%
\section{Example model}\label{example-model}}

We will code the canonical closed economy New Keynesian model with three
variables: Output gap, \(y_t\), inflation deviations from target,
\(\pi_t\), and the nominal interest rate, \(i_t\). Model equations are:
\begin{eqnarray}
y_t   &=& y_{t+1}-\frac{1}{\sigma} (i_t - \pi^e_{t+1}) \\
\pi_t &=& \beta \pi^e_{t+1}+\kappa y_t + \varepsilon_t \\
i_t   &=& \theta_y y_t + \theta_\pi \pi_t
\end{eqnarray} comprising

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  a dynamic IS curve;
\item
  a New Keynesian Phillips curve w/cost-push shock;
\item
  a standard Taylor-type rule
\end{enumerate}

and \(x^e_{t+1}\) is shorthand for \(E_t x_{t+1}\). To meaningfully
simulate this we will need to specify some values for the parameters
(\(\beta\), \(\kappa\), \(\sigma\), \(\theta_\pi\), \(\theta_y\),
\(\hbox{var}(\varepsilon_t)\)), some sort of data and probably some
shocks.

\hypertarget{dynare-code}{%
\subsection{Dynare code}\label{dynare-code}}

\hypertarget{preamble}{%
\subsubsection{Preamble}\label{preamble}}

Define the variables and parameters of the model, and set numerical
values for any parameters. All endogenous variables are declared as of
type \texttt{var} and shocks as \texttt{varexo}. All the parameters are
declared with the command \texttt{parameters}.

For the example model:

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{var} \VariableTok{y}\OperatorTok{,} \VariableTok{i}\OperatorTok{,} \VariableTok{pi}\OperatorTok{;}
\VariableTok{varexo} \VariableTok{eps}\OperatorTok{;}

\VariableTok{parameters} \VariableTok{beta}\OperatorTok{,} \VariableTok{kappa}\OperatorTok{,} \VariableTok{sigma}\OperatorTok{,} \VariableTok{theta\_y}\OperatorTok{,} \VariableTok{theta\_pi}\OperatorTok{;}

\VariableTok{beta} \OperatorTok{=} \FloatTok{0.99}\OperatorTok{;}
\VariableTok{kappa} \OperatorTok{=} \FloatTok{0.05}\OperatorTok{;}
\VariableTok{sigma} \OperatorTok{=} \FloatTok{2}\OperatorTok{;}
\VariableTok{theta\_y} \OperatorTok{=} \FloatTok{0.5}\OperatorTok{;}
\VariableTok{theta\_pi} \OperatorTok{=} \FloatTok{1.5}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\hypertarget{model-1}{%
\subsubsection{Model}\label{model-1}}

Whatever the model, linearize each equation (although Dynare is able to
solve the model from its original form) and enter them into a mod file.
We need to follow some simple rules so \(x^e_{t+1}\) is entered as
\texttt{x(+1)} or \(x_{t-1}\) as \texttt{x(-1)}. We don't specify the
conditioning date as they are always the same.

For a linear model the block usually starts with \texttt{model(linear);}
and conclude with \texttt{end;}. In between we enter all the
(linearized) model equations. This is the big advantage: Dynare doesn't
need you to input the model matrices as a pre-processor does this for
you.

One way of entering the model above is:

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{model}\NormalTok{(}\VariableTok{linear}\NormalTok{)}\OperatorTok{;}

    \VariableTok{y} \OperatorTok{=} \VariableTok{y}\NormalTok{(}\OperatorTok{+}\FloatTok{1}\NormalTok{) }\OperatorTok{{-}}\NormalTok{ (}\FloatTok{1}\OperatorTok{/}\VariableTok{sigma}\NormalTok{)}\OperatorTok{*}\NormalTok{(}\VariableTok{i}\OperatorTok{{-}}\VariableTok{pi}\NormalTok{(}\OperatorTok{+}\FloatTok{1}\NormalTok{))}\OperatorTok{;}
    \VariableTok{pi} \OperatorTok{=} \VariableTok{beta}\OperatorTok{*}\VariableTok{pi}\NormalTok{(}\OperatorTok{+}\FloatTok{1}\NormalTok{) }\OperatorTok{+} \VariableTok{kappa}\OperatorTok{*}\VariableTok{y} \OperatorTok{+} \VariableTok{eps}\OperatorTok{;}
    \VariableTok{i} \OperatorTok{=} \VariableTok{theta\_y}\OperatorTok{*}\VariableTok{y} \OperatorTok{+} \VariableTok{theta\_pi}\OperatorTok{*}\VariableTok{pi}\OperatorTok{;}

\KeywordTok{end}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

Equation order doesn't matter, nor does normalization.

\hypertarget{datasteady-state}{%
\subsubsection{Data/steady state}\label{datasteady-state}}

All models need data, but some can get away with zero (but not missing).
In particular linear models specified as \emph{deviations from steady
state} are, but few others. Most models won't fit any arbitrary data set
-- there will always be some \textbf{implicit residual}. How we deal
with these is important for forecasting, so we will want to supply some
historical data and calculate the implicit residual which can be done.

For now, assume a linear model. Of course linearity doesn't mean zero is
the baseline around which we should simulate a model by applying
arbitrary shocks, but ours is. We can either enter the exact
steady-state values in the \texttt{.mod} file or leave Dynare to find
the steady-state values through numerical methods. In either case, some
initial values are entered with the command \texttt{initval;} followed
by the numerical values for each variables and then \texttt{end;}. By
using the command \texttt{steady;} Dynare uses the steady state of the
model rather than the values you just entered which become the
approximate steady state.

For our example we could use:

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{initval}\OperatorTok{;}

    \VariableTok{y}  \OperatorTok{=} \FloatTok{0}\OperatorTok{;}
    \VariableTok{pi} \OperatorTok{=} \FloatTok{0}\OperatorTok{;}
    \VariableTok{i}  \OperatorTok{=} \FloatTok{0}\OperatorTok{;}

\KeywordTok{end}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

and then

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{steady}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

This is a belt-and-braces approach; we've given it both the exact steady
state and made sure it solve for it too. Beware: Dynare isn't very good
at finding arbitrary steady states without helpful starting values.

\hypertarget{shocks}{%
\subsubsection{Shocks}\label{shocks}}

Shocks are entered using the command \texttt{shocks;} followed by the
variance of each shocks which is declared as \texttt{var}.\footnote{Not
  to be confused with \texttt{var} above. I did say it was
  strange\ldots{}} Once all the shocks are declared we conclude with the
command \texttt{end;}.

For our example, the single shock is declared as:

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{shocks}\OperatorTok{;}

    \VariableTok{var} \VariableTok{eps} \OperatorTok{=} \FloatTok{0.001}\OperatorTok{;}

\KeywordTok{end}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

Often in Dynare things can be done in several different ways. An
alternative formulation is:

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{shocks}\OperatorTok{;}

    \VariableTok{var} \VariableTok{eps}\OperatorTok{;}
    \VariableTok{stderr} \FloatTok{0.001}\OperatorTok{;}

\KeywordTok{end}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

\hypertarget{computation}{%
\subsubsection{Computation}\label{computation}}

Once we have defined preamble, the model, the steady state and the
shocks we can then simulate the model. To simulate a model we usually
enter the command \texttt{stoch\_simul} perhaps with some options that
will tell Dynare what to do with the model. (Why \texttt{stoch\_simul}
you may ask: what's wrong with calling it something like
\texttt{simul}?) Dynare produces outputs such as a model summary,
eigenvalues, the covariance of exogenous shocks, the solved transition
function, model moments, correlations and autocorrelations of the
simulated variables as well as graphs.

A common option is to set the number of periods for the impulse
responses, for example \texttt{IRF=30}, which computes impulse response
functions for 30 periods.

So we would specify

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{stoch\_simul}\NormalTok{(}\VariableTok{IRF}\OperatorTok{=}\FloatTok{30}\NormalTok{)}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

Other additional options will be necessary if we have a lot of shocks or
endogenous variables and so on.

\hypertarget{exercises}{%
\section{Exercises}\label{exercises}}

Let's code this up and look at the simulations. (Hint: copy/paste.)

They're going to look pretty boring. To be more interesting we can do a
couple of things:

\begin{itemize}
\tightlist
\item
  More shock persistence, so
  \(\varepsilon_t = \rho\varepsilon_{t-1} + \epsilon_t\).
\item
  Interest rate persistence, so
  \(i_t=\gamma i_{t-1}+(1-\gamma ) (\theta_y y_t + \theta_{\pi}\pi_t)\)
\end{itemize}

Code these up following the rules above (how do we define
\(\varepsilon\) now?) and check you get the same answers if
\(\rho =\gamma =0\). Set \(\rho =\gamma =0.8\) and see what happens.

\hypertarget{aside-on-estimation}{%
\section{Aside on estimation}\label{aside-on-estimation}}

For estimation we need additional information. Sometimes we need to
introduce measurement equations specifically, unless straight state
variables. Smets/Wouters model, for example, uses

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{//} \VariableTok{measurement} \VariableTok{equations}

\VariableTok{pinfobs} \OperatorTok{=} \VariableTok{pinf} \OperatorTok{+} \VariableTok{constepinf}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

so observed inflation is related to the state variable. We also need to
declare the estimated parameters, which we do together with their
priors. Example again from Smets/Wouters:

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{estimated\_params}\OperatorTok{;}

    \OperatorTok{//} \VariableTok{Param} \VariableTok{Name}\OperatorTok{,} \VariableTok{init}\OperatorTok{,} \VariableTok{lb}\OperatorTok{,} \VariableTok{ub}\OperatorTok{,} \VariableTok{Prior}\OperatorTok{,} \VariableTok{P1}\OperatorTok{,} \VariableTok{P2}
    \OperatorTok{//} \VariableTok{Priors}\OperatorTok{:} \OperatorTok{\textless{}}\VariableTok{P}\OperatorTok{\textgreater{}}\NormalTok{\_}\VariableTok{PDF}\OperatorTok{,} \OperatorTok{\textless{}}\VariableTok{P}\OperatorTok{\textgreater{}} \OperatorTok{=} \VariableTok{BETA}\OperatorTok{,} \VariableTok{GAMMA}\OperatorTok{,} \VariableTok{etc}\NormalTok{\}}

    \VariableTok{stderr} \VariableTok{ea}\OperatorTok{,} \FloatTok{0.4}\OperatorTok{,} \FloatTok{0.01}\OperatorTok{,} \FloatTok{3}\OperatorTok{,} \VariableTok{INV\_GAMMA\_PDF}\OperatorTok{,} \FloatTok{0.1}\OperatorTok{,} \FloatTok{0.2}\OperatorTok{;}
    \VariableTok{cthetaa}\OperatorTok{,} \FloatTok{.9}\OperatorTok{,} \FloatTok{.01}\OperatorTok{,} \FloatTok{.9999}\OperatorTok{,} \VariableTok{BETA\_PDF}\OperatorTok{,} \FloatTok{0.5}\OperatorTok{,} \FloatTok{0.20}\OperatorTok{;}
    \VariableTok{csadjcost}\OperatorTok{,} \FloatTok{6.3325}\OperatorTok{,} \FloatTok{2}\OperatorTok{,} \FloatTok{15}\OperatorTok{,} \VariableTok{NORMAL\_PDF}\OperatorTok{,} \FloatTok{4}\OperatorTok{,} \FloatTok{1.5}\OperatorTok{;}
    \OperatorTok{...}

\KeywordTok{end}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

Observed variables are attached to states using:

\begin{Shaded}
\begin{Highlighting}[]
\VariableTok{varobs} \VariableTok{dy} \VariableTok{dc} \VariableTok{dinve} \VariableTok{labobs} \VariableTok{pinfobs} \VariableTok{dw} \VariableTok{robs}\OperatorTok{;}
\end{Highlighting}
\end{Shaded}

Essentially just another declaration, can be put anywhere before
estimation.

\part{End matter}

\hypertarget{summary}{%
\chapter{Summary}\label{summary}}

In summary, this book has no content whatsoever\ldots{}

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-Adrian}{}}%
Adrian, Tobias, Nina Boyarchenko, and Domenico Giannone. 2019.
{``{Vulnerable Growth}.''} \emph{American Economic Review} 109 (4):
1263--89.

\leavevmode\vadjust pre{\hypertarget{ref-TWA}{}}%
Anderson, T. W. 2004. \emph{An Introduction to Multivariate Statistical
Analysis}. 3rd ed. New York: John Wiley; Sons.

\leavevmode\vadjust pre{\hypertarget{ref-BFP}{}}%
Bahaj, Saleem, Angus Foulis, and Gabor Pinter. 2020. {``Home Values and
Firm Behavior.''} \emph{American Economic Review} 110 (7): 2225--70.
\url{https://doi.org/10.1257/aer.20180649}.

\leavevmode\vadjust pre{\hypertarget{ref-Banbura}{}}%
Banbura, M., D. Giannone, and L. Reichlin. 2010. {``Large {B}ayesian
Vector Auto Regressions.''} \emph{Journal of Applied Econometrics} 25
(1): 71--92.

\leavevmode\vadjust pre{\hypertarget{ref-BlakeCE}{}}%
Blake, Andrew P. 2004. {``Analytic Derivatives in Linear Rational
Expectations Models.''} \emph{Computational Economics} 24 (1): 77--96.

\leavevmode\vadjust pre{\hypertarget{ref-Mumtaz_BOOK}{}}%
Blake, Andrew P, and Haroon Mumtaz. 2017. \emph{Applied Bayesian
Econometrics for Central Bankers}. Revised. Technical Books. Centre for
Central Banking Studies, Bank of England.
\url{https://www.bankofengland.co.uk/-/media/boe/files/ccbs/resources/applied-bayesian-econometrics-for-central-bankers-updated-2017.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-BK80}{}}%
Blanchard, O., and C. Kahn. 1980. {``The Solution of Linear Difference
Models Under Rational Expectations.''} \emph{Econometrica} 48 (5):
1305--11.

\leavevmode\vadjust pre{\hypertarget{ref-HOMLR}{}}%
Boehmke, Brad, and Brandon M. Greenwell. 2019. \emph{Hands-on Machine
Learning with {R}}. The {R} Series. Boca Raton: Chapman \& Hall/CRC.
\url{https://bradleyboehmke.github.io/HOML/}.

\leavevmode\vadjust pre{\hypertarget{ref-Capistran}{}}%
Capistrán, Carlos, and Allan Timmermann. 2009. {``Forecast Combination
with Entry and Exit of Experts.''} \emph{Journal of Business \& Economic
Statistics} 27 (4): 428--40.

\leavevmode\vadjust pre{\hypertarget{ref-CARTER01091994}{}}%
Carter, C. K., and R. Kohn. 1994. {``{On {G}ibbs sampling for state
space models}.''} \emph{Biometrika} 81 (3): 541--53.
\url{https://doi.org/10.1093/biomet/81.3.541}.

\leavevmode\vadjust pre{\hypertarget{ref-Mixtape}{}}%
Cunningham, Scott. 2021. \emph{Causal Inference: The Mixtape}. New Haven
\& London: Yale University Press. \url{https://mixtape.scunning.com/}.

\leavevmode\vadjust pre{\hypertarget{ref-Durbin}{}}%
Durbin, J., and S. J. Koopman. 2001. \emph{Time Series Analysis by State
Space Methods}. Oxford: Oxford University Press.

\leavevmode\vadjust pre{\hypertarget{ref-GL2012}{}}%
Gaglianone, W. P., and L. R. Lima. 2012. {``Constructing Density
Forecasts from Quantile Regressions.''} \emph{Journal of Money, Credit
and Banking} 44 (8): 1589--1607.

\leavevmode\vadjust pre{\hypertarget{ref-GHV}{}}%
Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2019. \emph{Regression
and Other Stories}. Cambridge: Cambridge University Press.
\url{http://www.stat.columbia.edu/~gelman/regression}.

\leavevmode\vadjust pre{\hypertarget{ref-PriorsVAR}{}}%
Giannone, Domenico, Michele Lenza, and Giorgio E. Primiceri. 2015.
{``{Prior Selection for Vector Autoregressions}.''} \emph{The Review of
Economics and Statistics} 97 (2): 436--51.

\leavevmode\vadjust pre{\hypertarget{ref-Green1997}{}}%
Greene, William H. 1997. \emph{Econometric Analysis}. Third. McGraw
Hill.

\leavevmode\vadjust pre{\hypertarget{ref-Hamilton1994}{}}%
Hamilton, J. D. 1994. \emph{Time Series Analysis}. Princeton, NJ:
Princeton University Press.

\leavevmode\vadjust pre{\hypertarget{ref-Hamilton}{}}%
Hamilton, James D. 1994. \emph{Time Series Analysis}. Princeton, NJ:
Princeton University Press.

\leavevmode\vadjust pre{\hypertarget{ref-Hammond2006}{}}%
Hammond, Gill. 2006. {``The Centre for Central Banking Studies.''}
\emph{Quarterly Bulletin} Q2: 191--95.
\url{https://www.bankofengland.co.uk/-/media/boe/files/quarterly-bulletin/2006/the-centre-for-central-banking-studies.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-Harvey89}{}}%
Harvey, Andrew C. 1989. \emph{Forecasting, Structural Time Series Models
and the Kalman Filter}. Cambridge: Cambridge University Press.

\leavevmode\vadjust pre{\hypertarget{ref-HarveyPierse}{}}%
Harvey, Andrew C., and Richard G. Pierse. 1984. {``Estimating Missing
Observations in Economic Time Series.''} \emph{Journal of the American
Statistical Association} 79 (385): 125--31.

\leavevmode\vadjust pre{\hypertarget{ref-ESL}{}}%
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. \emph{The
Elements of Statistical Learning: Data Mining, Inference, and
Prediction}. 2nd ed. New York, NY: Springer.
\url{https://web.stanford.edu/~hastie/ElemStatLearn/}.

\leavevmode\vadjust pre{\hypertarget{ref-SuperText}{}}%
Hvitfeldt, Emil, and Julia Silge. 2021. \emph{Supervised Machine
Learning for Text Analysis in {R}}. Chapman \& Hall: CRC Press.
\url{https://smltar.com/}.

\leavevmode\vadjust pre{\hypertarget{ref-ITSL}{}}%
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.
2021. \emph{An Introduction to Statistical Learning: With Applications
in {R}}. 2nd ed. Springer Texts in Statistics. New York, NY: Springer.
\url{https://www.statlearning.com/}.

\leavevmode\vadjust pre{\hypertarget{ref-Jazwinski}{}}%
Jazwinski, Andrew H. 1970. \emph{Stochastic Processes and Filtering
Theory}. Mineola, N.Y.: Dover Publications Inc.

\leavevmode\vadjust pre{\hypertarget{ref-Kalman}{}}%
Kalman, R. E. 1960. {``A New Approach to Linear Filtering and Prediction
Problems.''} \emph{Transactions of the ASME Journal of Basic
Engineering} 82 (Series D): 35--45.

\leavevmode\vadjust pre{\hypertarget{ref-KimNelson}{}}%
Kim, Chang-Jin, and Charles R. Nelson. 1999. \emph{State-Space Models
with Regime Switching: Classical and Gibbs-Sampling Approaches with
Applications}. MIT Press.

\leavevmode\vadjust pre{\hypertarget{ref-KW2002}{}}%
King, Robert G., and Mark W. Watson. 2002. {``System Reduction and
Solution Algorithms for Singular Linear Difference Systems Under
Rational Expectations.''} \emph{Computational Economics} 20 (1--2):
57--86.

\leavevmode\vadjust pre{\hypertarget{ref-Klein}{}}%
Klein, Paul. 2000. {``Using the Generalized {S}chur Form to Solve a
Multivariate Linear Rational Expectations Model.''} \emph{Journal of
Economic Dynamics and Control} 24 (10): 1405--23.

\leavevmode\vadjust pre{\hypertarget{ref-LossModels}{}}%
Klugman, Stuart A., Harry H. Panjer, and Gordon E. Willmot. 2008.
\emph{Loss Models: From Data to Decisions}. Third. Hoboken, N.J.: John
Wiley \& Sons, Inc.

\leavevmode\vadjust pre{\hypertarget{ref-Geocomputation}{}}%
Lovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019.
\emph{Geocomputation with {R}}. 1st ed. The {R} Series. Boca Raton:
Chapman \& Hall/CRC. \url{https://geocompr.robinlovelace.net/}.

\leavevmode\vadjust pre{\hypertarget{ref-McElreath}{}}%
McElreath, Richard. 2020. \emph{Statistical Rethinking: A Bayesian
Course with Examples in {R} and Stan}. 2nd ed. Abingdon, Oxfordshire:
CRC Press. \url{https://github.com/rmcelreath/rethinking}.

\leavevmode\vadjust pre{\hypertarget{ref-Pappas}{}}%
Pappas, T., A. J. Laub, and N. R. Sandell. 1980. {``On the Numerical
Solution of the Discrete-Time Algebraic Riccati Equation.''} \emph{IEEE
Transaction on Automatic Control} AC-25.4: 631--41.

\leavevmode\vadjust pre{\hypertarget{ref-Primer}{}}%
Pearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016.
\emph{Causal Inference in Statistics: A Primer}. Chichester: John Wiley
\& Sons. \url{http://bayes.cs.ucla.edu/PRIMER/}.

\leavevmode\vadjust pre{\hypertarget{ref-Silge}{}}%
Silge, Julia, and David Robinson. 2017. \emph{Text Mining with {R}: A
{T}idy Approach}. O'Reilly. \url{https://www.tidytextmining.com/}.

\leavevmode\vadjust pre{\hypertarget{ref-Taddy}{}}%
Taddy, Matt. 2019. \emph{Business Data Science: Combining Machine
Learning and Economics to Optimize, Automate, and Accelerate Business
Decisions}. New York, NY: McGraw-Hill Education.
\url{https://github.com/TaddyLab/BDS}.

\leavevmode\vadjust pre{\hypertarget{ref-Theil}{}}%
Theil, Henri, and Arthur S. Goldberger. 1961. {``On Pure and Mixed
Statistical Estimation in Economics.''} \emph{International Economic
Review} 2: 317--32.

\leavevmode\vadjust pre{\hypertarget{ref-BISSM}{}}%
Triantafyllopoulos, Kostas. 2021. \emph{Bayesian Inference of State
Space Models: Kalman Filtering and Beyond}. Springer Texts in
Statistics. Cham, Switzerland: Springer.

\leavevmode\vadjust pre{\hypertarget{ref-Vaughan}{}}%
Vaughan, D. R. 1970. {``A Non Recursive Algorithm Solution for the
Discrete Riccati Equation.''} \emph{IEEE Transactions on Automatic
Control} AC-15.5: 597--99.

\leavevmode\vadjust pre{\hypertarget{ref-ggridges}{}}%
Wilke, Claus O. 2020. \emph{{ggridges}: Ridgeline Plots in {'ggplot2'}}.
\url{https://CRAN.R-project.org/package=ggridges}.

\leavevmode\vadjust pre{\hypertarget{ref-Leland}{}}%
Wilkinson, Leland. 2013. \emph{The Grammar of Graphics}. 2nd ed. New
York, NY: Springer-Verlag.

\leavevmode\vadjust pre{\hypertarget{ref-Approval}{}}%
Yong, Laurel Harbridge, Jon A. Krosnick, and Jeffrey M. Wooldridge.
2016. {``Presidential Approval and Gas Prices: Sociotropic or Pocketbook
Influence?''} In \emph{Political Psychology}, edited by Jon A. Krosnick,
I-Chant A. Chiang, and Tobias H. Stark, 246--75. Taylor; Francis Inc.

\end{CSLReferences}

\cleardoublepage
\phantomsection
\addcontentsline{toc}{part}{Appendices}
\appendix

\hypertarget{basic-ggplot2}{%
\chapter{\texorpdfstring{Basic
\texttt{ggplot2}}{Basic ggplot2}}\label{basic-ggplot2}}

\hypertarget{plotting-in-the-tidyverse}{%
\section{\texorpdfstring{Plotting in the
\texttt{tidyverse}}{Plotting in the tidyverse}}\label{plotting-in-the-tidyverse}}

\texttt{ggplot2} forms a key part of the
\href{https://www.tidyverse.org/}{\texttt{tidyverse}}\index{tidyverse}
-- for many \href{https://en.wikipedia.org/wiki/ggplot2}{the only part}.
It builds on the \emph{\textbf{g}rammar of \textbf{g}raphics} proposed
by the late Leland Wilkinson, Wilkinson (2013). In essence it provides
rules for how graphics should be treated, simple rules that drive you
mad until you get it.

The process for building a graph is something like the following.

\begin{itemize}
\tightlist
\item
  Initiate a plot using \texttt{ggplot}.
\item
  Specify \textbf{aesthetics} which indicate \emph{what} you want to
  plot from some data set.
\item
  Call a \texttt{geom} (or an alternative) to say \emph{how} you want to
  plot it.
\item
  Add \textbf{modifiers} to change how it \emph{looks}.
\end{itemize}

The order of operations is essentially always this, although quite how
the ordering is apllied differs subtly, which we will show here.

\hypertarget{example-3}{%
\section{Example}\label{example-3}}

To illustrate, we take the \texttt{wooldridge} data set
\texttt{approval} from Yong, Krosnick, and Wooldridge (2016), do a
little wrangling and (eventually) produce some quite nice plots. Start
with the libraries and retrieve data the data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(wooldridge)}
\FunctionTok{data}\NormalTok{(}\StringTok{"approval"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The first few columns and rows of this looks like:

\begin{verbatim}
   id month year   sp500   cpi cpifood approve
1 302     2 2001 1239.94 184.4   171.8   59.24
2 303     3 2001 1160.33 185.3   172.2   57.01
3 304     4 2001 1249.46 185.6   172.4   60.31
4 305     5 2001 1255.82 185.5   172.9   55.82
5 306     6 2001 1224.42 185.9   173.4   54.93
6 307     7 2001 1211.23 186.2   174.0   56.36
\end{verbatim}

and all the available variables are

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(approval)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "id"         "month"      "year"       "sp500"      "cpi"       
 [6] "cpifood"    "approve"    "gasprice"   "unemploy"   "katrina"   
[11] "rgasprice"  "lrgasprice" "X11.Sep"    "iraqinvade" "lsp500"    
[16] "lcpifood"  
\end{verbatim}

Typically we want to investigate trends and correlations and graphing
pairs or more of series is a good way to begin.

\hypertarget{scatter-plot}{%
\subsection{Scatter plot}\label{scatter-plot}}

A first scatter plot, using \texttt{geom\_point} of food against gas
(petrol) prices

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(approval, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{lcpifood, }\AttributeTok{y=}\NormalTok{lrgasprice)) }\SpecialCharTok{+}    \CommentTok{\# Initiate, set aesthetics}
  \FunctionTok{geom\_point}\NormalTok{()                                       }\CommentTok{\# Display as points}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Appendix1_files/figure-pdf/p1-1.pdf}

}

\end{figure}

OK, I guess, but a bit dull -- so add some colour. This time,
\texttt{aes} is specified in the \texttt{geom} -- either is fine, but
there are some advantages either way which we will see shortly.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(approval) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{lcpifood, }\AttributeTok{y=}\NormalTok{lrgasprice, }\AttributeTok{color=}\NormalTok{month))  }\CommentTok{\# Colours by month}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Appendix1_files/figure-pdf/p2-1.pdf}

}

\end{figure}

Better, but how about\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(approval) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{lcpifood, }\AttributeTok{y=}\NormalTok{lrgasprice, }\AttributeTok{color=}\NormalTok{approve), }\AttributeTok{size=}\DecValTok{2}\NormalTok{, }\AttributeTok{shape=}\DecValTok{17}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# Colours by popularity!}
  \FunctionTok{scale\_color\_gradient}\NormalTok{(}\AttributeTok{low=}\StringTok{"red"}\NormalTok{, }\AttributeTok{high=}\StringTok{"green"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Appendix1_files/figure-pdf/p2a-1.pdf}

}

\end{figure}

where the colours are a gradient we specify. But months can only be one
of twelve categories, so a categorical variable (a \emph{factor}) is
needed to get different actual colours, otherwise for a continuous
variable I get shades of one colour or a continuous change we need to
specify.

Lets do this -- and add a different aesthetic, size, for year.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(approval) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{lcpifood, }\AttributeTok{y=}\NormalTok{lrgasprice, }\AttributeTok{color=}\FunctionTok{as.factor}\NormalTok{(month), }\AttributeTok{size=}\FunctionTok{as.factor}\NormalTok{(year)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: Using size for a discrete variable is not advised.
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{Appendix1_files/figure-pdf/p3-1.pdf}

}

\end{figure}

Note there is now a lot going n, and maybe too much. \texttt{ggplot}
thinks so!

\hypertarget{time-series-plots}{%
\subsection{Time series plots}\label{time-series-plots}}

Our time index is a bit odd as the data set has year and month
separately. Create a proper date series using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{approval }\SpecialCharTok{\%\textless{}\textgreater{}\%} 
  \FunctionTok{unite}\NormalTok{(date, year, month, }\AttributeTok{sep=}\StringTok{"/"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{date =} \FunctionTok{as.Date}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(date,}\StringTok{"/01"}\NormalTok{), }\StringTok{"\%Y/\%m/\%d"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

I've used the \texttt{\%\textless{}\textgreater{}\%} pipe operator to
send and get back \texttt{approval} so this is now

\begin{verbatim}
   id       date   sp500   cpi cpifood approve gasprice unemploy katrina
1 302 2001-02-01 1239.94 184.4   171.8   59.24    148.4      4.6       0
2 303 2001-03-01 1160.33 185.3   172.2   57.01    144.7      4.5       0
3 304 2001-04-01 1249.46 185.6   172.4   60.31    156.4      4.2       0
4 305 2001-05-01 1255.82 185.5   172.9   55.82    172.9      4.1       0
5 306 2001-06-01 1224.42 185.9   173.4   54.93    164.0      4.7       0
6 307 2001-07-01 1211.23 186.2   174.0   56.36    148.2      4.7       0
  rgasprice lrgasprice X11.Sep iraqinvade   lsp500 lcpifood
1  80.47723   4.387974       0          0 7.122818 5.146331
2  78.08958   4.357857       0          0 7.056460 5.148656
3  84.26724   4.433993       0          0 7.130467 5.149817
4  93.20755   4.534829       0          0 7.135544 5.152713
5  88.21947   4.479828       0          0 7.110222 5.155601
6  79.59184   4.376912       0          0 7.099391 5.159055
\end{verbatim}

Then I can plot a couple of series using two calls to
\texttt{geom\_line}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(approval) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{date, }\AttributeTok{y=}\NormalTok{unemploy), }\AttributeTok{colour=}\StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{date, }\AttributeTok{y=}\NormalTok{cpi), }\AttributeTok{colour=}\StringTok{"blue"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Appendix1_files/figure-pdf/unnamed-chunk-5-1.pdf}

}

\end{figure}

But this is pretty inefficient, as I would need a call to
\texttt{geom\_line} for every series I wanted to plot and even then
scales are unsuitable. Plus the labels are not right.

This is where things really get interesting. I \texttt{pivot\_longer}
all the variables into a single column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{pivot\_longer}\NormalTok{(approval, }\AttributeTok{cols=}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(date, id), }\AttributeTok{names\_to=} \StringTok{"Var"}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{"Val"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 4
     id date       Var         Val
  <int> <date>     <chr>     <dbl>
1   302 2001-02-01 sp500    1240. 
2   302 2001-02-01 cpi       184. 
3   302 2001-02-01 cpifood   172. 
4   302 2001-02-01 approve    59.2
5   302 2001-02-01 gasprice  148. 
6   302 2001-02-01 unemploy    4.6
\end{verbatim}

Great! Now I can plot \texttt{Val} using one call to
\texttt{geom\_line}. This time, put the graph object into \texttt{p} and
then explicitly plot it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p  }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{date, }\AttributeTok{y=}\NormalTok{Val))}
\FunctionTok{plot}\NormalTok{(p)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Appendix1_files/figure-pdf/unnamed-chunk-7-1.pdf}

}

\end{figure}

Oops! I need to tell \texttt{ggplot2} to separate out the variables
which are stored in \texttt{Var}. For this, use \texttt{group}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p  }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{date, }\AttributeTok{y=}\NormalTok{Val, }\AttributeTok{group=}\NormalTok{Var))}
\FunctionTok{plot}\NormalTok{(p)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Appendix1_files/figure-pdf/unnamed-chunk-8-1.pdf}

}

\end{figure}

But this could better be done by using an aesthetic like colour which
implies group

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p  }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(df) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{date, }\AttributeTok{y=}\NormalTok{Val, }\AttributeTok{colour=}\NormalTok{Var))}
\FunctionTok{plot}\NormalTok{(p)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Appendix1_files/figure-pdf/unnamed-chunk-9-1.pdf}

}

\end{figure}

OK, but can I plot them so we can see what's going on, like in a grid?
This is where \texttt{facet} comes in.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p  }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Var, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(p)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Appendix1_files/figure-pdf/unnamed-chunk-10-1.pdf}

}

\end{figure}

A bit more formatting\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p  }\OtherTok{\textless{}{-}}\NormalTok{ p }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Facet plots"}\NormalTok{, }\AttributeTok{x=}\StringTok{""}\NormalTok{, }\AttributeTok{y=}\StringTok{""}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(p)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Appendix1_files/figure-pdf/unnamed-chunk-11-1.pdf}

}

\end{figure}

Finally all in one go, dropping the dummies, don't store as an object.
Also no legend, as series labelled in the facets. And I call a rather
handy little function \texttt{geom\_smooth} which fits (by default) a
Loess smoothing line.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{approval }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{iraqinvade, }\SpecialCharTok{{-}}\NormalTok{katrina, }\SpecialCharTok{{-}}\NormalTok{X11.Sep) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols=}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(date, id), }\AttributeTok{names\_to=}\StringTok{"Var"}\NormalTok{, }\AttributeTok{values\_to=}\StringTok{"Val"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{date, }\AttributeTok{y=}\NormalTok{Val, }\AttributeTok{group=}\NormalTok{Var, }\AttributeTok{colour=}\NormalTok{Var)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# Smoother}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Var, }\AttributeTok{scales =} \StringTok{"free"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Facet plots"}\NormalTok{, }\AttributeTok{x=}\StringTok{""}\NormalTok{, }\AttributeTok{y=}\StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Appendix1_files/figure-pdf/unnamed-chunk-12-1.pdf}

}

\end{figure}

Cool, huh?

\hypertarget{linear-algebra}{%
\chapter{Linear algebra}\label{linear-algebra}}

\hypertarget{modelling-in-economics-is-linear-algebra}{%
\section{\texorpdfstring{Modelling in economics \textbf{is} linear
algebra}{Modelling in economics is linear algebra}}\label{modelling-in-economics-is-linear-algebra}}

That's a bit extreme, but you mostly need to do linear algebra to
program up many of the estimators we need, or to solve a rational
expectations models.

\hypertarget{beginning-to-program}{%
\section{Beginning to program}\label{beginning-to-program}}

A few non-linear algebra things we will need are summarized in the
following table.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2698}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2222}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1746}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3333}}@{}}
\caption{Useful things}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
R
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Notes
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
R
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Notes
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Assign a value & \texttt{\textless{}-} & \texttt{a\ \textless{}-\ 4} &
Also legal is \texttt{a\ =\ 4}. But I hate it. \\
Create a list of values & \texttt{c(.)} &
\texttt{v\ \textless{}-\ c(1,\ -2,\ 22)} & Defining `on the fly' \\
Sequence & \texttt{seq(i,\ k,\ l)} & \(5\), \(7\), \ldots{} ,\(21\) &
Create a sequence \\
& \texttt{i:k} & \(i\), \(i\pm 1\), \ldots{} ,\(k\) & Short cut for unit
in/de-crements \\
Loop commands & \texttt{for\ (var\ in\ seq)\ expr} &
\texttt{for\ (i\ in\ 5:1)\ print(i)} & Loops. We need loops. \\
Draw a random number & \texttt{rnorm(k,a,b)} &
\texttt{rnorm(60,\ 0,\ 5)} & Example draws 60 values \textasciitilde{}
\(N(0,5)\) \\
Create a matrix & \texttt{matrix(v,i,j)} & \texttt{matrix(5,\ 2,\ 2)} &
Create a \(2\times 2\) matrix of 5s \\
\end{longtable}

\hypertarget{functions}{%
\subsection{Functions}\label{functions}}

Everything in R is a function (although it doesn't look like it).
Defining a function is simple:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{name\_of\_function }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(function\_arguments)\{}
  \CommentTok{\# Body of function where stuff is done  }
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Here's one that actually does something:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{addaddadd }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x,y)\{}
\NormalTok{  z }\OtherTok{\textless{}{-}} \DecValTok{3}\SpecialCharTok{*}\NormalTok{(x}\SpecialCharTok{+}\NormalTok{y)}
  \FunctionTok{return}\NormalTok{(z)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

and if we run it:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{addaddadd}\NormalTok{(}\DecValTok{4}\NormalTok{,}\DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 30
\end{verbatim}

\hypertarget{linear-algebra-1}{%
\section{Linear algebra}\label{linear-algebra-1}}

Assume the following: \(A\) and \(B\) are real matrices of dimension
\(n\times n\), \(b\) and \(c\) are real \(n-\)vectors, \(X\) is a real
\(T\times k\) matrix, and \(S\) is a symmetric real matrix.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2698}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2222}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1746}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3333}}@{}}
\caption{Maths commands essential to linear algebra}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Maths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
R
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Notes
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Maths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
R
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Notes
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Hadamard product & \(A\bigodot B\) & \texttt{A\ *\ B} &
Element-by-element, \(A\), \(B\) same size \\
Matrix/vector product & \(A\times B\), \(A \times b\) &
\texttt{A\ \%*\%\ B}, \texttt{A\ \%*\%\ b} & Normal product rule \\
Inner product & \(X'X\) & \texttt{t(X)\ \%*\%\ X} & Also uses transpose
operator, \texttt{t()} \\
& & \texttt{crossprod(X)} & More efficient, but less mathy \\
& \(A'B\) & \texttt{t(A)\ \%*\%\ B} & \\
& & \texttt{crossprod(A,B)} & \\
Outer product & \(A\times B'\) & \texttt{tcrossprod(A,B)} & \\
Inverse & \(A^{-1}\) & \texttt{solve(A)} & Matrix inverse is a special
case of\ldots{} \\
Solve for \(d\) & \(Ad = b \Rightarrow d = A^{-1}b\) &
\texttt{d\ \textless{}-\ solve(A,\ b)} & \ldots linear solution! \\
Cholesky decomp & \(S = R'R\) & \texttt{R\ \textless{}-\ chol(S)} &
\(S\) is a symmetric, positive definite matrix \\
Cholesky inverse & \(S^{-1}\) & \texttt{chol2inv(R)} & Fast! \\
Determinant & \(\vert A \vert\) & \texttt{det(A)} & \\
Diagonal & & & \\
\(\quad\) of a matrix & & \texttt{diag(A)} & Retrieve the elements
\(a_{ii}\), \(i=1,..,n\) \\
\(\quad\) in a matrix & & \texttt{A\ \textless{}-\ diag(b)} & Set the
diagonal of \(A\) to \(b\), zero elsewhere \\
\(\quad\) Identity matrix & \(I_n\) & \texttt{diag(n)} & \\
Eigenvalues/vectors & & \texttt{E\ \textless{}-\ eigen(A)} & Returns a
\emph{list}: \texttt{E\$values}, \texttt{E\$vectors} \\
\end{longtable}

\hypertarget{starting-to-do-linear-algebra}{%
\section{Starting to do linear
algebra}\label{starting-to-do-linear-algebra}}

\hypertarget{problem-1}{%
\subsection{Problem}\label{problem-1}}

Consider the following simultaneous system of equations:

\begin{align*}
x_1 + 2x_2 &= 6 \\
x_1 - 3x_2 +2 x_3 &= 0 \\
-2 x_1 + 3 x_3 &= 2
\end{align*}

Find the values of \(x\) that solve this using R.

\textbf{Hint} -- write the problem in matrix form

\begin{equation*}
 Ax = b
\end{equation*}

where

\begin{equation*}
A = \left [ \begin{array}{rrr}
             1 &  2 & 0 \\
             1 & -3 & 2 \\
            -2 &  0 & 3
            \end{array} \right ], \qquad 
  b = \left[ \begin{matrix}6 \\ 0 \\ 2\end{matrix} \right ]
\end{equation*}

and then use \texttt{solve}.

\hypertarget{solution-1}{%
\subsection{Solution}\label{solution-1}}

R code to create these matrices is:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Matrices are populated by column by default}
\NormalTok{A }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{),}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{)}
\NormalTok{b }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{),}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The solution is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(A,b)}
\end{Highlighting}
\end{Shaded}

where \texttt{x} is:

\begin{verbatim}
     [,1]
[1,]    2
[2,]    2
[3,]    2
\end{verbatim}

\hypertarget{eigenvalue-decomposition}{%
\section{Eigenvalue decomposition}\label{eigenvalue-decomposition}}

Any square matrix can be decomposed into a non-singular matrix \(V\) of
eigenvectors and a diagonal matrix of eigenvalues \(\Lambda\) such that:
\begin{equation}
  A V = V \Lambda \Rightarrow A = V\Lambda V^{-1}
\end{equation} Call \texttt{eigen} to decompose our previously defined
matrix \(A\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{e }\OtherTok{\textless{}{-}} \FunctionTok{eigen}\NormalTok{(A)}
\NormalTok{L }\OtherTok{\textless{}{-}}\NormalTok{ e}\SpecialCharTok{$}\NormalTok{values    }\CommentTok{\# Returns a list, assign vectors/values}
\NormalTok{V }\OtherTok{\textless{}{-}}\NormalTok{ e}\SpecialCharTok{$}\NormalTok{vectors}
\end{Highlighting}
\end{Shaded}

Note \(A\) is \textbf{not} symmetric so it may have complex roots, which
is does

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{L}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -3.682744+0.000000i  2.341372+0.873683i  2.341372-0.873683i
\end{verbatim}

If we calculate \(A = V\Lambda V^{-1}\) we get

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s }\OtherTok{\textless{}{-}}\NormalTok{ V }\SpecialCharTok{\%*\%} \FunctionTok{diag}\NormalTok{(L) }\SpecialCharTok{\%*\%} \FunctionTok{solve}\NormalTok{(V)}
\NormalTok{s}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      [,1]             [,2]             [,3]
[1,]  1+0i  2.000000e+00+0i -2.220446e-16+0i
[2,]  1+0i -3.000000e+00+0i  2.000000e+00+0i
[3,] -2+0i -1.054712e-15+0i  3.000000e+00+0i
\end{verbatim}

and when we drop the zero imaginary parts

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{Re}\NormalTok{(s)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     [,1]          [,2]          [,3]
[1,]    1  2.000000e+00 -2.220446e-16
[2,]    1 -3.000000e+00  2.000000e+00
[3,]   -2 -1.054712e-15  3.000000e+00
\end{verbatim}

Round to eliminate numerical error, to get

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{Re}\NormalTok{(s), }\AttributeTok{digits=}\DecValTok{12}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     [,1] [,2] [,3]
[1,]    1    2    0
[2,]    1   -3    2
[3,]   -2    0    3
\end{verbatim}

\hypertarget{precision}{%
\section{Precision}\label{precision}}

Why do we round? Take a real matrix \(A_{mn}\) with \(n \le m\) and
pre-multiply by its own transpose, i.e.~\(S = A'A\). \(AA\) is
symmetric, positive semi-definite. If \(rank(A) = n\), then
\(rank(S) = n\) and positive definite, and its inverse exists.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{A  }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, .}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{S }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(A) }\SpecialCharTok{\%*\%}\NormalTok{ A}
\NormalTok{S}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     [,1] [,2]
[1,] 1.04  0.2
[2,] 0.20  1.0
\end{verbatim}

Let's invert \(S\) three different ways.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{i1 }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(S) }
\NormalTok{i2 }\OtherTok{\textless{}{-}} \FunctionTok{chol2inv}\NormalTok{(}\FunctionTok{chol}\NormalTok{(S))}
\NormalTok{i3 }\OtherTok{\textless{}{-}} \FunctionTok{qr.solve}\NormalTok{(S) }
\end{Highlighting}
\end{Shaded}

Pre-multiplying \(S\) by its inverse gives

\begin{equation}
   I_k = \left[\begin{matrix}1 &0 \\0 &1 \\\end{matrix}\right]
\end{equation}

Looking at the results of doing this for each method gives

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{i1 }\SpecialCharTok{\%*\%}\NormalTok{ S}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             [,1] [,2]
[1,] 1.000000e+00    0
[2,] 2.775558e-17    1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{i2 }\SpecialCharTok{\%*\%}\NormalTok{ S}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     [,1]         [,2]
[1,]    1 5.551115e-17
[2,]    0 1.000000e+00
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{i3 }\SpecialCharTok{\%*\%}\NormalTok{ S}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     [,1]          [,2]
[1,]    1 -2.775558e-17
[2,]    0  1.000000e+00
\end{verbatim}

which are all slightly different but by tiny -- and insignificant --
amounts. Don't be fooled by this, they are all numerically the same.

\hypertarget{programming-the-regression-problem}{%
\section{Programming the regression
problem}\label{programming-the-regression-problem}}

Let's look at the familiar regression problem for some generated data.
\begin{equation}
  y = XB + \epsilon
\end{equation} where \(\epsilon \sim N(0,.2)\), \(X\) is a
\((k+1)\times n\) matrix of regressors including a constant and \(B\) a
\(k+1\) vector of coefficients. Let's generate some random data of an
arbitrary sized problem:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{180}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), }\DecValTok{60}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\FunctionTok{head}\NormalTok{(X, }\DecValTok{6}\NormalTok{) }\CommentTok{\# Print first six rows}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         [,1]        [,2]     [,3]
[1,] 1.906710  4.86573396 2.390330
[2,] 3.268898  4.05527385 1.705367
[3,] 1.622163  2.10549632 2.176462
[4,] 1.304953  0.40456346 3.078402
[5,] 3.106988  2.54012774 1.180277
[6,] 4.346681 -0.09252894 2.508839
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\DecValTok{1}\NormalTok{, X) }\CommentTok{\# Add a constant}
\FunctionTok{tail}\NormalTok{(X,}\DecValTok{6}\NormalTok{) }\CommentTok{\# Print last six rows}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      [,1]       [,2]      [,3]      [,4]
[55,]    1 -1.0234818 1.3021403 1.4330546
[56,]    1  1.2814302 0.7535604 1.0542110
[57,]    1  2.3069226 1.4478565 0.3346868
[58,]    1 -0.2652897 3.0628273 2.0804648
[59,]    1  2.1082132 1.5461640 1.8327673
[60,]    1  2.4949328 4.4734549 2.5290737
\end{verbatim}

Now create a dependent variable that is a linear combination of these
variables plus some noise. Create the linear relationship first so we
know what it is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{B }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{,}\DecValTok{1}\NormalTok{,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,.}\DecValTok{2}\NormalTok{), }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

and then the dependent variable:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ X }\SpecialCharTok{\%*\%}\NormalTok{ B }\SpecialCharTok{+} \FloatTok{0.2}\SpecialCharTok{*}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{60}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We could now do a regression -- i.e.~calculate

\begin{equation}
  \hat B = (X'X)^{-1}X'y
\end{equation}

which can be written:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Bhat }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(X)}\SpecialCharTok{\%*\%}\NormalTok{X)}\SpecialCharTok{\%*\%}\FunctionTok{t}\NormalTok{(X)}\SpecialCharTok{\%*\%}\NormalTok{y}
\end{Highlighting}
\end{Shaded}

which gives

\begin{verbatim}
           [,1]
[1,]  0.5013554
[2,]  1.0131585
[3,] -1.0033866
[4,]  0.1707199
\end{verbatim}

But I wouldn't do it like this (we'll see why in a minute). A better way
would be

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Bhat2 }\OtherTok{\textless{}{-}} \FunctionTok{chol2inv}\NormalTok{(}\FunctionTok{chol}\NormalTok{(}\FunctionTok{crossprod}\NormalTok{(X))) }\SpecialCharTok{\%*\%} \FunctionTok{crossprod}\NormalTok{(X,y)}
\end{Highlighting}
\end{Shaded}

or even

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Bhat3 }\OtherTok{\textless{}{-}} \FunctionTok{qr.solve}\NormalTok{(X,y)}
\end{Highlighting}
\end{Shaded}

which both evaluate to the same \(\hat B\) values. Each is better in
different circumstances.

\hypertarget{test-timings}{%
\subsection{Test timings}\label{test-timings}}

Why does it matter how you do things? It should be obvious that it
might, but it turns out some fairly trivial things can make a lot of
difference. We set some parameters so we can create a bigger problem.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{400}
\NormalTok{k }\OtherTok{\textless{}{-}} \DecValTok{12}
\end{Highlighting}
\end{Shaded}

We will use seven different methods to calculate an estimate of \(B\).
These are two variations on the three calculations below (where the
brackets matter!): \begin{align}
\hat B_1 &= ((X'X)^{-1}) X'y \\
\hat B_2 &= ((X'X)^{-1}) (X'y) \\
\hat B_3 &= ((X'X)^{-1} (X'y))
\end{align} where we do each of these either `by hand' or using
\texttt{crossprod}, with a final solution using \texttt{qr.solve}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tictoc)}

\NormalTok{reps }\OtherTok{\textless{}{-}} \DecValTok{10000}
\NormalTok{t    }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{()}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\FunctionTok{tic}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{reps) \{ }
\NormalTok{  B }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{runif}\NormalTok{(k}\SpecialCharTok{+}\DecValTok{1}\NormalTok{), k}\SpecialCharTok{+}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  X }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(1L, }\FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{k, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), n, k))}
\NormalTok{  y }\OtherTok{\textless{}{-}}\NormalTok{ X }\SpecialCharTok{\%*\%}\NormalTok{ B }\SpecialCharTok{+} \FloatTok{0.2} \SpecialCharTok{*} \FunctionTok{rnorm}\NormalTok{(n) }
\NormalTok{  Bhat }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ X) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ y }
\NormalTok{  \}}
\NormalTok{t[[}\DecValTok{1}\NormalTok{]] }\OtherTok{\textless{}{-}} \FunctionTok{toc}\NormalTok{(}\AttributeTok{quiet =} \ConstantTok{TRUE}\NormalTok{)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\FunctionTok{tic}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{reps) \{ }
\NormalTok{  B }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{runif}\NormalTok{(k}\SpecialCharTok{+}\DecValTok{1}\NormalTok{), k}\SpecialCharTok{+}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  X }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(1L, }\FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{k, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), n, k))}
\NormalTok{  y }\OtherTok{\textless{}{-}}\NormalTok{ X }\SpecialCharTok{\%*\%}\NormalTok{ B }\SpecialCharTok{+} \FloatTok{0.2} \SpecialCharTok{*} \FunctionTok{rnorm}\NormalTok{(n) }
\NormalTok{  Bhata }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(}\FunctionTok{t}\NormalTok{(X)}\SpecialCharTok{\%*\%}\NormalTok{X) }\SpecialCharTok{\%*\%}\NormalTok{ (}\FunctionTok{t}\NormalTok{(X)}\SpecialCharTok{\%*\%}\NormalTok{y) }
\NormalTok{\}}
\NormalTok{t[[}\DecValTok{2}\NormalTok{]] }\OtherTok{\textless{}{-}} \FunctionTok{toc}\NormalTok{(}\AttributeTok{quiet =} \ConstantTok{TRUE}\NormalTok{)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\FunctionTok{tic}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{reps) \{ }
\NormalTok{  B }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{runif}\NormalTok{(k}\SpecialCharTok{+}\DecValTok{1}\NormalTok{), k}\SpecialCharTok{+}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  X }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(1L, }\FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{k, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), n, k))}
\NormalTok{  y }\OtherTok{\textless{}{-}}\NormalTok{ X }\SpecialCharTok{\%*\%}\NormalTok{ B }\SpecialCharTok{+} \FloatTok{0.2} \SpecialCharTok{*} \FunctionTok{rnorm}\NormalTok{(n) }
\NormalTok{  Bhatb }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{((}\FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ X), (}\FunctionTok{t}\NormalTok{(X) }\SpecialCharTok{\%*\%}\NormalTok{ y)) }
\NormalTok{  \}}
\NormalTok{t[[}\DecValTok{3}\NormalTok{]] }\OtherTok{\textless{}{-}} \FunctionTok{toc}\NormalTok{(}\AttributeTok{quiet =} \ConstantTok{TRUE}\NormalTok{)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\FunctionTok{tic}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{reps) \{ }
\NormalTok{  B }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{runif}\NormalTok{(k}\SpecialCharTok{+}\DecValTok{1}\NormalTok{), k}\SpecialCharTok{+}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  X }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(1L, }\FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{k, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), n, k))}
\NormalTok{  y }\OtherTok{\textless{}{-}}\NormalTok{ X }\SpecialCharTok{\%*\%}\NormalTok{ B }\SpecialCharTok{+} \FloatTok{0.2} \SpecialCharTok{*} \FunctionTok{rnorm}\NormalTok{(n) }
\NormalTok{  Bhat2 }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(}\FunctionTok{crossprod}\NormalTok{(X)) }\SpecialCharTok{\%*\%} \FunctionTok{crossprod}\NormalTok{(X,y) }
\NormalTok{  \}}
\NormalTok{t[[}\DecValTok{4}\NormalTok{]] }\OtherTok{\textless{}{-}} \FunctionTok{toc}\NormalTok{(}\AttributeTok{quiet =} \ConstantTok{TRUE}\NormalTok{)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\FunctionTok{tic}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{reps) \{ }
\NormalTok{  B }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{runif}\NormalTok{(k}\SpecialCharTok{+}\DecValTok{1}\NormalTok{), k}\SpecialCharTok{+}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  X }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(1L, }\FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{k, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), n, k))}
\NormalTok{  y }\OtherTok{\textless{}{-}}\NormalTok{ X }\SpecialCharTok{\%*\%}\NormalTok{ B }\SpecialCharTok{+} \FloatTok{0.2} \SpecialCharTok{*} \FunctionTok{rnorm}\NormalTok{(n) }
\NormalTok{  Bhat2a }\OtherTok{\textless{}{-}} \FunctionTok{solve}\NormalTok{(}\FunctionTok{crossprod}\NormalTok{(X), }\FunctionTok{crossprod}\NormalTok{(X,y)) }
\NormalTok{  \}}
\NormalTok{t[[}\DecValTok{5}\NormalTok{]] }\OtherTok{\textless{}{-}} \FunctionTok{toc}\NormalTok{(}\AttributeTok{quiet =} \ConstantTok{TRUE}\NormalTok{)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\FunctionTok{tic}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{reps) \{ }
\NormalTok{  B }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{runif}\NormalTok{(k}\SpecialCharTok{+}\DecValTok{1}\NormalTok{), k}\SpecialCharTok{+}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  X }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(1L, }\FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{k, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), n, k))}
\NormalTok{  y }\OtherTok{\textless{}{-}}\NormalTok{ X }\SpecialCharTok{\%*\%}\NormalTok{ B }\SpecialCharTok{+} \FloatTok{0.2} \SpecialCharTok{*} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{  Bhat2b }\OtherTok{\textless{}{-}} \FunctionTok{chol2inv}\NormalTok{(}\FunctionTok{chol}\NormalTok{(}\FunctionTok{crossprod}\NormalTok{(X))) }\SpecialCharTok{\%*\%} \FunctionTok{crossprod}\NormalTok{(X,y) }
\NormalTok{  \}}
\NormalTok{t[[}\DecValTok{6}\NormalTok{]] }\OtherTok{\textless{}{-}} \FunctionTok{toc}\NormalTok{(}\AttributeTok{quiet =} \ConstantTok{TRUE}\NormalTok{)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\FunctionTok{tic}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{reps) \{ }
\NormalTok{  B }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{runif}\NormalTok{(k}\SpecialCharTok{+}\DecValTok{1}\NormalTok{), k}\SpecialCharTok{+}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  X }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(1L, }\FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n}\SpecialCharTok{*}\NormalTok{k, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), n, k))}
\NormalTok{  y }\OtherTok{\textless{}{-}}\NormalTok{ X }\SpecialCharTok{\%*\%}\NormalTok{ B }\SpecialCharTok{+} \FloatTok{0.2} \SpecialCharTok{*} \FunctionTok{rnorm}\NormalTok{(n) }
\NormalTok{  Bhat3 }\OtherTok{\textless{}{-}} \FunctionTok{qr.solve}\NormalTok{(X,y) }
\NormalTok{  \}}
\NormalTok{t[[}\DecValTok{7}\NormalTok{]] }\OtherTok{\textless{}{-}} \FunctionTok{toc}\NormalTok{(}\AttributeTok{quiet =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

How do we display the timings we saved in \texttt{t}? We could do a
simple (but dull) table, or something a bit nicer.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{FF }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"solve(t(X)\%*\%X) \%*\% t(X) \%*\% y"}\NormalTok{,}
        \StringTok{"solve(t(X)\%*\%X) \%*\% (t(X)\%*\%y)"}\NormalTok{,}
        \StringTok{"solve(t(X)\%*\%X, (t(X)\%*\%y))"}\NormalTok{,}
        \StringTok{"solve(crossprod(X)) \%*\% crossprod(X,y)"}\NormalTok{,}
        \StringTok{"solve(crossprod(X), crossprod(X,y))"}\NormalTok{,}
        \StringTok{"chol2inv(chol(crossprod(X))) \%*\% crossprod(X,y)"}\NormalTok{,}
        \StringTok{"qr.solve(X,y)"}\NormalTok{)}

\NormalTok{tms }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{gsub}\NormalTok{(}\StringTok{" sec elapsed"}\NormalTok{, }\StringTok{""}\NormalTok{, }\FunctionTok{unlist}\NormalTok{(t)[}\FunctionTok{seq}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{21}\NormalTok{,}\DecValTok{3}\NormalTok{)]))}
\NormalTok{v   }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{Method  =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{, }
              \AttributeTok{Times   =}\NormalTok{ tms,}
              \AttributeTok{Formula =}\NormalTok{ FF) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Col =} \FunctionTok{case\_when}\NormalTok{(Times }\SpecialCharTok{==} \FunctionTok{min}\NormalTok{(Times) }\SpecialCharTok{\textasciitilde{}} \StringTok{"red"}\NormalTok{,}
                         \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{"blue"}
\NormalTok{  ))}

\FunctionTok{ggplot}\NormalTok{(v) }\SpecialCharTok{+} 
  \FunctionTok{geom\_col}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Method, }\AttributeTok{y=}\NormalTok{Times, }\AttributeTok{fill=}\FunctionTok{as.factor}\NormalTok{(Method)), }\AttributeTok{alpha=}\NormalTok{.}\DecValTok{6}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Method, }\AttributeTok{y=}\NormalTok{.}\DecValTok{1}\NormalTok{, }\AttributeTok{label=}\NormalTok{Formula, }\AttributeTok{color=}\NormalTok{Col), }\AttributeTok{size=}\FloatTok{5.5}\NormalTok{, }\AttributeTok{hjust=}\DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{scale\_color\_identity}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{axis.text.y =} \FunctionTok{element\_blank}\NormalTok{()) }\SpecialCharTok{+} 
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y=}\FunctionTok{paste}\NormalTok{(}\StringTok{"Seconds taken to do"}\NormalTok{,reps,}\StringTok{"replications"}\NormalTok{), }\AttributeTok{x=}\StringTok{""}\NormalTok{, }
       \AttributeTok{title=}\StringTok{"Timings of different numerical regression methods"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{LinAlg_files/figure-pdf/unnamed-chunk-17-1.pdf}

}

\end{figure}


\backmatter

\printindex

\end{document}
