[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantiles",
    "section": "",
    "text": "Genesis\nThis book is the result of an initiative that the Bank of England began in the early 1990s. This was a different world with a burgeoning new world order, as the Iron Curtain crumbled and the European experiment gathered momentum.\nPlaceholders abound.\nDisclaimer: The Bank of England does not accept any liability for misleading or inaccurate information or omissions in the information provided. The subject matter reflects the views of the individual presenter and not the wider Bank of England or its Policy Committees."
  },
  {
    "objectID": "intro.html#reading-is-good-for-you",
    "href": "intro.html#reading-is-good-for-you",
    "title": "1  Introduction",
    "section": "1.1 Reading is good for you",
    "text": "1.1 Reading is good for you\nFor me, the best (although slightly dated) text is Hastie, Tibshirani, and Friedman (2009) The Elements of Statistical Learning and the best source for the mathematics, with an easy-reading version by some of the same authors James et al. (2021) Introduction to Statistical Learning.\nI also rather like Boehmke and Greenwell (2019) Hands-On Machine Learning with R which is something of a cookbook rather than a technical manual but with wide scope. Taddy (2019) is more elementary.\nOn text, just read Silge and Robinson (2017) Text Mining with R: A Tidy Approach and then Hvitfeldt and Silge (2021) Supervised Machine Learning for Text Analysis in R. That’s it.\nTwo books I would solidly recommend to make us all into better statisticians and not just econometricians are Gelman, Hill, and Vehtari (2019) Regression and Other Stories, and McElreath (2020) Statistical Rethinking.\n\n\n\n\nBlake, Andrew P, and Haroon Mumtaz. 2017. Applied Bayesian Econometrics for Central Bankers. Revised. Technical Books. Centre for Central Banking Studies, Bank of England. https://www.bankofengland.co.uk/-/media/boe/files/ccbs/resources/applied-bayesian-econometrics-for-central-bankers-updated-2017.pdf.\n\n\nBoehmke, Brad, and Brandon M. Greenwell. 2019. Hands-on Machine Learning with R. The R Series. Boca Raton: Chapman & Hall/CRC. https://bradleyboehmke.github.io/HOML/.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2019. Regression and Other Stories. Cambridge: Cambridge University Press. http://www.stat.columbia.edu/~gelman/regression.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. New York, NY: Springer. https://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. Chapman & Hall: CRC Press. https://smltar.com/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. 2nd ed. Springer Texts in Statistics. New York, NY: Springer. https://www.statlearning.com/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Abingdon, Oxfordshire: CRC Press. https://github.com/rmcelreath/rethinking.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. O’Reilly. https://www.tidytextmining.com/.\n\n\nTaddy, Matt. 2019. Business Data Science: Combining Machine Learning and Economics to Optimize, Automate, and Accelerate Business Decisions. New York, NY: McGraw-Hill Education. https://github.com/TaddyLab/BDS."
  },
  {
    "objectID": "R2021.html#selected-ml-and-dataviz-techniques-using-r",
    "href": "R2021.html#selected-ml-and-dataviz-techniques-using-r",
    "title": "2  Non-econometric methods for econometricians",
    "section": "2.1 Selected ML and Dataviz techniques using R",
    "text": "2.1 Selected ML and Dataviz techniques using R\nEconometricians are used to handling data, performing analysis and reporting results. But somewhere along the line data became big and unstructured, analysis was now machines learning about something and outputs became visualisations.\nThis online seminar takes some big(ish) datasets, sets the machines on them and draws some great graphs. If you ever wondered what use a tree was for forecasting or why everything is a network (including neural ones), or wanted to draw a map with your house in it, or to understand a document without the bother of reading it (or a few other things besides) then you might find something to interest you. All done in R.\nSpecifically, this seminar is designed to introduce some of the key methods used outside of econometrics that econometricians will find very useful in their work in a central bank. This includes some important machine learning techniques as a gateway to others, particularly tree-based methods and neural networks, as well as text processing and map making. All the way through there is an emphasis on the network properties of many of these techniques. We make extensive use of the tidyverse, including ggplot2 and tidytext, and a number of statistics, machine learning, geographical data and other packages.\nThe framework for each day is the following:\n\nEach day is divided into two two-hour sessions starting at 10.30 am and 2.00 pm GMT.\nThe first hour of each will be an online presentation covering a particular topic (or topics) with a look at both techniques and code.\nAfter a quick break the second hour will be largely devoted to the code itself or resources to understand how to code the material.\n\nWe may run polls during the event to prioritize the topics covered in the webinars as it is not expected that everyone will be able to try out everything.\n\n2.1.1 The code\nAll code and some of the data will be made available through the Juno portal. For each presentation the .Rmd (R markdown) file is supplied that creates the presentation, an HTML file of the presentation for you to step through which can be re-created from the .Rmd file, and a further .R file of the code that we use. Some additional code and data is included, including links to a number of videos that cover some additional aspects both in this file and in the presentations.\nSome data will need to be downloaded from original other sites if all the examples are to be followed. All code is additionally available at https://github.com/andrewpeterblake/R2020 or https://github.com/andrewpeterblake/R2021 or through the QR codes below.\n\n\n\n\n\nGitHub: 2020 (grey, left), 2021 (pink, right)\n\n\n\n\n\n\n\nGitHub: 2020 (grey, left), 2021 (pink, right)"
  },
  {
    "objectID": "R2021.html#how-to-ensure-rstudio-finds-the-code",
    "href": "R2021.html#how-to-ensure-rstudio-finds-the-code",
    "title": "2  Non-econometric methods for econometricians",
    "section": "2.2 HOW TO ENSURE RSTUDIO FINDS THE CODE",
    "text": "2.2 HOW TO ENSURE RSTUDIO FINDS THE CODE\nTo use the code, in particular so that R Studio finds the data files etc, create a directory for each topic, (e.g. Trees, ANN etc) and copy the contents from the zip file or GitHub. Then create a new project in R Studio that uses that directory as its home directory, using “File/New Project” in the drop down menu. Opening files within a project sets the home directory to that directory, so everything (including the sub-directories) can be found."
  },
  {
    "objectID": "R2021.html#typical-program-structure",
    "href": "R2021.html#typical-program-structure",
    "title": "2  Non-econometric methods for econometricians",
    "section": "2.3 Typical program structure",
    "text": "2.3 Typical program structure\n\n2.3.1 Day 1: Trees and maps\n\n2.3.1.1 Trees\n\nClassification and regression trees\nEconometrics strikes back: Bootstrap/bagging and Boosting/Model selection\nRandom forests\nVisualising decision trees\nUse example: House prices\n\nThe presentations for this are Trees.html and LondonHP.html; The two programs TreeCancer.R and TreeNW.R are the use examples.\n\n\n2.3.1.2 Maps\n\nHow to draw a map in R\nA guide to some resources\nChoropleths\nUse examples: Climate change, regional data, postcode wrangling\n\nThe presentation for this is MapAER.html (see also Weatherpretty.html); The program MapAERcode.R is the main map drawing code, I’ve included ZAF.R as as short simple way and source for two countries, and the directory Trendz contains the program (app.R) and data for the weather example.\n\n\n\n\n\n\n\n\n\nI’ve included an additional video (red QR code) for more about Shiny. This uses unemployment data from the Survey of Professional Forecasters. The code we look at is for climate change data World Bank data.\nA comprehensive treatment of maps is Lovelace, Nowosad, and Muenchow (2019) Geocomputation in R, but it is quite a lot to assimilate all at once.\n\n\n\n\n\n2.3.2 Day 2: Networks\n\n2.3.2.1 Neural networks\n\nWhat is an ANN? Deep learning?\nFunction approximation via a network\nData: fit, validate, test\nNetwork architecture\nUse examples: House prices revisited\n\nThe presentation for this is IntroANN.html; The program ANN.R replicates the ANN estimation. The data used is the same as for Day 1.\n\n\n2.3.2.2 Networks (real ones)\n\nDAGs and ANNs as network graphs\nIncidence matrices\nMeasuring connectivity: Degree and betweenness\nPlotting with igraph\nUse examples: Industry inter-relationships\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe presentation used for the first part of this is DAG.html and the program Draw_DAG_ANN.R draws the ANN examples from Day 2 Session 1 as well as some of the DAG examples. The example is modified from Cunningham (2021) Causal Inference: The Mixtape, which is a great read with R code. The pdf HandShake3.pdf is the source of the director network graphs, and Graph101a.R is a subset of the analytical work on the corruption data set as described in the post Graph Theory 101 (purple QR code), which is the work of Marina Medina (blue QR code link to presentation site).\n\n\n\n\n2.3.3 Day 3: Text\n\n2.3.3.1 Text modelling, a ‘tidytext’ approach (Session 1)\n\nData cleaning\nSentiment\nTopic modelling\n\n\n\n2.3.3.2 Text modelling, a ‘tidytext’ approach (Session 2)\n\nParts-of-speech tagging\nText regression\nUse examples: Central bank minutes, reports\n\n\n\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. New Haven & London: Yale University Press. https://mixtape.scunning.com/.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with R. 1st ed. The R Series. Boca Raton: Chapman & Hall/CRC. https://geocompr.robinlovelace.net/."
  },
  {
    "objectID": "QR.html#getting-the-data",
    "href": "QR.html#getting-the-data",
    "title": "4  Quantile regression",
    "section": "4.1 Getting the data",
    "text": "4.1 Getting the data\nWe download the data and save it locally.\n\nh <- \"https://www.philadelphiafed.org/-/media/frbp/assets/surveys-and-data/survey-of-professional-forecasters/historical-data/\"\nf <- \"meanlevel.xlsx\"\n\ndownload.file(paste0(h, f), destfile=f, mode=\"wb\")\n\nRetrieve the unemployment data for the average unemployment forecast.\n\nUNEMP <- f %>%\n  read_excel(na=\"#N/A\", sheet=\"UNEMP\") %>% \n  mutate(Date=as.Date(as.yearqtr(paste(YEAR, QUARTER), format=\"%Y %q\"))) \n\nUsel <- UNEMP %>% \n  select(Date, UNEMP1, UNEMP3, UNEMP4, UNEMP5, UNEMP6) %>%\n  mutate(UNRATE = lead(UNEMP1,1)) %>%\n  select(Date, UNRATE, \n         UNEMP1=UNEMP3, UNEMP2=UNEMP4, UNEMP3=UNEMP5, UNEMP4=UNEMP6) %>%\n  mutate(UNEMP1 = lag(UNEMP1,1), \n         UNEMP2 = lag(UNEMP2,2), \n         UNEMP3 = lag(UNEMP3,3), \n         UNEMP4 = lag(UNEMP4,4)) %>%\n  pivot_longer(cols = -c(Date, UNRATE), names_to=\"Which\", values_to=\"Val\") %>%\n  filter(year(Date) > 2000)"
  },
  {
    "objectID": "QR.html#plots",
    "href": "QR.html#plots",
    "title": "4  Quantile regression",
    "section": "4.2 Plots",
    "text": "4.2 Plots\n\nUsel %>% \n  ggplot(aes(x=Date)) + \n  geom_line(aes(y=UNRATE), colour=\"red\") + \n  geom_point(aes(y=Val, colour=Which, shape=Which)) +\n  theme_light() + \n  labs(title=\"Mean unemployment forecasts\", x=\"\", y=\"\", caption=\"Source: SPF\")"
  },
  {
    "objectID": "Stemp.html#study-question-1.3.2",
    "href": "Stemp.html#study-question-1.3.2",
    "title": "5  Causal Inference",
    "section": "5.1 Study question 1.3.2",
    "text": "5.1 Study question 1.3.2\nData:\n\nlibrary(tidyverse)\ned <- tibble(Gender = c(\"M\",\"M\",\"M\",\"M\",\"F\",\"F\",\"F\",\"F\"),\n             eLevel = c(\"U\",\"H\",\"C\",\"G\",\"U\",\"H\",\"C\",\"G\"),\n             num    = c(112,231,595,242,136,189,763,172)) %>%\n  mutate(total = sum(num))\n\nwhich we tabulate as\n\ned %>%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\n\n\n\n\nM\nU\n112\n2440\n\n\nM\nH\n231\n2440\n\n\nM\nC\n595\n2440\n\n\nM\nG\n242\n2440\n\n\nF\nU\n136\n2440\n\n\nF\nH\n189\n2440\n\n\nF\nC\n763\n2440\n\n\nF\nG\n172\n2440"
  },
  {
    "objectID": "Stemp.html#exercises-and-answers",
    "href": "Stemp.html#exercises-and-answers",
    "title": "5  Causal Inference",
    "section": "5.2 Exercises and answers",
    "text": "5.2 Exercises and answers\n\n5.2.1 Find \\(P(eLevel = H)\\)\n\ned %>%\n  filter(eLevel == \"H\") %>%\n  mutate(p_H = sum(num)/total) %>%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\np_H\n\n\n\n\nM\nH\n231\n2440\n0.1721311\n\n\nF\nH\n189\n2440\n0.1721311\n\n\n\n\n\n\n\n5.2.2 Find \\(P(eLevel = H\\ \\vee \\ Gender = F)\\)\n\ned %>%\n  filter(Gender == \"F\" | eLevel == \"H\") %>%\n  mutate(p_HorF = sum(num)/total) %>%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\np_HorF\n\n\n\n\nM\nH\n231\n2440\n0.6110656\n\n\nF\nU\n136\n2440\n0.6110656\n\n\nF\nH\n189\n2440\n0.6110656\n\n\nF\nC\n763\n2440\n0.6110656\n\n\nF\nG\n172\n2440\n0.6110656\n\n\n\n\n\n\n\n5.2.3 Find \\(P(eLevel = H\\ |\\ Gender = F)\\)\n\ned %>%\n  filter(Gender == \"F\") %>%\n  mutate(tcond = sum(num)) %>% \n  filter(eLevel == \"H\") %>%\n  mutate(p_HgivenF = sum(num)/tcond) %>%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\ntcond\np_HgivenF\n\n\n\n\nF\nH\n189\n2440\n1260\n0.15\n\n\n\n\n\n\n\n5.2.4 Find \\(P(Gender = F\\ | \\ eLevel = H)\\)\n\ned %>%\n  filter(eLevel == \"H\") %>%\n  mutate(tcond = sum(num)) %>% \n  filter(Gender == \"F\") %>%\n  mutate(p_FgivenH = sum(num)/tcond) %>%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\ntcond\np_FgivenH\n\n\n\n\nF\nH\n189\n2440\n420\n0.45\n\n\n\n\n\n\n\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal Inference in Statistics: A Primer. Chichester: John Wiley & Sons. http://bayes.cs.ucla.edu/PRIMER/."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "7  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever…"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bahaj, Saleem, Angus Foulis, and Gabor Pinter. 2020. “Home Values\nand Firm Behavior.” American Economic Review 110 (7):\n2225–70. https://doi.org/10.1257/aer.20180649.\n\n\nBanbura, M., D. Giannone, and L. Reichlin. 2010. “Large\nBayesian Vector Auto Regressions.” Journal of\nApplied Econometrics 25 (1): 71–92.\n\n\nBlake, Andrew P. 2004. “Analytic Derivatives in Linear Rational\nExpectations Models.” Computational Economics 24 (1):\n77–96.\n\n\nBlake, Andrew P, and Haroon Mumtaz. 2017. Applied Bayesian\nEconometrics for Central Bankers. Revised. Technical Books. Centre\nfor Central Banking Studies, Bank of England. https://www.bankofengland.co.uk/-/media/boe/files/ccbs/resources/applied-bayesian-econometrics-for-central-bankers-updated-2017.pdf.\n\n\nBlanchard, O., and C. Kahn. 1980. “The Solution of Linear\nDifference Models Under Rational Expectations.”\nEconometrica 48 (5): 1305–11.\n\n\nBoehmke, Brad, and Brandon M. Greenwell. 2019. Hands-on Machine\nLearning with R. The R Series. Boca\nRaton: Chapman & Hall/CRC. https://bradleyboehmke.github.io/HOML/.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. New\nHaven & London: Yale University Press. https://mixtape.scunning.com/.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2019. Regression and\nOther Stories. Cambridge: Cambridge University Press. http://www.stat.columbia.edu/~gelman/regression.\n\n\nGiannone, Domenico, Michele Lenza, and Giorgio E. Primiceri. 2015.\n“Prior Selection for Vector\nAutoregressions.” The Review of Economics and\nStatistics 97 (2): 436–51.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The\nElements of Statistical Learning: Data Mining, Inference, and\nPrediction. 2nd ed. New York, NY: Springer. https://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning\nfor Text Analysis in R. Chapman & Hall: CRC Press.\nhttps://smltar.com/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical Learning: With Applications in\nR. 2nd ed. Springer Texts in Statistics. New York, NY:\nSpringer. https://www.statlearning.com/.\n\n\nKing, Robert G., and Mark W. Watson. 2002. “System Reduction and\nSolution Algorithms for Singular Linear Difference Systems Under\nRational Expectations.” Computational Economics 20\n(1–2): 57–86.\n\n\nKlein, Paul. 2000. “Using the Generalized Schur Form\nto Solve a Multivariate Linear Rational Expectations Model.”\nJournal of Economic Dynamics and Control 24 (10): 1405–23.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019.\nGeocomputation with R. 1st ed. The R\nSeries. Boca Raton: Chapman & Hall/CRC. https://geocompr.robinlovelace.net/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in R and Stan. 2nd ed. Abingdon,\nOxfordshire: CRC Press. https://github.com/rmcelreath/rethinking.\n\n\nPappas, T., A. J. Laub, and N. R. Sandell. 1980. “On the Numerical\nSolution of the Discrete-Time Algebraic Riccati Equation.”\nIEEE Transaction on Automatic Control AC-25.4: 631–41.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal\nInference in Statistics: A Primer. Chichester: John Wiley &\nSons. http://bayes.cs.ucla.edu/PRIMER/.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with\nR: A Tidy Approach. O’Reilly. https://www.tidytextmining.com/.\n\n\nTaddy, Matt. 2019. Business Data Science: Combining Machine Learning\nand Economics to Optimize, Automate, and Accelerate Business\nDecisions. New York, NY: McGraw-Hill Education. https://github.com/TaddyLab/BDS.\n\n\nTheil, Henri, and Arthur S. Goldberger. 1961. “On Pure and Mixed\nStatistical Estimation in Economics.” International Economic\nReview 2: 317–32.\n\n\nVaughan, D. R. 1970. “A Non Recursive Algorithm Solution for the\nDiscrete Riccati Equation.” IEEE Transactions on Automatic\nControl AC-15.5: 597–99."
  },
  {
    "objectID": "R2021.html#selected-ml-and-dataviz-techniques-in-r",
    "href": "R2021.html#selected-ml-and-dataviz-techniques-in-r",
    "title": "2  Non-econometric methods for econometricians",
    "section": "2.1 Selected ML and Dataviz techniques in R",
    "text": "2.1 Selected ML and Dataviz techniques in R\nEconometricians are used to handling data, performing analysis and reporting results. But somewhere along the line data became big and unstructured, analysis was now machines learning about something and outputs became visualisations.\nThis online seminar takes some big(ish) datasets, sets the machines on them and draws some great graphs. If you ever wondered what use a tree was for forecasting or why everything is a network (including neural ones), or wanted to draw a map with your house in it, or to understand a document without the bother of reading it (or a few other things besides) then you might find something to interest you. All done in R.\nSpecifically, this seminar is designed to introduce some of the key methods used outside of econometrics that econometricians will find very useful in their work in a central bank. This includes some important machine learning techniques as a gateway to others, particularly tree-based methods and neural networks, as well as text processing and map making. All the way through there is an emphasis on the network properties of many of these techniques. We make extensive use of the tidyverse, including ggplot2 and tidytext, and a number of statistics, machine learning, geographical data and other packages.\nThe framework for each day is the following:\n\nEach day is divided into two two-hour sessions starting at 10.30 am and 2.00 pm GMT.\nThe first hour of each will be an online presentation covering a particular topic (or topics) with a look at both techniques and code.\nAfter a quick break the second hour will be largely devoted to the code itself or resources to understand how to code the material.\n\nWe may run polls during the event to prioritize the topics covered in the webinars as it is not expected that everyone will be able to try out everything.\n\n2.1.1 The code\nAll code and some of the data will be made available through the Juno portal. For each presentation the .Rmd (R markdown) file is supplied that creates the presentation, an HTML file of the presentation for you to step through which can be re-created from the .Rmd file, and a further .R file of the code that we use. Some additional code and data is included, including links to a number of videos that cover some additional aspects both in this file and in the presentations.\nSome data will need to be downloaded from original other sites if all the examples are to be followed. All code is additionally available at https://github.com/andrewpeterblake/R2020 or https://github.com/andrewpeterblake/R2021 or through the QR codes below.\n\n\n\n\n\nGitHub: 2020 (grey, left), 2021 (pink, right)\n\n\n\n\n\n\n\nGitHub: 2020 (grey, left), 2021 (pink, right)"
  },
  {
    "objectID": "index.html#genesis",
    "href": "index.html#genesis",
    "title": "Quantiles",
    "section": "Genesis",
    "text": "Genesis\nThis book is the result of an initiative that the Bank of England began in the early 1990s. This was a different world with a burgeoning new world order, as the Iron Curtain crumbled and the European experiment gathered momentum.\nPlaceholders abound."
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Quantiles",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe Bank of England does not accept any liability for misleading or inaccurate information or omissions in the information provided. The subject matter reflects the views of the individual presenter and not the wider Bank of England or its Policy Committees."
  },
  {
    "objectID": "index.html#sec-genesis",
    "href": "index.html#sec-genesis",
    "title": "Quantiles, Networks, Time",
    "section": "Genesis",
    "text": "Genesis\nThis book is the result of an initiative the Bank of England began in the early 1990s, and a prescient one. This was at a major historical turning point, one that signalled a burgeoning new world order, as the Iron Curtain crumbled, the European experiment gathered momentum, and industrial might began an inexorable shift eastwards.\n\nPlaceholders abound."
  },
  {
    "objectID": "index.html#sec-disclaimer",
    "href": "index.html#sec-disclaimer",
    "title": "Quantiles, Networks, Time",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe Bank of England does not accept any liability for misleading or inaccurate information or omissions in the information provided. The subject matter reflects the views of the individual presenter and not the wider Bank of England or its Policy Committees."
  },
  {
    "objectID": "Maps.html#how-heterogenous-is-uk-house-price-inflation",
    "href": "Maps.html#how-heterogenous-is-uk-house-price-inflation",
    "title": "6  Mapping regional house price inflation",
    "section": "6.1 How heterogenous is UK house price inflation?",
    "text": "6.1 How heterogenous is UK house price inflation?\nA simple enough question, and one that Bahaj, Foulis, and Pinter (2020) thought was best answered with a map – actually a referee asked for one. As I know how to draw a map in R they asked me if I could do it. Well yes, but there are some particular difficulties.\n\nThe UK (actually Great Britain) is an awkward (but not too awkward) shape.\nPopulation in the UK is heavily concentrated in a small number of centres, such as London or Manchester.\nThere are three different periods to compare.\nIt has to be in grayscale.\n\nBefore all of this we need some data, with boundaries that correspond to areas that we have data for. The regional inflation data is available at the level of the Land Registry, which almost by local authority but amalgamates a number of the areas. So a map at Local Authority level would be fine as long as we can amalgamate some of the regions.\nThe map data used here is available from the UK’s ONS geoportal, with a lot of administrative data available including local authority boundaries. The Local Authority data is specifically available from here, where I use the clipped full extent version. There are a number of possibilities, but in general high water mark, and enough but not too much detail is needed.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(sf)\n\nThe information in the map file is comprehensive, and by Local Authority as of December 2015.\n\nfle <- \"LAD_Dec_2015_GCB_GB\"\nshape <- read_sf(dsn=\".\", layer=fle)\n\nWe can look at the attributes using summary.\n\nsummary(shape)\n\n   lad15cd            lad15nm            lad15nmw           GlobalID        \n Length:380         Length:380         Length:380         Length:380        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n          geometry  \n MULTIPOLYGON :380  \n epsg:27700   :  0  \n +proj=tmer...:  0  \n\n\nThis can be plotted straightforwardly using ggplot.\n\nshape %>%\n  ggplot() +\n  geom_sf(aes(geometry=geometry, fill=lad15nm), \n          color=NA, alpha=.66, show.legend=FALSE) +\n  theme_void()\n\n\n\n\n\n\n\n\nLooking at the read-out above, each of the 380 regions have some metadata associated, which are contained in each of the listed attributes. It should be obvious that objectid is just a sequence from 1 to 380. lad15nm turns out to be a list of names of the regions – I suspect lad for Local Authority District, 15 for 2015 and nm for name – and it is easy to specify this as the name to use for the region when using tidy.\nNow this can be plotted using ggplot, using geometry for the \\(x\\) and \\(y\\) coordinates. The choice of fill colour is determined by fill and we can set the colour of the lines by colour (or color). The two extra arguments are for a suitable blank style and to impose an appropriate ratio of height to width.\nImmediately, the awkward shape of the British Isles is apparent. (Note this is a plot of Great Britain, and there is no Northern Ireland.) The islands to the far north are somewhat unnecessary, although quite rightly the inhabitants get a bit tired of being left off maps! Nonetheless I’ll do exactly the same by filtering out the polygons associated with Orkney Islands and Shetland Islands.\nFewer Scottish Islands makes the graphs a lot clearer with little loss of information, given the tiny number of transactions in the Orkneys and the Shetlands, very far to the north.\nIn what follows we filter out the islands using\n\nshape <- read_sf(dsn=\".\", layer=fle) %>%\n  filter(!lad15nm %in% c(\"Shetland Islands\",\"Orkney Islands\")) %>%\n  mutate(Country=str_sub(lad15cd, 1, 1), .after=1)\n\nwhere we also create an indicator of country using the first letter of the code string.\nSo the country map is\n\nshape %>%\n  group_by(Country) %>%\n  summarise() %>%\n  ggplot() +\n  geom_sf(aes(fill=Country), color=\"grey77\", linewidth=.25, alpha=.66) +\n  theme_void()\n\n\n\n\n\n\n\n\nNote the really nice feature – if we group by something, in this case country, we can summarise to amalgamate the geometries!\nYou may have noticed, one thing that that’s missing on the LA graphs is the boundaries. They aren’t, they’re just invisible. That’s because I set colour = NA, so I can fix that by choosing a colour and making the lines very thin so they don’t swamp the map, as in the country one.\nOne further amendment, the fill is moved inside the aes() specification and made conditional. R now chooses unique colours for each of the regions.\nTwo things now need to be done to get the map colours right to illustrate regional inflation rates. First we need to amalgamate some of the Local Authority boundaries to the Land Registry definitions, and second we need to assign the inflation rate to each area."
  },
  {
    "objectID": "Maps.html#inflation-in-grayscale",
    "href": "Maps.html#inflation-in-grayscale",
    "title": "5  Mapping regional house price inflation",
    "section": "5.2 Inflation in grayscale",
    "text": "5.2 Inflation in grayscale\nAll the information required to plot the Land Registry-based regional inflation rates is now available. As you can see from the buckinghamshire data above, there are three average rates in three different periods, so I’ll focus on one, 2002-2007 to begin with.\nFirst, augment the geographic data with the inflation data, and call them something better.\n\ngg %>%\n  ggplot() +\n  geom_sf(aes(geometry=geometry, fill=land_reg_region), \n          color=NA, alpha=.66, show.legend = FALSE) +\n  theme_void()\n\n\n\n\n\n\n\n\nThen calculate some limits for the gradient, and modify the specify a plot to include specifically gray and put the legend at the bottom.\n\nnms <- gsub(\"av_hp_growth\", \"HPI\", colnames(hp_data))\n\nhp_data %>%\n  rename_all( ~ nms) %>%\n  select(land_reg_region, starts_with(\"HPI\")) %>%\n  distinct() %>%\n  left_join(gg) %>%\n  ggplot() +\n  geom_sf(aes(geometry=geometry, fill=HPI_02_07), \n          color=NA, alpha=.66, show.legend = FALSE) +\n  theme_void() +\n  scale_fill_gradient(low=grey(0.9), high=grey(0.05)) +\n  theme(legend.direction = \"horizontal\", \n        legend.position  = c(0.75,0.05),\n        legend.title     = element_blank())\n\nJoining with `by = join_by(land_reg_region)`\n\n\n\n\n\n\n\n\n\n\n\n\n\nBahaj, Saleem, Angus Foulis, and Gabor Pinter. 2020. “Home Values and Firm Behavior.” American Economic Review 110 (7): 2225–70. https://doi.org/10.1257/aer.20180649."
  },
  {
    "objectID": "Maps.html#inflation-data-and-regions",
    "href": "Maps.html#inflation-data-and-regions",
    "title": "6  Mapping regional house price inflation",
    "section": "6.2 Inflation data and regions",
    "text": "6.2 Inflation data and regions\nWe have a map, and we have that data in a form that is easy to understand. If we can suitably attach an inflation rate to each area then we can fill the individual areas with a colour unique to each individual inflation rates.\nRecall that the Land Registry areas aren’t quite what we have, and will need amalgamating. Bahaj, Foulis, and Pinter (2020) supplied me the areas that needed amalgamating (and the inflation rates) using the ONS codes. This is contained in the metadata lad15cd above.\nThe data is structured in ‘wide’ format with one row for each Land Registry region. The details aren’t very important for us now, but what it means is I can manipulate it to get\n\n# Price data by Land Registry region, converted to long format\nhp_data <- read_excel(\"house_price_data_figure_1.xls\")  %>% \n  select(\"land_reg_region\", starts_with(\"e_\"), starts_with(\"av_\")) %>% \n  pivot_longer(names_to  = \"name\", \n               values_to = \"lad15cd\", \n               cols      = c(-land_reg_region, -starts_with(\"av_\"))) %>% \n  drop_na() %>%\n  select(land_reg_region, lad15cd, starts_with(\"av_\")) \n\ncodes <- hp_data %>% \n  select(lad15cd, land_reg_region) \n\nThe important thing that the pivot_longer achieves is that for every land_reg_region I get a list of all the ONS codes that makes up the Local Authority level. So if I look at buckinghamshire as an example there are four ONS codes now associated with it.\n\nfilter(codes, land_reg_region == \"buckinghamshire\")\n\n# A tibble: 4 × 2\n  lad15cd   land_reg_region\n  <chr>     <chr>          \n1 E07000004 buckinghamshire\n2 E07000005 buckinghamshire\n3 E07000006 buckinghamshire\n4 E07000007 buckinghamshire\n\n\nJoin these together\n\n# Join polygons defined by Land Registry regions\ngg <- shape %>%\n  select(starts_with(c(\"lad\",\"C\"))) %>% \n  left_join(codes, by=\"lad15cd\") %>%\n  group_by(land_reg_region) %>%\n  summarise() \n\nwhich produces a match between the Land Registry and the Local Authority areas, plus the inflation rates.\n\n6.2.1 Inflation in grayscale\nAll the information required to plot the Land Registry-based regional inflation rates is now available. As you can see from the buckinghamshire data above, there are three average rates in three different periods, so I’ll focus on one, 2002-2007 to begin with.\nFirst, augment the geographic data with the inflation data, and call them something better.\n\ngg %>%\n  ggplot() +\n  geom_sf(aes(geometry=geometry, fill=land_reg_region), \n          color=NA, alpha=.66, show.legend = FALSE) +\n  theme_void()\n\n\n\n\n\n\n\n\nThen specify gray and put the legend at the bottom.\n\nnms <- gsub(\"av_hp_growth\", \"HPI\", colnames(hp_data))\n\nhp_data %>%\n  rename_all( ~ nms) %>%\n  select(land_reg_region, starts_with(\"HPI\")) %>%\n  distinct() %>%\n  left_join(gg) %>%\n  ggplot() +\n  geom_sf(aes(geometry=geometry, fill=HPI_02_07), \n          color=NA, alpha=.66, show.legend = TRUE) +\n  theme_void() +\n  scale_fill_gradient(low=grey(0.9), high=grey(0.05)) +\n  theme(legend.direction = \"horizontal\", \n        legend.position  = c(0.75,0.05),\n        legend.title     = element_blank())\n\nJoining with `by = join_by(land_reg_region)`\n\n\n\n\n\n\n\n\n\n\n\n\n\nBahaj, Saleem, Angus Foulis, and Gabor Pinter. 2020. “Home Values and Firm Behavior.” American Economic Review 110 (7): 2225–70. https://doi.org/10.1257/aer.20180649."
  },
  {
    "objectID": "BVAR.html#estimating-bvars-using-us-data",
    "href": "BVAR.html#estimating-bvars-using-us-data",
    "title": "3  BVAR with dummies",
    "section": "3.1 Estimating BVARs using US data",
    "text": "3.1 Estimating BVARs using US data\nWe will the Fed Funds rate, annual GDP growth and annual CPI inflation data from FRED, retrieved 2023-05-23. These are:\n\n\n\n\n\n\n\n\n\nWe will build a variety and two and three variable BVARs. More details on the data are given below."
  },
  {
    "objectID": "BVAR.html#bvars-with-dummy-variable-priors",
    "href": "BVAR.html#bvars-with-dummy-variable-priors",
    "title": "3  BVAR with dummies",
    "section": "3.2 BVARs with dummy variable priors",
    "text": "3.2 BVARs with dummy variable priors\nRather than combine a prior distribution with a likelihood and draw from the resulting joint posterior distribution there is another convenient way of parameterizing the problem. We can instead add some ‘dummy variables’ that have the same properties of the prior so we have a single modified likelihood that incorporates the prior information. This approach was most obviously adopted by Banbura, Giannone, and Reichlin (2010). Further discussion of this can be found in Giannone, Lenza, and Primiceri (2015). In general this is a version of the Theil and Goldberger (1961) mixed estimator given a Bayesian interpretation.\n\n3.2.1 VAR model\nSimple bi-variate two-lag VAR model: \\[\n  \\left[\\matrix{g_t \\cr \\pi_t}\\right] =\n  \\left[\\matrix{c_1 \\cr c_2}\\right] +\n  \\left[\\matrix{b_{11} & b_{12} \\cr\n    b_{21} & b_{22}}\\right]\n\\left[\\matrix{g_{t-1} \\cr \\pi_{t-1} }\\right] +\n   \\left[\\matrix{d_{11} & d_{12} \\cr\n    d_{21} & d_{22}}\\right]\n\\left[\\matrix{g_{t-2} \\cr \\pi_{t-2} }\\right] +\n  \\left[\\matrix{\\nu_{g,t} \\cr \\nu_{\\pi,t}}\\right]\n\\] \\[\n  \\left[\\matrix{\\nu_{g,t} \\cr \\nu_{\\pi,t}}\\right] \\sim N(0, \\Sigma)\n\\]"
  },
  {
    "objectID": "BVAR.html#bvar-hyperparameters",
    "href": "BVAR.html#bvar-hyperparameters",
    "title": "3  BVAR with dummies",
    "section": "3.3 BVAR hyperparameters",
    "text": "3.3 BVAR hyperparameters\nWe will (similarly to the straightforward Minnesota prior) need some control parameters:\n\n\\(\\tau\\) controls the overall tightness of the prior for the AR coefficients\n\\(d\\) controls the prior on higher lags;\n\\(\\lambda\\) controls the prior on constants;\n\\(\\gamma\\) controls the prior on the sum of coefficients;\n\\(\\delta\\) controls the cointegration prior;\n\nwhere\n\n\\(\\sigma_i\\) standard deviation of error terms from individual OLS regressions;\n\\(\\mu_i\\) sample means of the data.\n\n\n3.3.0.1 First lag\nNow consider the following artificial data for the first lag. We construct some dummy observations of the dependent and explanatory variables that look like: \\[\n  Y_{D,1} = \\left[\\matrix{\\frac{1}{\\tau}\\sigma_1 & 0 \\cr\n    0 & \\frac{1}{\\tau}\\sigma_2}\\right]\n\\] and \\[\n  X_{D,1} = \\left [ \\matrix{0 & \\frac{1}{\\tau}\\sigma_1 & 0 & 0 & 0\\cr\n    0 & 0 & \\frac{1}{\\tau}\\sigma_2 & 0 & 0}\\right]\n\\] Intuition: \\[\n  \\left[\\matrix{\\frac{\\sigma_1}{\\tau} & 0 \\cr\n    0 & \\frac{\\sigma_2}{\\tau}}\\right] =\n  \\left[\\matrix{0 & \\frac{\\sigma_1}{\\tau} & 0 & 0 & 0\\cr\n   0 & 0 & \\frac{\\sigma_2}{\\tau} & 0 & 0} \\right]\n\\left[\\matrix{c_1    & c_2 \\cr\n  b_{11} & b_{21} \\cr\n  b_{12} & b_{22} \\cr\n  d_{11} & d_{21} \\cr\n  d_{12} & d_{22}}\\right]\n+\n  \\left[\\matrix{\\xi_{11} & \\xi_{12} \\cr \\xi_{21} & \\xi_{22} }\\right]\n\\] Multiplying out we get: \\[\n  \\left[\\matrix{\\frac{\\sigma_1}{\\tau} & 0 \\cr\n    0 & \\frac{\\sigma_2}{\\tau}}\\right]\n=\n  \\left[\\matrix{\\frac{\\sigma_1}{\\tau}b_{11} &   \\frac{\\sigma_1}{\\tau}b_{21}\\cr\n  \\frac{\\sigma_2}{\\tau}b_{12} &  \\frac{\\sigma_2}{\\tau}b_{22}} \\right]\n+\n  \\left[\\matrix{\\xi_{11} & \\xi_{12} \\cr \\xi_{21} & \\xi_{22} }\\right]\n\\] Concentrating on the first row, notice: \\[\n  \\frac{\\sigma_1}{\\tau} = \\frac{\\sigma_1}{\\tau}b_{11} + \\xi_{11}\n\\] implying: \\[\n  b_{11} = 1 - \\frac{\\tau}{\\sigma_1}\\xi_{11}\n\\] so we can write: \\[\n  b_{11} \\sim N\\left(1, \\frac{\\tau^2var(\\xi_{11})}{\\sigma^2_1}\\right)\n\\] as \\(E[b_{11}] = 1 - \\frac{\\tau}{\\sigma_1}E[\\xi_{11}] = 1\\) and the variance is easily derived. Similarly: \\[\n  b_{12} = - \\frac{\\tau}{\\sigma_1}\\xi_{12}\n\\] which is clearly zero in expectation.\n\n\n3.3.1 Further priors\n\n3.3.1.1 Higher lags\nRather than derive the implications we state the rest of the dummy priors. Consider the following artificial data for the second lag: \\[\nY_{D,2} = \\left[\\matrix{0 & 0 \\cr 0 & 0}\\right]\n\\] and: \\[\nX_{D,2} = \\left [ \\matrix{0 & 0 & 0 & \\frac{\\sigma_1 2^d}{\\tau} & 0 \\cr\n    0 & 0 & 0 & 0 & \\frac{\\sigma_2 2^d}{\\tau} }\\right]\n\\] We can multiply these out and check the properties, in particular we can verify in the same way as for the first lag that: \\[\n  b_{ji} \\sim N\\left(0, \\frac{1}{4}\\frac{\\tau^2var(\\xi_{ji})} {2^d\\sigma^2_j}\\right)\n\\] for \\(j=1,...N\\), \\(i=1,...l\\).\n\n\n3.3.1.2 Constant\nConsider the following artificial data for the constant: \\[\n  Y_{D,3} = \\left[\\matrix{0 & 0 }\\right]\n\\] \\[\n  X_{D,3} = \\left [ \\matrix{\\lambda & 0 & 0 & 0 & 0 }\\right]\n\\] so \\(\\lambda c_1 = \\varepsilon_1\\) and \\(\\lambda c_2 = \\varepsilon_2\\). As \\(\\lambda \\rightarrow \\infty\\) the prior is implemented more tightly.\n\n\n3.3.1.3 Covariances\nDummy observations to implement the prior on the error covariance matrix are: \\[\n  Y_{D,4} = \\left[\\matrix{\\sigma_1 & 0 \\cr 0 & \\sigma_2}\\right]\n\\] and \\[\n  X_{D,4} = \\left [ \\matrix{0 & 0 & 0 & 0 & 0 \\cr\n    0 & 0 & 0 & 0 & 0 }\\right]\n\\] with the magnitude of the diagonal elements of \\(\\Sigma\\) controlled by the scale of the diagonal elements of \\(Y_{D,4}\\), as larger diagonal elements implement the prior belief that the variance of \\(\\nu_1\\) and \\(\\nu_2\\) is larger.\nBanbura, Giannone, and Reichlin (2010) stop here, but there are additional priors that could be added.\n\n\n3.3.1.4 Sum of coefficients\nWe could add a prior that reflects the belief that the sum of coefficients on ‘own’ lags add up to 1. This is an additional ‘unit root’-style prior. Consider: \\[\n  Y_{D,5} = \\left[\\matrix{\\gamma\\mu_1 & 0\\cr 0 & \\gamma\\mu_2}\\right]\n\\] and \\[\n  X_{D,5} = \\left [ \\matrix{0 & \\gamma\\mu_1 & 0 & \\gamma\\mu_1 & 0\\cr\n    0 & 0 & \\gamma\\mu_2 & 0 & \\gamma\\mu_2}\\right]\n\\] where \\(\\mu_1\\) is the sample mean of \\(y_t\\) and \\(\\mu_2\\) is the sample mean of \\(x_t\\). Note that these dummy observations imply prior means of the form \\(b_{ii} + d_{ii} = 1\\) where \\(i = 1, 2\\) and \\(\\gamma\\) controls the tightness of the prior. As \\(\\gamma \\rightarrow \\infty\\) the prior is implemented more tightly. Forecast growth rates eventually converge to their sample averages.\n\n\n3.3.1.5 Trends\nWe can also specify common stochastic trend dummies: \\[\n  Y_{D,6} = \\left[\\matrix{\\delta\\mu_1 & \\delta\\mu_2 }\\right]\n\\] and \\[\n  X_{D,6} = \\left [ \\matrix{\\delta & \\delta\\mu_1 & \\delta\\mu_2 & \\delta\\mu_1 & \\delta\\mu_2 }\\right]\n\\] where this imposes that the coefficients are consistent with limiting the amount of drift between the predictions at their average values.\n\n\n\n3.3.2 Implementation\nThe data and the artificial data are now stacked: \\[\n  Y^* = \\left[\\matrix{ g_3 & \\pi_3 \\cr\n    \\vdots & \\vdots \\cr\n    g_T & \\pi_T \\cr\n    \\frac{1}{\\tau}\\sigma_1 & 0 \\cr\n    0 & \\frac{1}{\\tau}\\sigma_2\\cr\n    0 & 0 \\cr\n    0 & 0 \\cr\n    0 & 0 \\cr\n    \\sigma_1 & 0 \\cr\n    0 & \\sigma_2 \\cr\n    \\gamma\\mu_1 & 0 \\cr\n    0 & \\gamma\\mu_2 \\cr\n    \\delta\\mu_1 & \\delta\\mu_2 }\\right], \\quad\nX^* = \\left [ \\matrix{1 & g_2 & \\pi_2 & g_1 & \\pi_1 \\cr\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\cr\n  1 & g_{T-1} & \\pi_{T-1} & g_{T-2} & \\pi_{T-2} \\cr\n  0 & \\frac{1}{\\tau}\\sigma_1 & 0 & 0 & 0\\cr\n  0 & 0 & \\frac{1}{\\tau}\\sigma_2 & 0 & 0\\cr\n  0 & 0 & 0 & \\frac{\\sigma_1 2^d}{\\tau} & 0 \\cr\n  0 & 0 & 0 & 0 & \\frac{\\sigma_2 2^d}{\\tau} \\cr\n  \\lambda & 0 & 0 & 0 & 0 \\cr\n  0 & 0 & 0 & 0 & 0 \\cr\n  0 & 0 & 0 & 0 & 0 \\cr\n  0 & \\gamma\\mu_1 & 0 & \\gamma\\mu_1 & 0 \\cr\n  0 & 0 & \\gamma\\mu_2 & 0 & \\gamma\\mu_2 \\cr\n  \\delta & \\delta\\mu_1 & \\delta\\mu_2 & \\delta\\mu_1 & \\delta\\mu_2 }\\right]\n\\] Estimation via Gibbs sampling now proceeds in a very straightforward way. There is no need to draw for the prior separately."
  },
  {
    "objectID": "BVAR.html#examples",
    "href": "BVAR.html#examples",
    "title": "3  BVAR with dummies",
    "section": "3.4 Examples",
    "text": "3.4 Examples\nFirst we use quarterly US Growth (FRED series A191RO1Q156NBEA) and CPI (FRED series CPALTT01USQ661S) expressed as the annual inflation rate from 1961-01-01 to 2023-01-01 in a bi-variate BVAR. The last ten observations are:\n\n\n\n\n\nDate\nGrowth\nInflation\n\n\n\n\n2020-10-01\n-1.5\n1.224176\n\n\n2021-01-01\n1.2\n1.905310\n\n\n2021-04-01\n12.5\n4.776278\n\n\n2021-07-01\n5.0\n5.264633\n\n\n2021-10-01\n5.7\n6.765892\n\n\n2022-01-01\n3.7\n8.023109\n\n\n2022-04-01\n1.8\n8.556077\n\n\n2022-07-01\n1.9\n8.284860\n\n\n2022-10-01\n0.9\n7.110821\n\n\n2023-01-01\n1.6\n5.769521\n\n\n\n\n\nWe specify a VAR with two lags, and use it to forecast 12 periods ahead. The BVAR are specified using the names above, with only tau particularly binding in this case. We set the total number of iterations in each case to 20000 and discard the first half. The parameter nb is used to set how much back data should appear in a fan chart.\n\n#########\n# Options\n#########\nnf <- 12 # Max forecast horizon\nnb <- 21 # No. back periods plotted in graphs\nl  <- 2  # Number of lags in VAR\n\n# specify parameters of the Minnesota-type prior\ntau    <- .1   # controls prior on own 1st lags (1 makes wibbly)\nd      <- 1    # decay for higher lags\nlambda <- 1    # prior for the constant\ngamma  <- 1    # sum of coefficients unit roots\ndelta  <- 1    # cointegration prior\n\n# Gibbs control\nreps <- 20000 # total numbers of Gibbs iterations\nburn <- 10000 # number of burn-in iterations\n\nIn what follows we vary tau and the lag length to illustrate their effects. To do this we create the augmented data and then run the Gibbs sampler, using:\n\n# Create augmented data\nYplus <- augmentData(Y, l, tau, d, lambda, gamma, delta)\n\n# Run Gibbs sampler\nout   <- Gibbs_estimate(Yplus[[1]], Yplus[[2]], reps, burn, 1, nf)\n\nwhere Y contains the data in a dataframe/tibble with the date in the first column as in the data example above. The code strips out the date and then uses the remaining \\(N\\) columns in the BVAR. See the Code Appendix for the details of the functions.\n\n\n\nThe output contains any forecast draws from the Gibbs sampler in the third list element from the Gibbs_estimate() function. The first two elements are coefficient draws. Two further functions plots the fan charts using the Gibbs draws:\n\n# String to put in subtitle\ncontrols <- paste0(\"Lag length \", l, \": tau=\", tau, \", d=\", d,\n                   \", lambda=\", lambda, \", gamma=\", gamma, \", delta=\", delta)\n\n# Plots\nfan_chart(Y, out[[3]], controls, nb)\np           <- coeff_plot(Y, l, out[[1]], out[[2]], 333, controls)\npnum        <- pnum+1\npce[[pnum]] <- p[[1]]\n\nwhere the string controls is put in the chart subtitle and the coefficient densities. It can be anything but is a good place to remind yourself of how you specified the model. Notice we save the coefficient plots for later use.\n\n3.4.0.1 Example 1: BVAR(2) with \\(\\tau=0.1\\)\n\n\n\n\n\n\n\n\n\n\n\n3.4.0.2 Example 2: BVAR(2) with \\(\\tau=1\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4.0.3 Example 3: BVAR(6) with \\(\\tau=0.1\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4.0.4 Example 4: BVAR(6) with \\(\\tau=1\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4.0.5 Coefficient estimates\nAll of these have underlying parameters. Their estimated posterior densities are:"
  },
  {
    "objectID": "BVAR.html#tri-variate-bvar",
    "href": "BVAR.html#tri-variate-bvar",
    "title": "3  BVAR with dummies",
    "section": "3.5 Tri-variate BVAR",
    "text": "3.5 Tri-variate BVAR\nNow we add the FedFunds rate (FRED series FEDFUNDS), so the last ten periods of the data set is now:\n\n\n\n\n\nDate\nGrowth\nInflation\nFedFunds\n\n\n\n\n2020-10-01\n-1.5\n1.224176\n0.09\n\n\n2021-01-01\n1.2\n1.905310\n0.09\n\n\n2021-04-01\n12.5\n4.776278\n0.07\n\n\n2021-07-01\n5.0\n5.264633\n0.10\n\n\n2021-10-01\n5.7\n6.765892\n0.08\n\n\n2022-01-01\n3.7\n8.023109\n0.08\n\n\n2022-04-01\n1.8\n8.556077\n0.33\n\n\n2022-07-01\n1.9\n8.284860\n1.68\n\n\n2022-10-01\n0.9\n7.110821\n3.08\n\n\n2023-01-01\n1.6\n5.769521\n4.33\n\n\n\n\n\nTwo more examples follow.\n\n3.5.0.1 Example 5: BVAR(4) with \\(\\tau=.1\\), 3 variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.0.2 Example 6: BVAR(6) with \\(\\tau=1\\), 3 variables"
  },
  {
    "objectID": "BVAR.html#code-appendix",
    "href": "BVAR.html#code-appendix",
    "title": "3  BVAR with dummies",
    "section": "3.6 Code appendix",
    "text": "3.6 Code appendix\nYou can download the program and functions used for the estimates above from the links below. Put them in the same directory and they should recreate exactly (within sampling error) the same graphs as above. Ensure you have all the libraries available that are loaded at the top of BVARdum.R.\nMain program:\n\n\nDownload BVARdum.R\n\n\nFunctions:\n\n\nDownload BVARdumFUNCs.R\n\n\n\n\n\n\nBanbura, M., D. Giannone, and L. Reichlin. 2010. “Large Bayesian Vector Auto Regressions.” Journal of Applied Econometrics 25 (1): 71–92.\n\n\nGiannone, Domenico, Michele Lenza, and Giorgio E. Primiceri. 2015. “Prior Selection for Vector Autoregressions.” The Review of Economics and Statistics 97 (2): 436–51.\n\n\nTheil, Henri, and Arthur S. Goldberger. 1961. “On Pure and Mixed Statistical Estimation in Economics.” International Economic Review 2: 317–32."
  },
  {
    "objectID": "BK.html",
    "href": "BK.html",
    "title": "3  Linear rational expectations models",
    "section": "",
    "text": "4 Introduction\nHow do we solve rational expectations models? Here I show how to implement versions of the Blanchard and Kahn (1980) and Klein (2000) solutions to linear rational expectations models in R. The implementation is fairly general, and copes with singular models. It is a very transparent implementation, with all the necessary code, and also shows how to calculate and plot impulse responses."
  },
  {
    "objectID": "BK.html#coding-up-the-model-in-r",
    "href": "BK.html#coding-up-the-model-in-r",
    "title": "3  Linear rational expectations models",
    "section": "5.1 Coding up the model in R",
    "text": "5.1 Coding up the model in R\nBefore we begin coding this in R, load the tidyverse libraries so we can do impulse responses with our usual toolkit and then we can forget about it.\n\nlibrary(tidyverse)\n\nSet the model parameters\n\nnf    <- 2\nns    <- 5\nne    <- 3\nnp    <- ns-nf\n\nbeta  <- 0.99   # Discount factor \nsigma <- 2.0    # Elas. substitution\nkappa <- 0.075  # Slope PC\ndelta <- 1.5    # Inflation feedback\ngamma <- 0.75   # Smoothing\nrho_1 <- 0.9    # AR1\nrho_2 <- 0.8    # AR1\nOmega <- diag(c(0.33,0.33,0.33)) # SE of 3 shocks\n\nNow define the model matrices ‘long hand’ and some variable names, which we put in labels.\n\nlabels <- c(\"e^1\",\"e^2\",\"i\",\"y\",\"pi\")\n\nE <- matrix(0,ns,ns)\nA <- matrix(0,ns,ns)\nB <- diag(1,ns,ne)\n\n# Now put the equations in matrix form\ndiag(E[1:2,1:2]) <- 1\ndiag(A[1:2,1:2]) <- c(rho_1, rho_2)\n\nE[3,3]             <- 1 \nE[4,c(1, 3, 4, 5)] <- c(1, -1/sigma, 1, 1/sigma)\nE[5,c(2, 5)]       <- c(1, beta)\n\nA[3,c(3, 5)]       <- c(gamma, (1-gamma)*delta)\nA[4,4]             <- 1\nA[5,c(4,5)]        <- c(-kappa, 1)\n\nwhere for example, \\(E\\) and \\(A\\) are\n\n\n\n\nE\n\n  \n    1 \n    0 \n    0.0 \n    0 \n    0.00 \n  \n  \n    0 \n    1 \n    0.0 \n    0 \n    0.00 \n  \n  \n    0 \n    0 \n    1.0 \n    0 \n    0.00 \n  \n  \n    1 \n    0 \n    -0.5 \n    1 \n    0.50 \n  \n  \n    0 \n    1 \n    0.0 \n    0 \n    0.99 \n  \n\n\n\n\n\n\n\nA\n\n  \n    0.9 \n    0.0 \n    0.00 \n    0.000 \n    0.000 \n  \n  \n    0.0 \n    0.8 \n    0.00 \n    0.000 \n    0.000 \n  \n  \n    0.0 \n    0.0 \n    0.75 \n    0.000 \n    0.375 \n  \n  \n    0.0 \n    0.0 \n    0.00 \n    1.000 \n    0.000 \n  \n  \n    0.0 \n    0.0 \n    0.00 \n    -0.075 \n    1.000 \n  \n\n\n\n\n\nCalculate the reduced form state-space model \\[\n\\begin{bmatrix} z_t \\\\ x_{t+1}^e \\end{bmatrix} = C \\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix} + D \\varepsilon_t  \n\\] which is done in R very simply as\n\nC <- solve(E,A)\nD <- solve(E,B)\n\nWhy can’t we solve this for impulse responses?\nThe following function simulates the impulse responses of a model in a loop within a loop1 and returns the time series in a suitably organised data frame.\n\nimpulse_responses <- function(P, Q, Omega, labels, T) {\n  s   <- matrix(0, ncol(Q), 1)\n  z   <- matrix(0, nrow(Q), T)\n  rownames(z) <- labels\n  dza <- NULL\n  for (j in 1:ncol(Q)) {\n    s[j]  <- Omega[j,j]\n    z[,1] <- Q %*% s\n    for (i in 1:(T-1)) {\n      z[,i+1] <- P %*% z[,i]\n    }\n    s[j] <- 0\n    dz <- as_tibble(t(z)) %>% \n      mutate(Period = 1:T, Shock = paste0(\"epsilon^\",j))\n    dza <- bind_rows(dza,dz)\n  }\n  return(dza)\n}\n\nA function to plot the impulses will be useful, so we create one.\n\nresponse_plot <- function(series, title) {\n  return(pivot_longer(series, cols = -c(Period,Shock), names_to=\"Var\", values_to = \"Val\") %>%\n           ggplot() +\n           geom_line(aes(x=Period, y=Val, group=Shock, colour=Var), show.legend=FALSE) +\n           facet_grid(Shock~Var, scales=\"free\", labeller=label_parsed) +\n           scale_x_continuous(expand=c(0,0)) +\n           theme_minimal() +\n           labs(title=title, x=\"\",y=\"\"))\n}\n\nCall the impulse response function using the model \\(C\\) and \\(D\\).\n\nT <- 25\nz <- impulse_responses(C, D, Omega, labels, T)\n\nand plot\n\nresponse_plot(z, \"Impulse responses: Taylor rule\")\n\n\n\n\n\n\n\n\nOh! That’s not looking good. Let’s try a few more periods.\n\nT <- 150\nz <- impulse_responses(C, D, Omega, labels, T)\nresponse_plot(z, \"Impulse responses: Taylor rule\")\n\n\n\n\n\n\n\n\nThis is clearly exploding. But it’s rational – we’re solving forward so expectations are always fulfilled. This is a key the insight of the early rational expectations modelers – rational isn’t enough, non-explosive is necessary too. Fortunately we know how to find this."
  },
  {
    "objectID": "BK.html#impulse-responses",
    "href": "BK.html#impulse-responses",
    "title": "3  Linear rational expectations models",
    "section": "6.1 Impulse responses",
    "text": "6.1 Impulse responses\nWe now call the impulse response function using the model solved for rational expectations.\n\nT <- 25\nz <- impulse_responses(P, Q, Omega, labels, T)\n\nNow plot these responses\n\nresponse_plot(z, \"Impulse responses: Taylor rule\")\n\n\n\n\n\n\n\n\nNow, that looks better! It is no longer explosive. It also makes complete economic sense, which you can verify by going through the dynamics of the different demand, supply and monetary shocks."
  },
  {
    "objectID": "BK.html#singular-models-optimal-policy",
    "href": "BK.html#singular-models-optimal-policy",
    "title": "3  Linear rational expectations models",
    "section": "3.5 Singular models: optimal policy",
    "text": "3.5 Singular models: optimal policy\nHowever, this is an easy test. What we need is to use a model that can’t be solved using the BK method. Under optimal policy, the interest rate instrument rule is replaced with a targeting rule, so that \\[\n  \\pi_t = -\\mu \\Delta y_t - \\varepsilon^3_t\n\\] for some value of \\(\\mu\\) that reflects the optimal trade-off between output (gap) growth and inflation, and we’ve included a disturbance which we can loosely describe as a monetary policy shock. We modify the model above by dropping the Taylor rule in favor of the targeting rule. This requires a lagged value of \\(y\\) to be created. The following does the trick\n\nnf <- 2\nne <- 3\nns <- 6      # One extra state\nnp <- ns-nf\nmu <- 0.75   # Representative trade-off\n\nlabels <- c(\"e^1\",\"e^2\",\"ylag\",\"i\",\"y\",\"pi\") # New variable order\n\nE <- matrix(0,ns,ns)\nA <- E\nB <- matrix(0,ns,ne)\nB[1,1] <- 1\nB[2,2] <- 1\nB[4,3] <- -1\n\ndiag(E[1:3,1:3]) <- 1\ndiag(A[1:2,1:2]) <- c(rho_1, rho_2)\nA[3,5]           <- 1\n\nE[4,3]           <- 1\nA[4,c(3, 6)]     <- c(1, -1/mu)\n\nE[5,c(1, 4, 5, 6)] <- c(1, -1/sigma, 1, 1/sigma)\nA[5,5]           <- 1\n\nE[6,c(2, 6)]     <- c(1, beta)\nA[6,c(5, 6)]     <- c(-kappa, 1)\n\nThe new \\(E\\) and \\(A\\) system matrices are then\n\n\n\n\nE\n\n  \n    1 \n    0 \n    0 \n    0.0 \n    0 \n    0.00 \n  \n  \n    0 \n    1 \n    0 \n    0.0 \n    0 \n    0.00 \n  \n  \n    0 \n    0 \n    1 \n    0.0 \n    0 \n    0.00 \n  \n  \n    0 \n    0 \n    1 \n    0.0 \n    0 \n    0.00 \n  \n  \n    1 \n    0 \n    0 \n    -0.5 \n    1 \n    0.50 \n  \n  \n    0 \n    1 \n    0 \n    0.0 \n    0 \n    0.99 \n  \n\n\n\n\n\n\n\nA\n\n  \n    0.9 \n    0.0 \n    0 \n    0 \n    0.000 \n    0.000 \n  \n  \n    0.0 \n    0.8 \n    0 \n    0 \n    0.000 \n    0.000 \n  \n  \n    0.0 \n    0.0 \n    0 \n    0 \n    1.000 \n    0.000 \n  \n  \n    0.0 \n    0.0 \n    1 \n    0 \n    0.000 \n    -1.333 \n  \n  \n    0.0 \n    0.0 \n    0 \n    0 \n    1.000 \n    0.000 \n  \n  \n    0.0 \n    0.0 \n    0 \n    0 \n    -0.075 \n    1.000 \n  \n\n\n\n\n\nNow we have a singular model. The matrix \\(E\\) is clearly singular as rows 3 and 4 are identical. But we have a problem using the code above. To use it we need the matrices \\(H\\) and \\((A_{22} - W A_{21})\\) to be non-singular. What to do?\nThere are two ways out. Klein (2000) gives a solution that depends on the decomposed matrix pencil, which is what is typically implemented, but you don’t actually need it although it is easiest. Instead, all you need to do is reorder the equations.\nThe real problem is that with a targeting rule that doesn’t include the interest rate, and the interest rate is now only determined by the IS curve. But we can swap the location of any two rows of the model arbitrarily. If we swap the positions of the equations for the IS curve and the targeting rule (rows 4 and 5) using the following\n\nE[4:5,] <- E[5:4,]\nA[4:5,] <- A[5:4,]\nB[4:5,] <- B[5:4,]\n\nthen the model is unchanged but now we have\n\nE[1:4,1:4]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0  0.0\n[2,]    0    1    0  0.0\n[3,]    0    0    1  0.0\n[4,]    1    0    0 -0.5\n\n\nso \\(E_{11} + E_{12}N\\) is likely non-singular (it is). Also, note after the re-ordering \\(A_{22}\\) is\n\nA[5:6,5:6]\n\n       [,1]      [,2]\n[1,]  0.000 -1.333333\n[2,] -0.075  1.000000\n\n\nwhich is guaranteed non-singular for zero \\(W\\). We can now proceed as before. First, check for saddle path stability\n\ne <- geigen::gevalues(geigen::gqz(A, E, sort=\"S\"))\ne[abs(e) > 1]\n\n[1] 1.378195      Inf\n\n\nwhich confirms that it has a unique saddle path stable solution. This is\n\nSo <- solveGenBK(E,A,B,np)\n\n[1] \"Number of unstable roots is 2\"\n\nPo <- So$P\nQo <- So$Q\n\nThe solved model is then\n\nPo\n\n     [,1]      [,2]          [,3] [,4] [,5] [,6]\n[1,]  0.9  0.000000  0.000000e+00    0    0    0\n[2,]  0.0  0.800000  6.986592e-17    0    0    0\n[3,]  0.0 -1.863455  7.329156e-01    0    0    0\n[4,]  1.8 -1.241330 -2.446879e-01    0    0    0\n[5,]  0.0 -1.863455  7.329156e-01    0    0    0\n[6,]  0.0  1.397591  2.003133e-01    0    0    0\n\nQo\n\n     [,1]      [,2]          [,3]\n[1,]    1  0.000000  0.000000e+00\n[2,]    0  1.000000 -6.986592e-17\n[3,]    0 -2.329318 -7.329156e-01\n[4,]    2 -1.551663  2.446879e-01\n[5,]    0 -2.329318 -7.329156e-01\n[6,]    0  1.746989 -2.003133e-01\n\n\n\n3.5.1 Optimal impulse responses\nWe can now simulate the model under optimal policy and plot using\n\nzo <- impulse_responses(Po, Qo, Omega, labels, T) %>%\n  select(-ylag) # Drop duplicate series\nresponse_plot(zo, \"Impulse responses: Optimal policy\")"
  },
  {
    "objectID": "BK.html#dummy-jumps",
    "href": "BK.html#dummy-jumps",
    "title": "3  Linear rational expectations models",
    "section": "3.6 Dummy jumps",
    "text": "3.6 Dummy jumps\nBut this isn’t the only way to get this to work. Effectively what we just did was create an extra predetermined variable and reorder the system to give us non-singularity. What if instead of including an unused \\(i_{t-1}\\) on the right hand side, we instead include an unused \\(i^e_{t+1}\\) on the left hand side? So we swap to having one more jump variable, one less predetermined one?\nCompare the following to the previous model. When we pick out the interest rate we do so on the right hand side of the matrix equation, not the left as before.\n\nns <- 6      # One extra state\nnf <- 3      # And one extra jump\nnp <- ns-nf\nlabels <- c(\"e^1\",\"e^2\",\"ylag\",\"i\",\"y\",\"pi\") # New variable order\n\nE <- matrix(0,ns,ns)\nA <- E\nB <- matrix(0,ns,ne)\nB[1,1] <- 1\nB[2,2] <- 1\nB[4,3] <- -1\n\ndiag(E[1:3,1:3]) <- 1\ndiag(A[1:2,1:2]) <- c(rho_1, rho_2)\nA[3,5]           <- 1\n\nE[4,3]           <- 1\nA[4,c(3, 6)]     <- c(1, -1/mu)\n\nE[5,c(1, 5, 6)]  <- c(1, 1, 1/sigma) # One less coeffiecint\nA[5,c(4, 5)]     <- c(1/sigma, 1)    # One more - nothing else changes\n\nE[6,c(2, 6)]     <- c(1, beta)\nA[6,c(5, 6)]     <- c(-kappa, 1)\n\nThis is still a singular model, as we can see from\n\nE\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    0    0    0    0 0.00\n[2,]    0    1    0    0    0 0.00\n[3,]    0    0    1    0    0 0.00\n[4,]    0    0    1    0    0 0.00\n[5,]    1    0    0    0    1 0.50\n[6,]    0    1    0    0    0 0.99\n\n\nwith column 4 all zeros. Is this model saddle path stable?\n\ne <- geigen::gevalues(geigen::gqz(A, E, sort=\"S\") )\ne[abs(e) > 1]\n\n[1]     -Inf 1.378195      Inf\n\n\nAgain, it is with an extra unstable root for the extra jump variable. We could simplify the solution. As that top left 3 by 3 block, \\(E_{11}\\), is the identity matrix and \\(E_{12}\\) is all zeros this Ei is always an identity matrix. However, here we simply re-use solveGenBG\n\nSo2 <- solveGenBK(E,A,B,np)\n\n[1] \"Number of unstable roots is 3\"\n\nPo2 <- So2$P\nQo2 <- So2$Q\n\nNow the solved model is\n\nPo2\n\n     [,1]      [,2]       [,3] [,4] [,5] [,6]\n[1,]  0.9  0.000000  0.0000000    0    0    0\n[2,]  0.0  0.800000  0.0000000    0    0    0\n[3,]  0.0 -1.863455  0.7329156    0    0    0\n[4,]  1.8 -1.241330 -0.2446879    0    0    0\n[5,]  0.0 -1.863455  0.7329156    0    0    0\n[6,]  0.0  1.397591  0.2003133    0    0    0\n\nQo2\n\n     [,1]      [,2]       [,3]\n[1,]    1  0.000000  0.0000000\n[2,]    0  1.000000  0.0000000\n[3,]    0 -2.329318 -0.7329156\n[4,]    2 -1.551663  0.2446879\n[5,]    0 -2.329318 -0.7329156\n[6,]    0  1.746989 -0.2003133\n\n\nwhich is actually identical to our previous solution. This is because I have preserved the order of the solved-out variables, and shows that the swap from a predetermined to a jump variable is completely arbitrary."
  },
  {
    "objectID": "BK.html#substituting-out",
    "href": "BK.html#substituting-out",
    "title": "3  Linear rational expectations models",
    "section": "3.7 Substituting out",
    "text": "3.7 Substituting out\nBut even this doesn’t exhaust the possible re-parametrisations of the model. We can reduce the number of jump variables to 1 and find the same solution. There exist formal methods for reducing models (see King and Watson (2002)) but there is an obvious way to proceed here. From the targeting rule, it must be that \\[\n  y^e_{t+1} = y_t - \\frac{1}{\\mu}\\pi^e_{t+1}\n\\] as the expected shock is zero. This means the IS curve can be rewritten \\[\ny_t = y_t - \\frac{1}{\\mu}\\pi^e_{t+1} - \\frac{1}{\\sigma} \\left (i_t - \\pi_{t+1}^e \\right ) + e_t^1\n\\] implying \\[\ni_t =  \\left (1 - \\frac{\\sigma}{\\mu} \\right )\\pi_{t+1}^e + \\sigma e_t^1\n\\] This is the required interest rate consistent with the targeting rule holding. Now the only jump variable is the inflation rate as we have eliminated the expected output gap. \\[\n\\begin{aligned}\ny_t    &= y_{t-1} -\\frac{1}{\\mu} \\pi_t  + \\frac{1}{\\mu} \\varepsilon^3_t \\\\\n\\pi_t  &= \\beta \\pi_{t+1}^e + \\kappa y_t + e_t^2 \\\\\ni_t    &= \\left (1 - \\frac{\\sigma}{\\mu} \\right ) \\pi_{t+1}^e + \\sigma e_t^1 \\\\\ne_t^1  &= \\rho_1 e_{t-1}^1 + \\varepsilon_t^1 \\\\\ne_t^2  &= \\rho_2 e_{t-1}^2 + \\varepsilon_t^2\n\\end{aligned}\n\\]\nWe can code this\n\nns <- 5      # Back to 5 states\nnf <- 1      # Now only one jump\nnp <- ns-nf\n\nlabels <- c(\"e^1\",\"e^2\",\"i\",\"y\",\"pi\") # Lose a y\n\nE <- matrix(0,ns,ns)\nA <- E\nB <- matrix(0,ns,ne)\nB[1,1] <- 1\nB[2,2] <- 1\nB[4,3] <- -1\n\ndiag(E[1:4,1:4]) <- 1\ndiag(A[1:2,1:2]) <- c(rho_1, rho_2)\n\nE[3,c(1, 3, 5)]  <- c(-sigma, 1, sigma/mu-1)\n\nA[4,c(4,5)]      <- c(1, -1/mu)\n\nE[5,c(2, 4, 5)]  <- c(1, kappa, beta)\nA[5,5]           <- 1\n\nand solve it using\n\nSs <- solveGenBK(E,A,B,np)\n\n[1] \"Number of unstable roots is 1\"\n\nPs <- Ss$P\nQs <- Ss$Q\n\nCompare the realized of Ps\n\nPs\n\n     [,1]      [,2] [,3]       [,4] [,5]\n[1,]  0.9  0.000000    0  0.0000000    0\n[2,]  0.0  0.800000    0  0.0000000    0\n[3,]  1.8 -1.241330    0 -0.2446879    0\n[4,]  0.0 -1.863455    0  0.7329156    0\n[5,]  0.0  1.397591    0  0.2003133    0\n\n\nwith Po above, say. This is the most ‘efficient’ way of programming the model, in that we have only five states, and indeed the repeated behavioral equations we had before have disappeared in the reduced form solution. Just to confirm this, simulating and plotting this version gives\n\nresponse_plot(impulse_responses(Ps,Qs,Omega,labels,T), \"Optimal, substituted out\")\n\n\n\n\n\n\n\n\nwhich are identical results to those above. But of course \\(E\\) is now convertible so we could solve this using the simplest Blanchard-Kahn variant.\n\n\n\n\nBlake, Andrew P. 2004. “Analytic Derivatives in Linear Rational Expectations Models.” Computational Economics 24 (1): 77–96.\n\n\nBlanchard, O., and C. Kahn. 1980. “The Solution of Linear Difference Models Under Rational Expectations.” Econometrica 48 (5): 1305–11.\n\n\nKing, Robert G., and Mark W. Watson. 2002. “System Reduction and Solution Algorithms for Singular Linear Difference Systems Under Rational Expectations.” Computational Economics 20 (1–2): 57–86.\n\n\nKlein, Paul. 2000. “Using the Generalized Schur Form to Solve a Multivariate Linear Rational Expectations Model.” Journal of Economic Dynamics and Control 24 (10): 1405–23.\n\n\nPappas, T., A. J. Laub, and N. R. Sandell. 1980. “On the Numerical Solution of the Discrete-Time Algebraic Riccati Equation.” IEEE Transaction on Automatic Control AC-25.4: 631–41.\n\n\nVaughan, D. R. 1970. “A Non Recursive Algorithm Solution for the Discrete Riccati Equation.” IEEE Transactions on Automatic Control AC-15.5: 597–99."
  },
  {
    "objectID": "BK.html#model",
    "href": "BK.html#model",
    "title": "3  Linear rational expectations models",
    "section": "3.2 Model",
    "text": "3.2 Model\nWe take a simple New Keynesian model \\[\n\\begin{align}\ny_t    &= y_{t+1}^e-\\frac{1}{\\sigma} (i_t - \\pi_{t+1}^e) + e_t^1 \\\\\n\\pi_t  &= \\beta \\pi_{t+1}^e + \\kappa y_t + e_t^2 \\\\\ni_t    &= \\gamma i_{t-1} + (1-\\gamma) \\delta \\pi_t + \\varepsilon_t^3 \\\\\ne_t^1  &= \\rho_1 e_{t-1}^1 + \\varepsilon_t^1 \\\\\ne_t^2  &= \\rho_2 e_{t-1}^2 + \\varepsilon_t^2\n\\end{align}\n\\] The model comprises a dynamic IS curve, a Phillips Curve and a policy rule with smoothing. There are three shocks, two of which are persistent. This we need to write in the general algebraic linear state-space form: \\[\nE\\begin{bmatrix} z_t \\\\ x_{t+1}^e \\end{bmatrix} = A \\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix} + B \\varepsilon_t  \n\\] We map our variables to their algebraic equivalent as (\\(z_t\\), \\(x_t\\)) \\(=\\) ((\\(e^1_t\\), \\(e^2_t\\), \\(i_t\\)), (\\(y_t\\), \\(\\pi_t\\))). Then the model in state-space form but including the matrix \\(E\\) is \\[\n\\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\\\\n                0 & 1 & 0 & 0 & 0 \\\\\n                0 & 0 & 1 & 0 & 0 \\\\\n                1 & 0 & -\\frac{1}{\\sigma} & 1 & \\frac{1}{\\sigma} \\\\\n                0 & 1 & 0 & 0 & \\beta\n\\end{bmatrix}\n\\begin{bmatrix} e^1_t \\\\ e^2_t \\\\ i_t \\\\ y^e_{t+1} \\\\ \\pi^e_{t+1} \\end{bmatrix}\n   =\n   \\begin{bmatrix} \\rho_1 & 0 & 0 & 0 & 0 \\\\\n                0 & \\rho_2 & 0 & 0 & 0 \\\\\n                0 & 0 & \\gamma & 0 & (1-\\gamma)\\delta \\\\\n                0 & 0 & 0 & 1 & 0 \\\\\n                0 & 0 & 0 & -\\kappa & 1\n   \\end{bmatrix}\n\\begin{bmatrix} e^1_{t-1} \\\\ e^2_{t-1} \\\\ i_{t-1} \\\\ y_t \\\\ \\pi_t \\end{bmatrix}    \n   +\n      \\begin{bmatrix}\n                1 & 0 & 0  \\\\\n                0 & 1 & 0 \\\\\n                0 & 0 & 1 \\\\\n                0 & 0 & 0 \\\\\n                0 & 0 & 0\n   \\end{bmatrix}\n   \\begin{bmatrix} \\varepsilon^1_t \\\\ \\varepsilon^2_t \\\\ \\varepsilon^3_t \\end{bmatrix}    \n\\] Anyone wanting to code up solutions should familiarize themselves with this before continuing.\n\n3.2.1 Coding up the model in R\nBefore we begin coding this in R, load the tidyverse libraries so we can do impulse responses with our usual toolkit and then we can forget about it.\n\nlibrary(tidyverse)\n\nSet the model parameters\n\nnf    <- 2\nns    <- 5\nne    <- 3\nnp    <- ns-nf\n\nbeta  <- 0.99   # Discount factor \nsigma <- 2.0    # Elas. substitution\nkappa <- 0.075  # Slope PC\ndelta <- 1.5    # Inflation feedback\ngamma <- 0.75   # Smoothing\nrho_1 <- 0.9    # AR1\nrho_2 <- 0.8    # AR1\nOmega <- diag(c(0.33,0.33,0.33)) # SE of 3 shocks\n\nNow define the model matrices ‘long hand’ and some variable names, which we put in labels.\n\nlabels <- c(\"e^1\",\"e^2\",\"i\",\"y\",\"pi\")\n\nE <- matrix(0,ns,ns)\nA <- matrix(0,ns,ns)\nB <- diag(1,ns,ne)\n\n# Now put the equations in matrix form\ndiag(E[1:2,1:2]) <- 1\ndiag(A[1:2,1:2]) <- c(rho_1, rho_2)\n\nE[3,3]             <- 1 \nE[4,c(1, 3, 4, 5)] <- c(1, -1/sigma, 1, 1/sigma)\nE[5,c(2, 5)]       <- c(1, beta)\n\nA[3,c(3, 5)]       <- c(gamma, (1-gamma)*delta)\nA[4,4]             <- 1\nA[5,c(4,5)]        <- c(-kappa, 1)\n\nwhere for example, \\(E\\) and \\(A\\) are\n\n\n\n\nE\n\n  \n    1 \n    0 \n    0.0 \n    0 \n    0.00 \n  \n  \n    0 \n    1 \n    0.0 \n    0 \n    0.00 \n  \n  \n    0 \n    0 \n    1.0 \n    0 \n    0.00 \n  \n  \n    1 \n    0 \n    -0.5 \n    1 \n    0.50 \n  \n  \n    0 \n    1 \n    0.0 \n    0 \n    0.99 \n  \n\n\n\n\n\n\n\nA\n\n  \n    0.9 \n    0.0 \n    0.00 \n    0.000 \n    0.000 \n  \n  \n    0.0 \n    0.8 \n    0.00 \n    0.000 \n    0.000 \n  \n  \n    0.0 \n    0.0 \n    0.75 \n    0.000 \n    0.375 \n  \n  \n    0.0 \n    0.0 \n    0.00 \n    1.000 \n    0.000 \n  \n  \n    0.0 \n    0.0 \n    0.00 \n    -0.075 \n    1.000 \n  \n\n\n\n\n\nCalculate the reduced form state-space model \\[\n\\begin{bmatrix} z_t \\\\ x_{t+1}^e \\end{bmatrix} = C \\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix} + D \\varepsilon_t  \n\\] which is done in R very simply as\n\nC <- solve(E,A)\nD <- solve(E,B)\n\nWhy can’t we solve this for impulse responses?\nThe following function simulates the impulse responses of a model in a loop within a loop1 and returns the time series in a suitably organised data frame.\n\nimpulse_responses <- function(P, Q, Omega, labels, T) {\n  s   <- matrix(0, ncol(Q), 1)\n  z   <- matrix(0, nrow(Q), T)\n  rownames(z) <- labels\n  dza <- NULL\n  for (j in 1:ncol(Q)) {\n    s[j]  <- Omega[j,j]\n    z[,1] <- Q %*% s\n    for (i in 1:(T-1)) {\n      z[,i+1] <- P %*% z[,i]\n    }\n    s[j] <- 0\n    dz <- as_tibble(t(z)) %>% \n      mutate(Period = 1:T, Shock = paste0(\"epsilon^\",j))\n    dza <- bind_rows(dza,dz)\n  }\n  return(dza)\n}\n\nA function to plot the impulses will be useful, so we create one.\n\nresponse_plot <- function(series, title) {\n  return(pivot_longer(series, cols = -c(Period,Shock), names_to=\"Var\", values_to = \"Val\") %>%\n           ggplot() +\n           geom_line(aes(x=Period, y=Val, group=Shock, colour=Var), show.legend=FALSE) +\n           facet_grid(Shock~Var, scales=\"free\", labeller=label_parsed) +\n           scale_x_continuous(expand=c(0,0)) +\n           theme_minimal() +\n           labs(title=title, x=\"\",y=\"\"))\n}\n\nCall the impulse response function using the model \\(C\\) and \\(D\\).\n\nT <- 25\nz <- impulse_responses(C, D, Omega, labels, T)\n\nand plot\n\nresponse_plot(z, \"Impulse responses: Taylor rule\")\n\n\n\n\n\n\n\n\nOh! That’s not looking good. Let’s try a few more periods.\n\nT <- 150\nz <- impulse_responses(C, D, Omega, labels, T)\nresponse_plot(z, \"Impulse responses: Taylor rule\")\n\n\n\n\n\n\n\n\nThis is clearly exploding. But it’s rational – we’re solving forward so expectations are always fulfilled. This is a key the insight of the early rational expectations modelers – rational isn’t enough, non-explosive is necessary too. Fortunately we know how to find this."
  },
  {
    "objectID": "BK.html#bk80",
    "href": "BK.html#bk80",
    "title": "3  Linear rational expectations models",
    "section": "3.3 Blanchard and Kahn (1980)",
    "text": "3.3 Blanchard and Kahn (1980)\nTo solve this model to give a unique stable rational expectations equilibrium, we appeal to the following. Consider the eigenvalue decomposition \\[\n  MC=\\Lambda M\n\\] where \\(\\Lambda\\) is a diagonal matrix of eigenvalues in increasing absolute value and \\(M\\) is a non-singular matrix of left eigenvectors. Note that computer routines (including the one in R) usually calculate right eigenvectors such that \\(CV=V\\Lambda\\) and that \\(M=V^{-1}\\), so be aware of this in what follows.\nWe can diagonalise \\(C\\) and write it as \\(C=M^{-1}\\Lambda M\\). So pre-multiplying the reduced form model by \\(M\\) gives \\[\nM \\begin{bmatrix} z_t \\\\ x_{t+1}^e \\end{bmatrix} = \\Lambda M \\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix} + M D \\varepsilon_t\n\\] Blanchard and Kahn (1980) (following Vaughan (1970)) show uniqueness requires as many unstable eigenvalues as jump variables. To see this, define \\[\n\\begin{bmatrix} \\xi_{t-1}^{s} \\\\  \\xi_t^{u} \\end{bmatrix}\n  =  \\begin{bmatrix} M_{11} & M_{12} \\\\  M_{21} & M_{22} \\end{bmatrix}\n      \\begin{bmatrix} z_{t-1} \\\\  x_t \\end{bmatrix}\n\\] Write the normalized model as \\[\n\\begin{bmatrix} \\xi_t^s \\\\ \\xi_{t+1}^u \\end{bmatrix}\n= \\begin{bmatrix} \\Lambda_s & 0 \\\\ 0 & \\Lambda_u \\end{bmatrix}\n\\begin{bmatrix} \\xi_{t-1}^s \\\\ \\xi_t^u \\end{bmatrix} +\n\\begin{bmatrix} M_1 \\\\ M_2 \\end{bmatrix} D\\varepsilon_t\n\\] where the eigenvalues are split into stable (\\(\\Lambda_s\\)) and unstable (\\(\\Lambda_u\\)). If we ignore the stochastic bit for a moment \\[\n\\begin{bmatrix} \\xi_t^s \\\\ \\xi_{t+1}^u \\end{bmatrix}\n= \\begin{bmatrix} \\Lambda_s & 0 \\\\ 0 & \\Lambda_u \\end{bmatrix}\n\\begin{bmatrix} \\xi_{t-1}^s \\\\ \\xi_t^u \\end{bmatrix}\n\\]\nWe seek a non-explosive solution, and this turns out to be easy to find using the following\n\nThe dynamics of \\(\\xi_t^u\\) are determined by \\(\\Lambda_u\\) and nothing else;\nIf they don’t start at \\(0\\) they must explode;\nThis implies they must start at \\(0\\) and are always \\(0\\).\n\nThus the definition of the canonical variables necessarily implies \\[\n\\begin{bmatrix} \\xi_{t-1}^s \\\\  0 \\end{bmatrix}\n  =  \\begin{bmatrix} M_{11} & M_{12} \\\\  M_{21} & M_{22} \\end{bmatrix}\n      \\begin{bmatrix} z_{t-1} \\\\  x_t \\end{bmatrix}\n\\]\nFrom this it is clear that the jump variables themselves are only on the saddle path if \\[\n   M_{21} z_{t-1} + M_{22} x_t = 0\n\\]\nThe rational solution implies that the jump variables are linearly related to the predetermined ones through \\[\n\\begin{aligned}\nx_t &= -M_{22}^{-1} M_{21}z_{t-1} \\\\\n    &= N z_{t-1}\n\\end{aligned}\n\\] We’ll deal with the shocks in a moment.\n\n3.3.1 R code\nHow do we do this in R? First, find the eigenvalue decomposition of \\(C\\) using\n\nm <- eigen(C, symmetric=FALSE)\n\nwhich yields\n\n\neigen() decomposition\n$values\n[1] 1.0715518+0.092734i 1.0715518-0.092734i 0.9000000+0.000000i\n[4] 0.8000000+0.000000i 0.6548762+0.000000i\n\n$vectors\n                      [,1]                  [,2]         [,3]           [,4]\n[1,]  0.0000000+0.0000000i  0.0000000+0.0000000i 0.2854942+0i  0.00000000+0i\n[2,]  0.0000000+0.0000000i  0.0000000+0.0000000i 0.0000000+0i  0.09783896+0i\n[3,]  0.1599159-0.5089425i  0.1599159+0.5089425i 0.7830500+0i  0.49622464+0i\n[4,] -0.6991064+0.0000000i -0.6991064+0.0000000i 0.4552131+0i -0.86012270+0i\n[5,]  0.2629802-0.3968579i  0.2629802+0.3968579i 0.3132200+0i  0.06616328+0i\n              [,5]\n[1,]  0.0000000+0i\n[2,]  0.0000000+0i\n[3,]  0.6351203+0i\n[4,] -0.7554249+0i\n[5,] -0.1611069+0i\n\n\nHowever this calculates right eigenvectors. We will need to invert it for left ones. Given the number of jump variables in the model satisfies the Blanchard-Kahn conditions of as many unstable roots (1.072+0.093i, 1.072-0.093i) as jump variables (2) we can calculate the reaction function from the eigenvectors\n\niz <- 1:np\nix <- (np+1):ns\nM  <- solve(m$vectors[,ns:1])        # Invert & reverse order for increasing abs value\nN  <- -Re(solve(M[ix,ix], M[ix,iz])) # Drop tiny complex bits (if any)\n\nwhere iz are the indices of the first np variables and ix those of the remaining nf ones.\n\n\n3.3.2 Stochastic part\nWhat about the shocks? Assume the stochastic reaction function is \\[\n  x_t = N z_{t-1} + G \\varepsilon_t\n\\] Following Blake (2004), note that \\(x_{t+1}^e = N z_t\\) as the expected value of \\(\\varepsilon_{t+1}=0\\), meaning we can write \\[\nNz_t = C_{21}z_{t-1} + C_{22} x_t + D_2 \\varepsilon_t\n\\] or \\[\nN\\left( C_{11}z_{t-1} + C_{12}x_t + D_1 \\varepsilon_t\\right) = C_{21}z_{t-1} + C_{22} x_t + D_2 \\varepsilon_t\n\\] Gathering terms we obtain \\[\n  (C_{22} - N C_{12}) x_t = (NC_{11} - C_{21}) z_{t-1} + (N D_1 - D_2) \\varepsilon_t\n\\] which implies \\[\nG=(C_{22} - N C_{12})^{-1}(N D_1 - D_2)\n\\] Notice it also implies \\(N = (C_{22} - N C_{12})^{-1}(NC_{11} - C_{21})\\). It is this fixed point nature of the solution for \\(N\\) – which in turn implies the quadratic matrix equation \\(C_{21} = NC_{11} - C_{22}N + N C_{12}N\\) – that means we need to use the Blanchard and Kahn (1980) method in the first place.\n\n\n3.3.3 R code\nAll of this means that\n\nG <- solve((C[ix,ix] - N %*% C[iz,ix]), (N %*% D[iz,]- D[ix,]))\n\nso for our model and parameters \\(N\\) and \\(G\\) are\n\nN\n\n        [,1]      [,2]       [,3]\n[1,] 4.85680 -2.758647 -1.1894200\n[2,] 1.79286  1.962790 -0.2536635\n\nG\n\n         [,1]      [,2]      [,3]\n[1,] 5.396445 -3.448309 -1.585893\n[2,] 1.992067  2.453488 -0.338218\n\n\nThe ‘fixed point’ check is that the following should be the same as \\(N\\)\n\nsolve((C[ix,ix] - N %*% C[iz,ix]), (N %*% C[iz,iz]- C[ix,iz]))\n\n        [,1]      [,2]       [,3]\n[1,] 4.85680 -2.758647 -1.1894200\n[2,] 1.79286  1.962790 -0.2536635\n\n\nwhich it is.\nThe solved model is finally \\[\n\\begin{align}\n\\begin{bmatrix} z_t \\\\ x_t \\end{bmatrix} &= \\begin{bmatrix} C_{11}+C_{12}N & 0 \\\\ N & 0 \\end{bmatrix} \\begin{bmatrix} z_{t-1} \\\\ x_{t-1} \\end{bmatrix} + \\begin{bmatrix} D_1+C_{12}G \\\\ G \\end{bmatrix} \\varepsilon_t \\\\\n&= P \\begin{bmatrix} z_{t-1} \\\\ x_{t-1} \\end{bmatrix} + Q \\varepsilon_t\n\\end{align}\n\\] which can be coded as\n\nP  <- cbind(rbind((C[iz,iz] + C[iz,ix] %*% N), N), matrix(0,ns,nf))\nQ  <- rbind(D[iz,] + C[iz,ix] %*% G, G)\n\n\n\n3.3.4 Digression – right eigenvector version\nIt turns out that we could use the output from the standard eigenvalue/vector routine directly by exploiting the following. This time, let \\(M\\) be the matrix of right eigenvectors so \\[\n  C M = M \\Lambda \\text{  or  } C = M\\Lambda M^{-1}\n\\]\nand \\[\n\\begin{bmatrix} M_{11} & M_{12} \\\\ M_{21} & M_{22} \\end{bmatrix}\n\\begin{bmatrix} \\xi_{t-1}^s \\\\ \\xi_t^u \\end{bmatrix}\n=\n\\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix}\n\\]\nWritten this way around, if \\(\\xi_t^{u}=0\\) \\(\\forall\\ t\\) then (again ignoring stochastics)\n\\[\n  M_{11} \\xi_{t-1}^s = z_{t-1}, \\ M_{21}\\xi_t^s = x_t\n\\]\n\\[\n   \\Rightarrow x_t = M_{21} M_{11}^{-1} z_t\n\\] so\n\nM <- m$vectors[,ns:1]            # Don't invert as already right vectors, but reorder\nRe(M[ix,iz] %*% solve(M[iz,iz])) # Again, drop tiny complex bits\n\n        [,1]      [,2]       [,3]\n[1,] 4.85680 -2.758647 -1.1894200\n[2,] 1.79286  1.962790 -0.2536635\n\n\nThe result is identical. This method is particularly useful if there are fewer predetermined variables than jumps as the matrix we need to invert is of the same dimension as the predetermined variables this way round.\n\n\n3.3.5 Impulse responses\nWe now call the impulse response function using the model solved for rational expectations.\n\nT <- 25\nz <- impulse_responses(P, Q, Omega, labels, T)\n\nNow plot these responses\n\nresponse_plot(z, \"Impulse responses: Taylor rule\")\n\n\n\n\n\n\n\n\nNow, that looks better! It is no longer explosive. It also makes complete economic sense, which you can verify by going through the dynamics of the different demand, supply and monetary shocks."
  },
  {
    "objectID": "BK.html#generalized-solution",
    "href": "BK.html#generalized-solution",
    "title": "3  Linear rational expectations models",
    "section": "3.4 Generalized solution",
    "text": "3.4 Generalized solution\nSometimes for a model \\(E\\) is singular. A more general solution was proposed by Klein (2000), that doesn’t require \\(E\\) to be non-singular. This uses a generalized Schur decomposition instead of an eigenvalue one and is applied to the structural model represented by the matrix pencil \\((A,E)\\), and is considered much more numerically stable (see Pappas, Laub, and Sandell (1980)). The generalized Schur form of \\((A,E)\\) is \\((QTZ', QSZ')\\), so we can write the model as \\[\nE \\begin{bmatrix} z_t \\\\ x_{t+1}^e \\end{bmatrix} \\equiv QTZ' \\begin{bmatrix} z_t \\\\ x_{t+1}^e \\end{bmatrix} \\equiv QT \\begin{bmatrix} \\xi_t^s \\\\ \\xi_{t+1}^u \\end{bmatrix}\n\\] and \\[\nA \\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix} \\equiv QSZ' \\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix} \\equiv QS\\begin{bmatrix} \\xi_{t-1}^s \\\\ \\xi_t^u \\end{bmatrix}\n\\] so the model pre-multiplied by \\(Q'\\) is \\[\nT \\begin{bmatrix} \\xi_{t+1}^s \\\\ \\xi_{t+1}^u \\end{bmatrix} = S \\begin{bmatrix} \\xi_t^s \\\\ \\xi_t^u \\end{bmatrix} + Q'B\\varepsilon_t\n\\]\nWe use the function gqz from the library geigen for this\n\nd <- geigen::gqz(A, E, sort=\"S\") # Option \"S\" puts the stable roots first\n\nWe can check that this is actually saddle path using gevalues() to get all the eigenvalues from the generalized Schur decomposition, and the unstable ones are\n\ne <- geigen::gevalues(d)\ne[abs(e) > 1]\n\n[1] 1.071552+0.092734i 1.071552-0.092734i\n\n\nThe number of stable roots is returned in d$sdim which is 3.\nWe then modify our solution function to calculate Ns and Gs using the matrix Z and a generalized version of the formula for \\(G\\) and calculate the reduced form model Ps and `Q which are \\[\n\\begin{align}\n    N_s &= Z_{21} Z_{11}^{-1} \\\\\n    H   &= (E_{11} + E_{12} N_s)^{-1} \\\\\n    W   &= (E_{21} + E_{22} N_s) H\\\\\n    G_s &= (A_{22} - W A_{12})^{-1} (W B_1 - B_2) \\\\\n    P_s &= H (A_{11} + A_{12} N_s) \\\\\n    Q_s &= H (B_1 + A_{12} G_s)\n\\end{align}\n\\] Verify this yourself with a bit of matrix algebra!\nThe R code for this is\n\nsolveGenBK <- function(E,A,B,n) {\n  d  <- geigen::gqz(A, E, sort=\"S\") \n  np <- d$sdim\n  ns <- nrow(E)\n  print(paste(\"Number of unstable roots is\", ns-np))\n  if (n == np) {\n    iz <- 1:n\n    ix <- (n+1):ns\n    Ns <- d$Z[ix,iz] %*% solve(d$Z[iz,iz])\n    H  <- solve(E[iz,iz] + E[iz,ix] %*% Ns)\n    W  <- (E[ix,iz] + E[ix,ix] %*% Ns) %*% H\n    Gs <- solve((A[ix,ix] - W %*% A[iz,ix]), (W %*% B[iz,] - B[ix,]))\n    As <- H %*% (A[iz,iz] + A[iz,ix] %*% Ns)\n    Bs <- H %*% (B[iz,] + A[iz,ix] %*% Gs)\n    return(list(P=cbind(rbind(As,Ns),matrix(0,ns,ns-n)), Q=rbind(Bs, Gs)))\n    } \n  else { \n    return(-1) \n    }\n}\n\nUsing this on our original model gives\n\nS  <- solveGenBK(E,A,B,np)\n\n[1] \"Number of unstable roots is 2\"\n\nPs <- S$P\nQs <- S$Q\n\nand comparing Ps and Qs with P and Q obtained using Blanchard-Kahn we find\n\nround(max(abs(P-Ps), abs(Q-Qs)), 12)\n\n[1] 0\n\n\nThey are, as expected, the same – at least up to 12 decimal places, which should be enough."
  },
  {
    "objectID": "BK.html#introduction",
    "href": "BK.html#introduction",
    "title": "3  Linear rational expectations models",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nHow do we solve rational expectations models? Here I show how to implement versions of the Blanchard and Kahn (1980) and Klein (2000) solutions to linear rational expectations models in R. The implementation is fairly general, and copes with singular models. It is a very transparent implementation, with all the necessary code, and also shows how to calculate and plot impulse responses."
  }
]