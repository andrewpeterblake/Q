[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantiles",
    "section": "",
    "text": "Genesis\nThis book is the result of an initiative that the Bank of England began in the early 1990s. This was a different world with a burgeoning new world order, as the Iron Curtain crumbled and the European experiment gathered momentum.\nPlaceholders abound.\nDisclaimer: The Bank of England does not accept any liability for misleading or inaccurate information or omissions in the information provided. The subject matter reflects the views of the individual presenter and not the wider Bank of England or its Policy Committees."
  },
  {
    "objectID": "intro.html#reading-is-good-for-you",
    "href": "intro.html#reading-is-good-for-you",
    "title": "1  Introduction",
    "section": "1.1 Reading is good for you",
    "text": "1.1 Reading is good for you\nFor me, the best (although slightly dated) text is Hastie, Tibshirani, and Friedman (2009) The Elements of Statistical Learning and the best source for the mathematics, with an easy-reading version by some of the same authors James et al. (2021) Introduction to Statistical Learning.\nI also rather like Boehmke and Greenwell (2019) Hands-On Machine Learning with R which is something of a cookbook rather than a technical manual but with wide scope. Taddy (2019) is more elementary.\nOn text, just read Silge and Robinson (2017) Text Mining with R: A Tidy Approach and then Hvitfeldt and Silge (2021) Supervised Machine Learning for Text Analysis in R. That’s it.\nTwo books I would solidly recommend to make us all into better statisticians and not just econometricians are Gelman, Hill, and Vehtari (2019) Regression and Other Stories, and McElreath (2020) Statistical Rethinking.\n\n\n\n\nBlake, Andrew P, and Haroon Mumtaz. 2017. Applied Bayesian Econometrics for Central Bankers. Revised. Technical Books. Centre for Central Banking Studies, Bank of England.\n\n\nBoehmke, Brad, and Brandon M. Greenwell. 2019. Hands-on Machine Learning with R. The R Series. Boca Raton: Chapman & Hall/CRC. https://bradleyboehmke.github.io/HOML/.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2019. Regression and Other Stories. Cambridge: Cambridge University Press. http://www.stat.columbia.edu/~gelman/regression.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. New York, NY: Springer. https://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. Chapman & Hall: CRC Press. https://smltar.com/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. 2nd ed. Springer Texts in Statistics. New York, NY: Springer. https://www.statlearning.com/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Abingdon, Oxfordshire: CRC Press. https://github.com/rmcelreath/rethinking.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. O’Reilly. https://www.tidytextmining.com/.\n\n\nTaddy, Matt. 2019. Business Data Science: Combining Machine Learning and Economics to Optimize, Automate, and Accelerate Business Decisions. New York, NY: McGraw-Hill Education. https://github.com/TaddyLab/BDS."
  },
  {
    "objectID": "R2021.html#selected-ml-and-dataviz-techniques-using-r",
    "href": "R2021.html#selected-ml-and-dataviz-techniques-using-r",
    "title": "2  Non-econometric methods for econometricians",
    "section": "2.1 Selected ML and Dataviz techniques using R",
    "text": "2.1 Selected ML and Dataviz techniques using R\nEconometricians are used to handling data, performing analysis and reporting results. But somewhere along the line data became big and unstructured, analysis was now machines learning about something and outputs became visualisations.\nThis online seminar takes some big(ish) datasets, sets the machines on them and draws some great graphs. If you ever wondered what use a tree was for forecasting or why everything is a network (including neural ones), or wanted to draw a map with your house in it, or to understand a document without the bother of reading it (or a few other things besides) then you might find something to interest you. All done in R.\nSpecifically, this seminar is designed to introduce some of the key methods used outside of econometrics that econometricians will find very useful in their work in a central bank. This includes some important machine learning techniques as a gateway to others, particularly tree-based methods and neural networks, as well as text processing and map making. All the way through there is an emphasis on the network properties of many of these techniques. We make extensive use of the tidyverse, including ggplot2 and tidytext, and a number of statistics, machine learning, geographical data and other packages.\nThe framework for each day is the following:\n\nEach day is divided into two two-hour sessions starting at 10.30 am and 2.00 pm GMT.\nThe first hour of each will be an online presentation covering a particular topic (or topics) with a look at both techniques and code.\nAfter a quick break the second hour will be largely devoted to the code itself or resources to understand how to code the material.\n\nWe may run polls during the event to prioritize the topics covered in the webinars as it is not expected that everyone will be able to try out everything.\n\n2.1.1 The code\nAll code and some of the data will be made available through the Juno portal. For each presentation the .Rmd (R markdown) file is supplied that creates the presentation, an HTML file of the presentation for you to step through which can be re-created from the .Rmd file, and a further .R file of the code that we use. Some additional code and data is included, including links to a number of videos that cover some additional aspects both in this file and in the presentations.\nSome data will need to be downloaded from original other sites if all the examples are to be followed. All code is additionally available at https://github.com/andrewpeterblake/R2020 or https://github.com/andrewpeterblake/R2021 or through the QR codes below.\n\n\n\n\n\nGitHub: 2020 (grey, left), 2021 (pink, right)\n\n\n\n\n\n\n\nGitHub: 2020 (grey, left), 2021 (pink, right)"
  },
  {
    "objectID": "R2021.html#how-to-ensure-rstudio-finds-the-code",
    "href": "R2021.html#how-to-ensure-rstudio-finds-the-code",
    "title": "2  Non-econometric methods for econometricians",
    "section": "2.2 HOW TO ENSURE RSTUDIO FINDS THE CODE",
    "text": "2.2 HOW TO ENSURE RSTUDIO FINDS THE CODE\nTo use the code, in particular so that R Studio finds the data files etc, create a directory for each topic, (e.g. Trees, ANN etc) and copy the contents from the zip file or GitHub. Then create a new project in R Studio that uses that directory as its home directory, using “File/New Project” in the drop down menu. Opening files within a project sets the home directory to that directory, so everything (including the sub-directories) can be found."
  },
  {
    "objectID": "R2021.html#typical-program-structure",
    "href": "R2021.html#typical-program-structure",
    "title": "2  Non-econometric methods for econometricians",
    "section": "2.3 Typical program structure",
    "text": "2.3 Typical program structure\n\n2.3.1 Day 1: Trees and maps\n\n2.3.1.1 Trees\n\nClassification and regression trees\nEconometrics strikes back: Bootstrap/bagging and Boosting/Model selection\nRandom forests\nVisualising decision trees\nUse example: House prices\n\nThe presentations for this are Trees.html and LondonHP.html; The two programs TreeCancer.R and TreeNW.R are the use examples.\n\n\n2.3.1.2 Maps\n\nHow to draw a map in R\nA guide to some resources\nChoropleths\nUse examples: Climate change, regional data, postcode wrangling\n\nThe presentation for this is MapAER.html (see also Weatherpretty.html); The program MapAERcode.R is the main map drawing code, I’ve included ZAF.R as as short simple way and source for two countries, and the directory Trendz contains the program (app.R) and data for the weather example.\n\n\n\n\n\n\n\n\n\nI’ve included an additional video (red QR code) for more about Shiny. This uses unemployment data from the Survey of Professional Forecasters. The code we look at is for climate change data World Bank data.\nA comprehensive treatment of maps is Lovelace, Nowosad, and Muenchow (2019) Geocomputation in R, but it is quite a lot to assimilate all at once.\n\n\n\n\n\n2.3.2 Day 2: Networks\n\n2.3.2.1 Neural networks\n\nWhat is an ANN? Deep learning?\nFunction approximation via a network\nData: fit, validate, test\nNetwork architecture\nUse examples: House prices revisited\n\nThe presentation for this is IntroANN.html; The program ANN.R replicates the ANN estimation. The data used is the same as for Day 1.\n\n\n2.3.2.2 Networks (real ones)\n\nDAGs and ANNs as network graphs\nIncidence matrices\nMeasuring connectivity: Degree and betweenness\nPlotting with igraph\nUse examples: Industry inter-relationships\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe presentation used for the first part of this is DAG.html and the program Draw_DAG_ANN.R draws the ANN examples from Day 2 Session 1 as well as some of the DAG examples. The example is modified from Cunningham (2021) Causal Inference: The Mixtape, which is a great read with R code. The pdf HandShake3.pdf is the source of the director network graphs, and Graph101a.R is a subset of the analytical work on the corruption data set as described in the post Graph Theory 101 (purple QR code), which is the work of Marina Medina (blue QR code link to presentation site).\n\n\n\n\n2.3.3 Day 3: Text\n\n2.3.3.1 Text modelling, a ‘tidytext’ approach (Session 1)\n\nData cleaning\nSentiment\nTopic modelling\n\n\n\n2.3.3.2 Text modelling, a ‘tidytext’ approach (Session 2)\n\nParts-of-speech tagging\nText regression\nUse examples: Central bank minutes, reports\n\n\n\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. New Haven & London: Yale University Press. https://mixtape.scunning.com/.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with R. 1st ed. The R Series. Boca Raton: Chapman & Hall/CRC. https://geocompr.robinlovelace.net/."
  },
  {
    "objectID": "QR.html#getting-the-data",
    "href": "QR.html#getting-the-data",
    "title": "3  Quantile regression in R",
    "section": "3.1 Getting the data",
    "text": "3.1 Getting the data\nWe download the data and save it locally.\n\nh <- \"https://www.philadelphiafed.org/-/media/frbp/assets/surveys-and-data/survey-of-professional-forecasters/historical-data/\"\nf <- \"meanlevel.xlsx\"\n\ndownload.file(paste0(h, f), destfile=f, mode=\"wb\")\n\nRetrieve the unemployment data for the average unemployment forecast.\n\nUNEMP <- f %>%\n  read_excel(na=\"#N/A\", sheet=\"UNEMP\") %>% \n  mutate(Date=as.Date(as.yearqtr(paste(YEAR, QUARTER), format=\"%Y %q\"))) \n\nUsel <- UNEMP %>% \n  select(Date, UNEMP1, UNEMP3, UNEMP4, UNEMP5, UNEMP6) %>%\n  mutate(UNRATE = lead(UNEMP1,1)) %>%\n  select(Date, UNRATE, \n         UNEMP1=UNEMP3, UNEMP2=UNEMP4, UNEMP3=UNEMP5, UNEMP4=UNEMP6) %>%\n  mutate(UNEMP1 = lag(UNEMP1,1), \n         UNEMP2 = lag(UNEMP2,2), \n         UNEMP3 = lag(UNEMP3,3), \n         UNEMP4 = lag(UNEMP4,4)) %>%\n  pivot_longer(cols = -c(Date, UNRATE), names_to=\"Which\", values_to=\"Val\") %>%\n  filter(year(Date) > 2000)"
  },
  {
    "objectID": "QR.html#plots",
    "href": "QR.html#plots",
    "title": "3  Quantile regression in R",
    "section": "3.2 Plots",
    "text": "3.2 Plots\n\nUsel %>% \n  ggplot(aes(x=Date)) + \n  geom_line(aes(y=UNRATE), colour=\"red\") + \n  geom_point(aes(y=Val, colour=Which, shape=Which)) +\n  theme_light() + \n  labs(title=\"Mean unemployment forecasts\", x=\"\", y=\"\", caption=\"Source: SPF\")"
  },
  {
    "objectID": "Stemp.html#study-question-1.3.2",
    "href": "Stemp.html#study-question-1.3.2",
    "title": "4  Causal Inference in Statistics",
    "section": "4.1 Study question 1.3.2",
    "text": "4.1 Study question 1.3.2\nData:\n\nlibrary(tidyverse)\ned <- tibble(Gender = c(\"M\",\"M\",\"M\",\"M\",\"F\",\"F\",\"F\",\"F\"),\n             eLevel = c(\"U\",\"H\",\"C\",\"G\",\"U\",\"H\",\"C\",\"G\"),\n             num    = c(112,231,595,242,136,189,763,172)) %>%\n  mutate(total = sum(num))\n\nwhich we tabulate as\n\ned %>%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\n\n\n\n\nM\nU\n112\n2440\n\n\nM\nH\n231\n2440\n\n\nM\nC\n595\n2440\n\n\nM\nG\n242\n2440\n\n\nF\nU\n136\n2440\n\n\nF\nH\n189\n2440\n\n\nF\nC\n763\n2440\n\n\nF\nG\n172\n2440"
  },
  {
    "objectID": "Stemp.html#exercises-and-answers",
    "href": "Stemp.html#exercises-and-answers",
    "title": "4  Causal Inference in Statistics",
    "section": "4.2 Exercises and answers",
    "text": "4.2 Exercises and answers\n\n4.2.1 Find \\(P(eLevel = H)\\)\n\ned %>%\n  filter(eLevel == \"H\") %>%\n  mutate(p_H = sum(num)/total) %>%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\np_H\n\n\n\n\nM\nH\n231\n2440\n0.1721311\n\n\nF\nH\n189\n2440\n0.1721311\n\n\n\n\n\n\n\n4.2.2 Find \\(P(eLevel = H\\ \\vee \\ Gender = F)\\)\n\ned %>%\n  filter(Gender == \"F\" | eLevel == \"H\") %>%\n  mutate(p_HorF = sum(num)/total) %>%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\np_HorF\n\n\n\n\nM\nH\n231\n2440\n0.6110656\n\n\nF\nU\n136\n2440\n0.6110656\n\n\nF\nH\n189\n2440\n0.6110656\n\n\nF\nC\n763\n2440\n0.6110656\n\n\nF\nG\n172\n2440\n0.6110656\n\n\n\n\n\n\n\n4.2.3 Find \\(P(eLevel = H\\ |\\ Gender = F)\\)\n\ned %>%\n  filter(Gender == \"F\") %>%\n  mutate(tcond = sum(num)) %>% \n  filter(eLevel == \"H\") %>%\n  mutate(p_HgivenF = sum(num)/tcond) %>%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\ntcond\np_HgivenF\n\n\n\n\nF\nH\n189\n2440\n1260\n0.15\n\n\n\n\n\n\n\n4.2.4 Find \\(P(Gender = F\\ | \\ eLevel = H)\\)\n\ned %>%\n  filter(eLevel == \"H\") %>%\n  mutate(tcond = sum(num)) %>% \n  filter(Gender == \"F\") %>%\n  mutate(p_FgivenH = sum(num)/tcond) %>%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\ntcond\np_FgivenH\n\n\n\n\nF\nH\n189\n2440\n420\n0.45\n\n\n\n\n\n\n\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal Inference in Statistics: A Primer. Chichester: John Wiley & Sons. http://bayes.cs.ucla.edu/PRIMER/."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "5  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever…"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bahaj, Saleem, Angus Foulis, and Gabor Pinter. 2020. “Home Values\nand Firm Behavior.” American Economic Review 110 (7):\n2225–70. https://doi.org/10.1257/aer.20180649.\n\n\nBlake, Andrew P, and Haroon Mumtaz. 2017. Applied Bayesian\nEconometrics for Central Bankers. Revised. Technical Books. Centre\nfor Central Banking Studies, Bank of England.\n\n\nBoehmke, Brad, and Brandon M. Greenwell. 2019. Hands-on Machine\nLearning with R. The R Series. Boca\nRaton: Chapman & Hall/CRC. https://bradleyboehmke.github.io/HOML/.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. New\nHaven & London: Yale University Press. https://mixtape.scunning.com/.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2019. Regression and\nOther Stories. Cambridge: Cambridge University Press. http://www.stat.columbia.edu/~gelman/regression.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The\nElements of Statistical Learning: Data Mining, Inference, and\nPrediction. 2nd ed. New York, NY: Springer. https://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning\nfor Text Analysis in R. Chapman & Hall: CRC Press.\nhttps://smltar.com/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical Learning: With Applications in\nR. 2nd ed. Springer Texts in Statistics. New York, NY:\nSpringer. https://www.statlearning.com/.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019.\nGeocomputation with R. 1st ed. The R\nSeries. Boca Raton: Chapman & Hall/CRC. https://geocompr.robinlovelace.net/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in R and Stan. 2nd ed. Abingdon,\nOxfordshire: CRC Press. https://github.com/rmcelreath/rethinking.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal\nInference in Statistics: A Primer. Chichester: John Wiley &\nSons. http://bayes.cs.ucla.edu/PRIMER/.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with\nR: A Tidy Approach. O’Reilly. https://www.tidytextmining.com/.\n\n\nTaddy, Matt. 2019. Business Data Science: Combining Machine Learning\nand Economics to Optimize, Automate, and Accelerate Business\nDecisions. New York, NY: McGraw-Hill Education. https://github.com/TaddyLab/BDS."
  },
  {
    "objectID": "R2021.html#selected-ml-and-dataviz-techniques-in-r",
    "href": "R2021.html#selected-ml-and-dataviz-techniques-in-r",
    "title": "2  Non-econometric methods for econometricians",
    "section": "2.1 Selected ML and Dataviz techniques in R",
    "text": "2.1 Selected ML and Dataviz techniques in R\nEconometricians are used to handling data, performing analysis and reporting results. But somewhere along the line data became big and unstructured, analysis was now machines learning about something and outputs became visualisations.\nThis online seminar takes some big(ish) datasets, sets the machines on them and draws some great graphs. If you ever wondered what use a tree was for forecasting or why everything is a network (including neural ones), or wanted to draw a map with your house in it, or to understand a document without the bother of reading it (or a few other things besides) then you might find something to interest you. All done in R.\nSpecifically, this seminar is designed to introduce some of the key methods used outside of econometrics that econometricians will find very useful in their work in a central bank. This includes some important machine learning techniques as a gateway to others, particularly tree-based methods and neural networks, as well as text processing and map making. All the way through there is an emphasis on the network properties of many of these techniques. We make extensive use of the tidyverse, including ggplot2 and tidytext, and a number of statistics, machine learning, geographical data and other packages.\nThe framework for each day is the following:\n\nEach day is divided into two two-hour sessions starting at 10.30 am and 2.00 pm GMT.\nThe first hour of each will be an online presentation covering a particular topic (or topics) with a look at both techniques and code.\nAfter a quick break the second hour will be largely devoted to the code itself or resources to understand how to code the material.\n\nWe may run polls during the event to prioritize the topics covered in the webinars as it is not expected that everyone will be able to try out everything.\n\n2.1.1 The code\nAll code and some of the data will be made available through the Juno portal. For each presentation the .Rmd (R markdown) file is supplied that creates the presentation, an HTML file of the presentation for you to step through which can be re-created from the .Rmd file, and a further .R file of the code that we use. Some additional code and data is included, including links to a number of videos that cover some additional aspects both in this file and in the presentations.\nSome data will need to be downloaded from original other sites if all the examples are to be followed. All code is additionally available at https://github.com/andrewpeterblake/R2020 or https://github.com/andrewpeterblake/R2021 or through the QR codes below.\n\n\n\n\n\nGitHub: 2020 (grey, left), 2021 (pink, right)\n\n\n\n\n\n\n\nGitHub: 2020 (grey, left), 2021 (pink, right)"
  },
  {
    "objectID": "index.html#genesis",
    "href": "index.html#genesis",
    "title": "Quantiles",
    "section": "Genesis",
    "text": "Genesis\nThis book is the result of an initiative that the Bank of England began in the early 1990s. This was a different world with a burgeoning new world order, as the Iron Curtain crumbled and the European experiment gathered momentum.\nPlaceholders abound."
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Quantiles",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe Bank of England does not accept any liability for misleading or inaccurate information or omissions in the information provided. The subject matter reflects the views of the individual presenter and not the wider Bank of England or its Policy Committees."
  },
  {
    "objectID": "index.html#sec-genesis",
    "href": "index.html#sec-genesis",
    "title": "Quantiles, Networks, Time",
    "section": "Genesis",
    "text": "Genesis\nThis book is the result of an initiative the Bank of England began in the early 1990s, and a prescient one. This was at a major historical turning point, one that signalled a burgeoning new world order, as the Iron Curtain crumbled, the European experiment gathered momentum, and industrial might began an inexorable shift eastwards.\n\nPlaceholders abound."
  },
  {
    "objectID": "index.html#sec-disclaimer",
    "href": "index.html#sec-disclaimer",
    "title": "Quantiles, Networks, Time",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe Bank of England does not accept any liability for misleading or inaccurate information or omissions in the information provided. The subject matter reflects the views of the individual presenter and not the wider Bank of England or its Policy Committees."
  },
  {
    "objectID": "Maps.html#how-heterogenous-is-uk-house-price-inflation",
    "href": "Maps.html#how-heterogenous-is-uk-house-price-inflation",
    "title": "5  Mapping regional house price inflation",
    "section": "5.1 How heterogenous is UK house price inflation?",
    "text": "5.1 How heterogenous is UK house price inflation?\nA simple enough question, and one that Bahaj, Foulis, and Pinter (2020) thought was best answered with a map – actually a referee asked for one. As I know how to draw a map in R they asked me if I could do it. Well yes, but there are some particular difficulties.\n\nThe UK (actually Great Britain) is an awkward (but not too awkward) shape.\nPopulation in the UK is heavily concentrated in a small number of centres, such as London or Manchester.\nThere are three different periods to compare.\nIt has to be in grayscale.\n\nBefore all of this we need some data, with boundaries that correspond to areas that we have data for. The regional inflation data is available at the level of the Land Registry, which almost by local authority but amalgamates a number of the areas. So a map at Local Authority level would be fine as long as we can amalgamate some of the regions.\nThe map data used here is available from the UK’s ONS geoportal, with a lot of administrative data available including local authority boundaries. The Local Authority data is specifically available from here, where I use the clipped full extent version. There are a number of possibilities, but in general high water mark, and enough but not too much detail is needed.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(sf)\n\nThe information in the map file is comprehensive, and by Local Authority as of December 2015.\n\nfle <- \"LAD_Dec_2015_GCB_GB\"\nshape <- read_sf(dsn=\".\", layer=fle) %>%\n  filter(!lad15nm %in% c(\"Shetland Islands\",\"Orkney Islands\")) %>%\n  mutate(Country=str_sub(lad15cd, 1, 1), .after=1)\n\nWe can look at the attributes using summary.\n\nsummary(shape)\n\n   lad15cd            Country            lad15nm            lad15nmw        \n Length:378         Length:378         Length:378         Length:378        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n   GlobalID                  geometry  \n Length:378         MULTIPOLYGON :378  \n Class :character   epsg:27700   :  0  \n Mode  :character   +proj=tmer...:  0  \n\n\nThis can be plotted straightforwardly using ggplot.\n\nshape %>%\n  ggplot() +\n  geom_sf(aes(geometry=geometry, fill=lad15nm), \n          color=NA, alpha=.66, show.legend=FALSE) +\n  theme_void()\n\n\n\n\n\n\n\nshape %>%\n  group_by(Country) %>%\n  summarise() %>%\n  ggplot() +\n  geom_sf(aes(fill=Country), color=\"grey77\", alpha=.66) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n5.1.1 Tidy data\nHowever, ggplot can be used.\nLooking at the read-out above, each of the 380 regions have some metadata associated, which are contained in each of the listed attributes. It should be obvious that objectid is just a sequence from 1 to 380. lad15nm turns out to be a list of names of the regions – I suspect lad for Local Authority District, 15 for 2015 and nm for name – and it is easy to specify this as the name to use for the region when using tidy.\nNow this can be plotted using ggplot, using long and lat as the \\(x\\) and \\(y\\) coordinates. The choice of fill colour is determined by fill and we can set the colour of the lines by colour (or color). The two extra arguments are for a suitable blank style and to impose an appropriate ratio of height to width.\nImmediately, the awkward shape of the British Isles is apparent. (Note this is a plot of Great Britain, and there is no Northern Ireland.) The islands to the far north are somewhat unnecessary, although quite rightly the inhabitants get a bit tired of being left off maps! Nonetheless I’ll do exactly the same by filtering out the polygons associated with Orkney Islands and Shetland Islands.\n\n\n\nFewer Scottish Islands makes the graphs a lot clearer with little loss of information, given the tiny number of transactions in the Orkneys and the Shetlands, very far to the north.\n\n\n5.1.2 LA boundaries\nYou may have noticed, one thing that that’s missing on all of these graphs is the Local Authority boundaries. They aren’t, they’re just invisible. That’s because I have set colour = NA in all the graphs above, so we can fix that by choosing a colour and making the lines very thin so they don’t swamp the map.\n\n\n\nOne further amendment, the fill is moved inside the aes() specification and made conditional. R now chooses unique colours for each of the regions.\nTwo things now need to be done to get the map colours right to illustrate regional inflation rates. First we need to amalgamate some of the Local Authority boundaries to the Land Registry definitions, and second we need to assign the inflation rate to each area.\n\n\n5.1.3 Inflation data and regions\nWe have a map, and we have that data in a form that is easy to understand. If we can suitably attach an inflation rate to each area then we can fill the individual areas with a colour unique to each individual inflation rates.\nRecall that the Land Registry areas aren’t quite what we have, and will need amalgamating. Bahaj, Foulis, and Pinter (2020) supplied me the areas that needed amalgamating (and the inflation rates) using the ONS codes. This is contained in the metadata lad15cd above.\nThe data is structured in ‘wide’ format with one row for each Land Registry region. The details aren’t very important for us now, but what it means is I can manipulate it to get\n\n# Price data by Land Registry region, converted to long format\nhp_data <- read_excel(\"house_price_data_figure_1.xls\")  %>% \n  select(\"land_reg_region\", starts_with(\"e_\"), starts_with(\"av_\")) %>% \n  pivot_longer(names_to  = \"name\", \n               values_to = \"lad15cd\", \n               cols      = c(-land_reg_region, -starts_with(\"av_\"))) %>% \n  drop_na() %>%\n  select(land_reg_region, lad15cd, starts_with(\"av_\")) \n\ncodes <- hp_data %>% \n  select(lad15cd, land_reg_region) \n\nThe important thing that the pivot_longer achieves is that for every land_reg_region I get a list of all the ONS codes that makes up the Local Authority level. So if I look at buckinghamshire as an example there are four ONS codes now associated with it.\n\nfilter(codes, land_reg_region == \"buckinghamshire\")\n\n# A tibble: 4 × 2\n  lad15cd   land_reg_region\n  <chr>     <chr>          \n1 E07000004 buckinghamshire\n2 E07000005 buckinghamshire\n3 E07000006 buckinghamshire\n4 E07000007 buckinghamshire\n\n\nJoin:\n\n# Join polygons defined by Land Registry regions\ngg <- shape %>%\n  select(starts_with(c(\"lad\",\"C\"))) %>% \n  left_join(codes, by=\"lad15cd\") %>%\n  group_by(land_reg_region) %>%\n  summarise() \n\nproduces a match between the Land Registry and the Local Authority areas, plus the inflation rates. To illustrate this just filter out the buckinghamshire ones again to see:"
  },
  {
    "objectID": "Maps.html#inflation-in-grayscale",
    "href": "Maps.html#inflation-in-grayscale",
    "title": "5  Mapping regional house price inflation",
    "section": "5.2 Inflation in grayscale",
    "text": "5.2 Inflation in grayscale\nAll the information required to plot the Land Registry-based regional inflation rates is now available. As you can see from the buckinghamshire data above, there are three average rates in three different periods, so I’ll focus on one, 2002-2007 to begin with.\nFirst, augment the geographic data with the inflation data, and call them something better.\n\ngg %>%\n  ggplot() +\n  geom_sf(aes(geometry=geometry, fill=land_reg_region), \n          color=NA, alpha=.66, show.legend = FALSE) +\n  theme_void()\n\n\n\n\n\n\n\n\nThen calculate some limits for the gradient, and modify the specify a plot to include specifically gray and put the legend at the bottom.\n\nnms <- gsub(\"av_hp_growth\", \"HPI\", colnames(hp_data))\n\nhp_data %>%\n  rename_all( ~ nms) %>%\n  select(land_reg_region, starts_with(\"HPI\")) %>%\n  distinct() %>%\n  left_join(gg) %>%\n  ggplot() +\n  geom_sf(aes(geometry=geometry, fill=HPI_02_07), \n          color=NA, alpha=.66, show.legend = FALSE) +\n  theme_void() +\n  scale_fill_gradient(low=grey(0.9), high=grey(0.05)) +\n  theme(legend.direction = \"horizontal\", \n        legend.position  = c(0.75,0.05),\n        legend.title     = element_blank())\n\nJoining with `by = join_by(land_reg_region)`\n\n\n\n\n\n\n\n\n\n\n\n\n\nBahaj, Saleem, Angus Foulis, and Gabor Pinter. 2020. “Home Values and Firm Behavior.” American Economic Review 110 (7): 2225–70. https://doi.org/10.1257/aer.20180649."
  }
]