[
  {
    "objectID": "LinAlg.html#modelling-in-economics-is-linear-algebra",
    "href": "LinAlg.html#modelling-in-economics-is-linear-algebra",
    "title": "Appendix B — Linear algebra",
    "section": "B.1 Modelling in economics is linear algebra",
    "text": "B.1 Modelling in economics is linear algebra\nThat’s a bit extreme, but you mostly need to do linear algebra to program up many of the estimators we need, or to solve a rational expectations models."
  },
  {
    "objectID": "LinAlg.html#beginning-to-program",
    "href": "LinAlg.html#beginning-to-program",
    "title": "Appendix B — Linear algebra",
    "section": "B.2 Beginning to program",
    "text": "B.2 Beginning to program\nA few non-linear algebra things we will need are summarized in the following table.\n\nUseful things\n\n\n\n\n\n\n\n\n\nR\nExample\nNotes\n\n\n\n\nAssign a value\n<-\na <- 4\nAlso legal is a = 4. But I hate it.\n\n\nCreate a list of values\nc(.)\nv <- c(1, -2, 22)\nDefining ‘on the fly’\n\n\nSequence\nseq(i, k, l)\n\\(5\\), \\(7\\), … ,\\(21\\)\nCreate a sequence\n\n\n\ni:k\n\\(i\\), \\(i\\pm 1\\), … ,\\(k\\)\nShort cut for unit in/de-crements\n\n\nLoop commands\nfor (var in seq) expr\nfor (i in 5:1) print(i)\nLoops. We need loops.\n\n\nDraw a random number\nrnorm(k,a,b)\nrnorm(60, 0, 5)\nExample draws 60 values ~ \\(N(0,5)\\)\n\n\nCreate a matrix\nmatrix(v,i,j)\nmatrix(5, 2, 2)\nCreate a \\(2\\times 2\\) matrix of 5s\n\n\n\n\nB.2.1 Functions\nEverything in R is a function (although it doesn’t look like it). Defining a function is simple:\n\nname_of_function <- function(function_arguments){\n  # Body of function where stuff is done  \n}\n\nHere’s one that actually does something:\n\naddaddadd <- function(x,y){\n  z <- 3*(x+y)\n  return(z)\n}\n\nand if we run it:\n\naddaddadd(4,6)\n\n[1] 30"
  },
  {
    "objectID": "LinAlg.html#linear-algebra",
    "href": "LinAlg.html#linear-algebra",
    "title": "Appendix B — Linear algebra",
    "section": "B.3 Linear algebra",
    "text": "B.3 Linear algebra\nAssume the following: \\(A\\) and \\(B\\) are real matrices of dimension \\(n\\times n\\), \\(b\\) and \\(c\\) are real \\(n-\\)vectors, \\(X\\) is a real \\(T\\times k\\) matrix, and \\(S\\) is a symmetric real matrix.\n\nMaths commands essential to linear algebra\n\n\n\n\n\n\n\n\n\nMaths\nR\nNotes\n\n\n\n\nHadamard product\n\\(A\\bigodot B\\)\nA * B\nElement-by-element, \\(A\\), \\(B\\) same size\n\n\nMatrix/vector product\n\\(A\\times B\\), \\(A \\times b\\)\nA %*% B, A %*% b\nNormal product rule\n\n\nInner product\n\\(X'X\\)\nt(X) %*% X\nAlso uses transpose operator, t()\n\n\n\n\ncrossprod(X)\nMore efficient, but less mathy\n\n\n\n\\(A'B\\)\nt(A) %*% B\n\n\n\n\n\ncrossprod(A,B)\n\n\n\nOuter product\n\\(A\\times B'\\)\ntcrossprod(A,B)\n\n\n\nInverse\n\\(A^{-1}\\)\nsolve(A)\nMatrix inverse is a special case of…\n\n\nSolve for \\(d\\)\n\\(Ad = b \\Rightarrow d = A^{-1}b\\)\nd <- solve(A, b)\n…linear solution!\n\n\nCholesky decomp\n\\(S = R'R\\)\nR <- chol(S)\n\\(S\\) is a symmetric, positive definite matrix\n\n\nCholesky inverse\n\\(S^{-1}\\)\nchol2inv(chol(S))\nFast!\n\n\nDeterminant\n\\(\\vert A \\vert\\)\ndet(A)\n\n\n\nDiagonal\n\n\n\n\n\n\\(\\quad\\) of a matrix\n\ndiag(A)\nRetrieve the elements \\(a_{ii}\\), \\(i=1,..,n\\)\n\n\n\\(\\quad\\) in a matrix\n\nA <- diag(b)\nSet the diagonal of \\(A\\) to \\(b\\), zero elsewhere\n\n\n\\(\\quad\\) Identity matrix\n\\(I_n\\)\ndiag(n)\n\n\n\nEigenvalues/vectors\n\nE <- eigen(A)\nReturns a list: E$values, E$vectors"
  },
  {
    "objectID": "LinAlg.html#starting-to-do-linear-algebra",
    "href": "LinAlg.html#starting-to-do-linear-algebra",
    "title": "Appendix B — Linear algebra",
    "section": "B.4 Starting to do linear algebra",
    "text": "B.4 Starting to do linear algebra\n\nB.4.1 Problem\nConsider the following simultaneous system of equations: \\[\n\\begin{align}\nx_1 + 2x_2 &= 6 \\\\\nx_1 - 3x_2 +2 x_3 &= 0 \\\\\n-2 x_1 + 3 x_3 &= 2\n\\end{align}\n\\] Find the values of \\(x\\) that solve this using R.\nHint – write the problem in matrix form \\[\nAx = b\n\\] where \\[\nA = \\left [ \\begin{array}{r}\n             1 &  2 & 0 \\cr\n             1 & -3 & 2\\cr\n            -2 &  0 & 3\n            \\end{array} \\right ], \\qquad\n  b = \\left[ \\matrix{6 \\\\ 0 \\\\ 2} \\right ]\n\\] and then use solve.\n\n\nB.4.2 Solution\nR code to create these matrices is:\n\nA <- matrix(c(1,1,-2,2,-3,0,0,2,3),3,3) # Matrices are populated by column by default\nb <- matrix(c(6,0,2),3,1)\n\nThe solution is:\n\nx <- solve(A,b)\n\nwhere x is:\n\n\n     [,1]\n[1,]    2\n[2,]    2\n[3,]    2"
  },
  {
    "objectID": "LinAlg.html#eigenvalue-decomposition",
    "href": "LinAlg.html#eigenvalue-decomposition",
    "title": "Appendix B — Linear algebra",
    "section": "B.5 Eigenvalue decomposition",
    "text": "B.5 Eigenvalue decomposition\nAny square matrix can be decomposed into a non-singular matrix \\(V\\) of eigenvectors and a diagonal matrix of eigenvalues \\(\\Lambda\\) such that: \\[\nA V = V \\Lambda \\Rightarrow A = V\\Lambda V^{-1}\n\\]\nCall eigen to decompose our previously defined matrix \\(A\\)\n\ne <- eigen(A)\nL <- e$values    # Recall a list is returned so assign vectors/values\nV <- e$vectors\n\nNote \\(A\\) is not symmetric so it may have complex roots, which is does\n\nL\n\n[1] -3.682744+0.000000i  2.341372+0.873683i  2.341372-0.873683i\n\n\nIf we calculate \\(A = V\\Lambda V^{-1}\\) we get\n\ns <- V %*% diag(L) %*% solve(V)\ns\n\n      [,1]             [,2]             [,3]\n[1,]  1+0i  2.000000e+00+0i -2.220446e-16+0i\n[2,]  1+0i -3.000000e+00+0i  2.000000e+00+0i\n[3,] -2+0i -1.054712e-15+0i  3.000000e+00+0i\n\n\nand when we drop the zero imaginary parts\n\nRe(s)\n\n     [,1]          [,2]          [,3]\n[1,]    1  2.000000e+00 -2.220446e-16\n[2,]    1 -3.000000e+00  2.000000e+00\n[3,]   -2 -1.054712e-15  3.000000e+00\n\n\nand then round we get\n\nround(Re(s), digits=12)\n\n     [,1] [,2] [,3]\n[1,]    1    2    0\n[2,]    1   -3    2\n[3,]   -2    0    3"
  },
  {
    "objectID": "LinAlg.html#precision",
    "href": "LinAlg.html#precision",
    "title": "Appendix B — Linear algebra",
    "section": "B.6 Precision",
    "text": "B.6 Precision\nTake a real matrix \\(A_{mn}\\) with \\(n \\le m\\) and pre-multiply by its own transpose, i.e. \\(S = A'A\\). \\(AA\\) is symmetric, positive semi-definite. If \\(rank(A) = n\\), then \\(rank(S) = n\\) and positive definite, and its inverse exists.\n\nA  <- matrix(c(1, .2, 0, 1), 2, 2)\nS <- t(A) %*% A\nS\n\n     [,1] [,2]\n[1,] 1.04  0.2\n[2,] 0.20  1.0\n\n\nLet’s invert \\(S\\) three different ways.\n\ni1 <- solve(S) \ni2 <- chol2inv(chol(S))\ni3 <- qr.solve(S) \n\nPre-multiplying \\(S\\) by its inverse gives \\[\nI_k = \\left[\\begin{array}{r}1 &0 \\\\0 &1 \\\\\\end{array}\\right]\n\\] Looking at the results of doing this for each method gives\n\ni1 %*% S\n\n             [,1] [,2]\n[1,] 1.000000e+00    0\n[2,] 2.775558e-17    1\n\ni2 %*% S\n\n     [,1]         [,2]\n[1,]    1 5.551115e-17\n[2,]    0 1.000000e+00\n\ni3 %*% S\n\n     [,1]          [,2]\n[1,]    1 -2.775558e-17\n[2,]    0  1.000000e+00\n\n\nwhich are all slightly different but by tiny – and insignificant – amounts."
  },
  {
    "objectID": "LinAlg.html#programming-the-regression-problem",
    "href": "LinAlg.html#programming-the-regression-problem",
    "title": "Appendix B — Linear algebra",
    "section": "B.7 Programming the regression problem",
    "text": "B.7 Programming the regression problem\nLet’s look at the familiar regression problem for some generated data. \\[\n  y = XB + \\epsilon\n\\] where \\(\\epsilon \\sim N(0,.2)\\), \\(X\\) is a \\((k+1)\\times n\\) matrix of regressors including a constant and \\(B\\) a \\(k+1\\) vector of coefficients. Let’s generate some random data of an arbitrary sized problem:\n\nX <- matrix(rnorm(180, 2, 1), 60, 3)\nhead(X, 6) # Print first six rows\n\n         [,1]       [,2]     [,3]\n[1,] 1.591524  1.9112153 1.964557\n[2,] 2.360774  3.2324808 2.297938\n[3,] 1.464025  1.8983024 2.732581\n[4,] 2.078359  1.6949893 3.293322\n[5,] 1.125727  2.3368681 3.432484\n[6,] 2.124659 -0.1894496 1.937154\n\nX <- cbind(1, X) # Add a constant\ntail(X,6) # Print last six rows\n\n      [,1]     [,2]     [,3]     [,4]\n[55,]    1 3.043344 2.390351 3.117802\n[56,]    1 1.997198 1.587965 2.194946\n[57,]    1 1.270505 1.142846 1.416780\n[58,]    1 1.558521 2.373420 1.658206\n[59,]    1 2.525228 3.095101 2.207356\n[60,]    1 2.711093 1.783989 2.044215\n\n\nNow create a dependent variable that is a linear combination of these variables plus some noise. Create the linear relationship first so we know what it is:\n\nB <- matrix(c(0.5,1,-1,.2), 4, 1)\n\nand then the dependent variable:\n\ny <- X %*% B + 0.2*rnorm(60)\n\nWe could now do a regression – i.e. calculate \\[\n  \\hat B = (X'X)^{-1}X'y\n\\] which can be written:\n\nBhat <- solve(t(X)%*%X)%*%t(X)%*%y\n\nwhich gives\n\n\n           [,1]\n[1,]  0.4127937\n[2,]  0.9993383\n[3,] -0.9948062\n[4,]  0.2309061\n\n\nBut I wouldn’t do it like this (we’ll see why in a minute). A better way would be\n\nBhat2 <- chol2inv(chol(crossprod(X))) %*% crossprod(X,y)\n\nor even\n\nBhat3 <- qr.solve(X,y)\n\nwhich both evaluate to the same \\(\\hat B\\) values.\n\nB.7.1 Test timings\nWhy does it matter how you do things? It should be obvious that it might, but it turns out some fairly trivial things can make a lot of difference. We set some parameters so we can create a bigger problem.\n\nn <- 400\nk <- 12\n\nWe will use seven different methods to calculate an estimate of \\(B\\). These are two variations on the three calculations below (where the brackets matter!): \\[\n\\hat B_1 = ((X'X)^{-1}) X' y\n\\] \\[\n\\hat B_2 = ((X'X)^{-1}) (X' y)\n\\] \\[\n\\hat B_3 = ((X'X)^{-1} (X' y))\n\\] where we do it either ‘by hand’ or using crossprob, plus using qr.solve.\n\nlibrary(tictoc)\n\nreps <- 10000\nt    <- list()\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B <- matrix(runif(k+1), k+1, 1)\n  X <- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y <- X %*% B + 0.2 * rnorm(n) \n  Bhat <- solve(t(X) %*% X) %*% t(X) %*% y \n  }\nt[[1]] <- toc(quiet = TRUE)\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B <- matrix(runif(k+1), k+1, 1)\n  X <- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y <- X %*% B + 0.2 * rnorm(n) \n  Bhata <- solve(t(X)%*%X) %*% (t(X)%*%y) \n}\nt[[2]] <- toc(quiet = TRUE)\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B <- matrix(runif(k+1), k+1, 1)\n  X <- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y <- X %*% B + 0.2 * rnorm(n) \n  Bhatb <- solve((t(X) %*% X), (t(X) %*% y)) \n  }\nt[[3]] <- toc(quiet = TRUE)\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B <- matrix(runif(k+1), k+1, 1)\n  X <- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y <- X %*% B + 0.2 * rnorm(n) \n  Bhat2 <- solve(crossprod(X)) %*% crossprod(X,y) \n  }\nt[[4]] <- toc(quiet = TRUE)\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B <- matrix(runif(k+1), k+1, 1)\n  X <- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y <- X %*% B + 0.2 * rnorm(n) \n  Bhat2a <- solve(crossprod(X), crossprod(X,y)) \n  }\nt[[5]] <- toc(quiet = TRUE)\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B <- matrix(runif(k+1), k+1, 1)\n  X <- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y <- X %*% B + 0.2 * rnorm(n)\n  Bhat2b <- chol2inv(chol(crossprod(X))) %*% crossprod(X,y) \n  }\nt[[6]] <- toc(quiet = TRUE)\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B <- matrix(runif(k+1), k+1, 1)\n  X <- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y <- X %*% B + 0.2 * rnorm(n) \n  Bhat3 <- qr.solve(X,y) \n  }\nt[[7]] <- toc(quiet = TRUE)\n\nHow do we display the timings we saved in t? We could do a simple (but dull) table, or something a but nicer.\n\nlibrary(tidyverse)\n\nFF <- c(\"solve(t(X)%*%X) %*% t(X) %*% y\",\n        \"solve(t(X)%*%X) %*% (t(X)%*%y)\",\n        \"solve(t(X)%*%X, (t(X)%*%y))\",\n        \"solve(crossprod(X)) %*% crossprod(X,y)\",\n        \"solve(crossprod(X), crossprod(X,y))\",\n        \"chol2inv(chol(crossprod(X))) %*% crossprod(X,y)\",\n        \"qr.solve(X,y)\")\n\nv <- tibble(Method = 1:7, \n            Times = as.numeric(gsub(\" sec elapsed\", \"\", unlist(t)[seq(3,21,3)])), \n            Formula = FF) %>% \n  mutate(\n    Col = case_when(\n      Times == min(Times) ~ \"red\",\n      TRUE ~ \"blue\"\n  ))\n\nggplot(v) + \n  geom_col(aes(x=Method, y=Times, fill=as.factor(Method)), alpha=.6) + \n  geom_text(aes(x=Method, y=.1, label=Formula, color=Col), size=5.5, hjust=0) +\n  theme_minimal() + \n  scale_color_identity() + \n  theme(legend.position = \"none\") + \n  theme(axis.text.y = element_blank()) + \n  coord_flip() + \n  labs(y=paste(\"Seconds taken to do\",reps,\"replications\"), x=\"\", \n       title=\"Timings of different numerical regression methods\")"
  },
  {
    "objectID": "LinAlg.html#testing-timings",
    "href": "LinAlg.html#testing-timings",
    "title": "Appendix B — Linear algebra",
    "section": "B.8 Testing timings",
    "text": "B.8 Testing timings\nWhy does it matter how you do things? It should be obvious that it might, but it turns out some fairly trivial things can make a lot of difference. We set some parameters so we can create a bigger problem.\n\nn <- 400\nk <- 12\n\nWe will use seven different methods to calculate an estimate of \\(B\\). These are two variations on the three calculations below (where the brackets matter!): \\[\n\\hat B_1 = ((X'X)^{-1}) X' y\n\\]\n\\[\n\\hat B_2 = ((X'X)^{-1}) (X' y)\n\\] \\[\n\\hat B_3 = ((X'X)^{-1} (X' y))\n\\] where we do it either ‘by hand’ or using crossprob, plus using qr.solve.\n\nlibrary(tictoc)\n\nreps <- 10000\nt    <- list()\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B <- matrix(runif(k+1), k+1, 1)\n  X <- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y <- X %*% B + 0.2 * rnorm(n) \n  Bhat <- solve(t(X) %*% X) %*% t(X) %*% y \n  }\nt[[1]] <- toc(quiet = TRUE)\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B <- matrix(runif(k+1), k+1, 1)\n  X <- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y <- X %*% B + 0.2 * rnorm(n) \n  Bhata <- solve(t(X)%*%X) %*% (t(X)%*%y) \n}\nt[[2]] <- toc(quiet = TRUE)\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B <- matrix(runif(k+1), k+1, 1)\n  X <- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y <- X %*% B + 0.2 * rnorm(n) \n  Bhatb <- solve((t(X) %*% X), (t(X) %*% y)) \n  }\nt[[3]] <- toc(quiet = TRUE)\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B <- matrix(runif(k+1), k+1, 1)\n  X <- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y <- X %*% B + 0.2 * rnorm(n) \n  Bhat2 <- solve(crossprod(X)) %*% crossprod(X,y) \n  }\nt[[4]] <- toc(quiet = TRUE)\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B <- matrix(runif(k+1), k+1, 1)\n  X <- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y <- X %*% B + 0.2 * rnorm(n) \n  Bhat2a <- solve(crossprod(X), crossprod(X,y)) \n  }\nt[[5]] <- toc(quiet = TRUE)\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B <- matrix(runif(k+1), k+1, 1)\n  X <- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y <- X %*% B + 0.2 * rnorm(n)\n  Bhat2b <- chol2inv(chol(crossprod(X))) %*% crossprod(X,y) \n  }\nt[[6]] <- toc(quiet = TRUE)\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B <- matrix(runif(k+1), k+1, 1)\n  X <- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y <- X %*% B + 0.2 * rnorm(n) \n  Bhat3 <- qr.solve(X,y) \n  }\nt[[7]] <- toc(quiet = TRUE)\n\nHow do we display the timings we saved in t? We could do a simple (but dull) table, or something a but nicer.\n\nlibrary(tidyverse)\n\nFF <- c(\"solve(t(X)%*%X) %*% t(X) %*% y\",\n        \"solve(t(X)%*%X) %*% (t(X)%*%y)\",\n        \"solve(t(X)%*%X, (t(X)%*%y))\",\n        \"solve(crossprod(X)) %*% crossprod(X,y)\",\n        \"solve(crossprod(X), crossprod(X,y))\",\n        \"chol2inv(chol(crossprod(X))) %*% crossprod(X,y)\",\n        \"qr.solve(X,y)\")\n\nv <- tibble(Method = 1:7, \n            Times = as.numeric(gsub(\" sec elapsed\", \"\", unlist(t)[seq(3,21,3)])), \n            Formula = FF) %>% \n  mutate(Col = case_when(\n    Times == min(Times) ~ \"red\",\n    TRUE ~ \"blue\"\n  ))\n\nggplot(v) + \n  geom_col(aes(x=Method, y=Times, fill=as.factor(Method)), alpha=.6) + \n  geom_text(aes(x=Method, y=.1, label=Formula, color=Col), size=5.5, hjust=0) +\n  theme_minimal() + \n  scale_color_identity() + \n  theme(legend.position = \"none\") + \n  theme(axis.text.y = element_blank()) + \n  coord_flip() + \n  labs(y=paste(\"Seconds taken to do\",reps,\"replications\"), x=\"\", \n       title=\"Timings of different numerical regression methods\")"
  },
  {
    "objectID": "intro.html#reading-is-good-for-you",
    "href": "intro.html#reading-is-good-for-you",
    "title": "1  Introduction",
    "section": "1.1 Reading is good for you",
    "text": "1.1 Reading is good for you\nFor me, the best (although slightly dated) text is Hastie, Tibshirani, and Friedman (2009) The Elements of Statistical Learning and the best source for the mathematics, with an easy-reading version by some of the same authors James et al. (2021) Introduction to Statistical Learning.\nI also rather like Boehmke and Greenwell (2019) Hands-On Machine Learning with R which is something of a cookbook rather than a technical manual but with wide scope. Taddy (2019) is more elementary.\nOn text, just read Silge and Robinson (2017) Text Mining with R: A Tidy Approach and then Hvitfeldt and Silge (2021) Supervised Machine Learning for Text Analysis in R. That’s it.\nTwo books I would solidly recommend to make us all into better statisticians and not just econometricians are Gelman, Hill, and Vehtari (2019) Regression and Other Stories, and McElreath (2020) Statistical Rethinking.\n\n\n\n\nBlake, Andrew P, and Haroon Mumtaz. 2017. Applied Bayesian Econometrics for Central Bankers. Revised. Technical Books. Centre for Central Banking Studies, Bank of England. https://www.bankofengland.co.uk/-/media/boe/files/ccbs/resources/applied-bayesian-econometrics-for-central-bankers-updated-2017.pdf.\n\n\nBoehmke, Brad, and Brandon M. Greenwell. 2019. Hands-on Machine Learning with R. The R Series. Boca Raton: Chapman & Hall/CRC. https://bradleyboehmke.github.io/HOML/.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2019. Regression and Other Stories. Cambridge: Cambridge University Press. http://www.stat.columbia.edu/~gelman/regression.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. New York, NY: Springer. https://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. Chapman & Hall: CRC Press. https://smltar.com/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. 2nd ed. Springer Texts in Statistics. New York, NY: Springer. https://www.statlearning.com/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Abingdon, Oxfordshire: CRC Press. https://github.com/rmcelreath/rethinking.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. O’Reilly. https://www.tidytextmining.com/.\n\n\nTaddy, Matt. 2019. Business Data Science: Combining Machine Learning and Economics to Optimize, Automate, and Accelerate Business Decisions. New York, NY: McGraw-Hill Education. https://github.com/TaddyLab/BDS."
  },
  {
    "objectID": "R2021.html#selected-ml-and-dataviz-techniques-in-r",
    "href": "R2021.html#selected-ml-and-dataviz-techniques-in-r",
    "title": "2  Non-econometric methods for econometricians",
    "section": "2.1 Selected ML and Dataviz techniques in R",
    "text": "2.1 Selected ML and Dataviz techniques in R\nEconometricians are used to handling data, performing analysis and reporting results. But somewhere along the line data became big and unstructured, analysis was now machines learning about something and outputs became visualisations.\nThis online seminar takes some big(ish) datasets, sets the machines on them and draws some great graphs. If you ever wondered what use a tree was for forecasting or why everything is a network (including neural ones), or wanted to draw a map with your house in it, or to understand a document without the bother of reading it (or a few other things besides) then you might find something to interest you. All done in R.\nSpecifically, this seminar is designed to introduce some of the key methods used outside of econometrics that econometricians will find very useful in their work in a central bank. This includes some important machine learning techniques as a gateway to others, particularly tree-based methods and neural networks, as well as text processing and map making. All the way through there is an emphasis on the network properties of many of these techniques. We make extensive use of the tidyverse, including ggplot2 and tidytext, and a number of statistics, machine learning, geographical data and other packages.\nThe framework for each day is the following:\n\nEach day is divided into two two-hour sessions starting at 10.30 am and 2.00 pm GMT.\nThe first hour of each will be an online presentation covering a particular topic (or topics) with a look at both techniques and code.\nAfter a quick break the second hour will be largely devoted to the code itself or resources to understand how to code the material.\n\nWe may run polls during the event to prioritize the topics covered in the webinars as it is not expected that everyone will be able to try out everything.\n\n2.1.1 The code\nAll code and some of the data will be made available through the Juno portal. For each presentation the .Rmd (R markdown) file is supplied that creates the presentation, an HTML file of the presentation for you to step through which can be re-created from the .Rmd file, and a further .R file of the code that we use. Some additional code and data is included, including links to a number of videos that cover some additional aspects both in this file and in the presentations.\nSome data will need to be downloaded from original other sites if all the examples are to be followed. All code is additionally available at https://github.com/andrewpeterblake/R2020 or https://github.com/andrewpeterblake/R2021 or through the QR codes below.\n\n\n\n \n\n\n\n\n\n2020\n\n\n\n\n \n\n\n\n\n\n2021\n\n\n\n\n \n\n\n\n\n \n\n\nGitHub repositories for historic CCBS courses"
  },
  {
    "objectID": "R2021.html#how-to-ensure-rstudio-finds-the-code",
    "href": "R2021.html#how-to-ensure-rstudio-finds-the-code",
    "title": "2  Non-econometric methods for econometricians",
    "section": "2.2 HOW TO ENSURE RSTUDIO FINDS THE CODE",
    "text": "2.2 HOW TO ENSURE RSTUDIO FINDS THE CODE\nTo use the code, in particular so that R Studio finds the data files etc, create a directory for each topic, (e.g. Trees, ANN etc) and copy the contents from the zip file or GitHub. Then create a new project in R Studio that uses that directory as its home directory, using “File/New Project” in the drop down menu. Opening files within a project sets the home directory to that directory, so everything (including the sub-directories) can be found."
  },
  {
    "objectID": "R2021.html#typical-program-structure",
    "href": "R2021.html#typical-program-structure",
    "title": "2  Non-econometric methods for econometricians",
    "section": "2.3 Typical program structure",
    "text": "2.3 Typical program structure\n\n2.3.1 Day 1: Trees and maps\n\n2.3.1.1 Trees\n\nClassification and regression trees\nEconometrics strikes back: Bootstrap/bagging and Boosting/Model selection\nRandom forests\nVisualising decision trees\nUse example: House prices\n\nThe presentations for this are Trees.html and LondonHP.html; The two programs TreeCancer.R and TreeNW.R are the use examples.\n\n\n2.3.1.2 Maps\n\nHow to draw a map in R\nA guide to some resources\nChoropleths\nUse examples: Climate change, regional data, postcode wrangling\n\nThe presentation for this is MapAER.html (see also Weatherpretty.html); The program MapAERcode.R is the main map drawing code, I’ve included ZAF.R as as short simple way and source for two countries, and the directory Trendz contains the program (app.R) and data for the weather example.\n\n\n\n\n\n\n\n\nI’ve included an additional video (red QR code) for more about Shiny. This uses unemployment data from the Survey of Professional Forecasters. The code we look at is for climate change data World Bank data.\n\n\n\nA comprehensive treatment of maps is Lovelace, Nowosad, and Muenchow (2019) Geocomputation in R, but it is quite a lot to assimilate all at once.\n\n\n\n2.3.2 Day 2: Networks\n\n2.3.2.1 Neural networks\n\nWhat is an ANN? Deep learning?\nFunction approximation via a network\nData: fit, validate, test\nNetwork architecture\nUse examples: House prices revisited\n\nThe presentation for this is IntroANN.html; The program ANN.R replicates the ANN estimation. The data used is the same as for Day 1.\n\n\n2.3.2.2 Networks (real ones)\n\nDAGs and ANNs as network graphs\nIncidence matrices\nMeasuring connectivity: Degree and betweenness\nPlotting with igraph\nUse examples: Industry inter-relationships\n\n\n\n\n \n\n\n\n\n\nCoding club\n\n\n\n\n \n\n\n\n\n\nR-Bloggers article\n\n\n\n\n \n\n\n\n\n \n\n\nNetwork examples\n\n\n \n\n\n \n\n\n\nThe presentation used for the first part of this is DAG.html and the program Draw_DAG_ANN.R draws the ANN examples from Day 2 Session 1 as well as some of the DAG examples. The example is modified from Cunningham (2021) Causal Inference: The Mixtape, which is a great read with R code. The pdf HandShake3.pdf is the source of the director network graphs, and Graph101a.R is a subset of the analytical work on the corruption data set as described in the post Graph Theory 101 (purple QR code), which is the work of Marina Medina (blue QR code link to presentation site).\n\n\n\n2.3.3 Day 3: Text\nText modelling, a ‘tidytext’ approach.\n\n\n\n2.3.3.1 Session 1\n\nData cleaning\nSentiment\nTopic modelling\n\n\n\n2.3.3.2 Session 2\n\nParts-of-speech tagging\nText regression\nUse examples: Central bank minutes, reports\n\n\n\n\n\n\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. New Haven & London: Yale University Press. https://mixtape.scunning.com/.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with R. 1st ed. The R Series. Boca Raton: Chapman & Hall/CRC. https://geocompr.robinlovelace.net/."
  },
  {
    "objectID": "QR.html#getting-the-data",
    "href": "QR.html#getting-the-data",
    "title": "4  Quantile regression",
    "section": "4.1 Getting the data",
    "text": "4.1 Getting the data\nWe download the data and save it locally.\n\nh &lt;- \"https://www.philadelphiafed.org/-/media/frbp/assets/surveys-and-data/survey-of-professional-forecasters/historical-data/\"\nf &lt;- \"meanlevel.xlsx\"\n\ndownload.file(paste0(h, f), destfile=f, mode=\"wb\")\n\nRetrieve the unemployment data for the average unemployment forecast.\n\nUNEMP &lt;- f %&gt;%\n  read_excel(na=\"#N/A\", sheet=\"UNEMP\") %&gt;% \n  mutate(Date=as.Date(as.yearqtr(paste(YEAR, QUARTER), format=\"%Y %q\"))) \n\nUsel &lt;- UNEMP %&gt;% \n  select(Date, UNEMP1, UNEMP3, UNEMP4, UNEMP5, UNEMP6) %&gt;%\n  mutate(UNRATE = lead(UNEMP1,1)) %&gt;%\n  select(Date, UNRATE, \n         UNEMP1=UNEMP3, UNEMP2=UNEMP4, UNEMP3=UNEMP5, UNEMP4=UNEMP6) %&gt;%\n  mutate(UNEMP1 = lag(UNEMP1,1), \n         UNEMP2 = lag(UNEMP2,2), \n         UNEMP3 = lag(UNEMP3,3), \n         UNEMP4 = lag(UNEMP4,4)) %&gt;%\n  pivot_longer(cols = -c(Date, UNRATE), names_to=\"Which\", values_to=\"Val\") %&gt;%\n  filter(year(Date) &gt; 2000)"
  },
  {
    "objectID": "QR.html#plots",
    "href": "QR.html#plots",
    "title": "4  Quantile regression",
    "section": "4.2 Plots",
    "text": "4.2 Plots\n\nUsel %&gt;% \n  ggplot(aes(x=Date)) + \n  geom_line(aes(y=UNRATE), colour=\"red\") + \n  geom_point(aes(y=Val, colour=Which, shape=Which)) +\n  theme_light() + \n  labs(title=\"Mean unemployment forecasts\", x=\"\", y=\"\", caption=\"Source: SPF\")"
  },
  {
    "objectID": "Stemp.html#study-question-1.3.2",
    "href": "Stemp.html#study-question-1.3.2",
    "title": "6  Causal Inference",
    "section": "6.1 Study question 1.3.2",
    "text": "6.1 Study question 1.3.2\nData:\n\nlibrary(tidyverse)\ned &lt;- tibble(Gender = c(\"M\",\"M\",\"M\",\"M\",\"F\",\"F\",\"F\",\"F\"),\n             eLevel = c(\"U\",\"H\",\"C\",\"G\",\"U\",\"H\",\"C\",\"G\"),\n             num    = c(112,231,595,242,136,189,763,172)) %&gt;%\n  mutate(total = sum(num))\n\nwhich we tabulate as\n\ned %&gt;%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\n\n\n\n\nM\nU\n112\n2440\n\n\nM\nH\n231\n2440\n\n\nM\nC\n595\n2440\n\n\nM\nG\n242\n2440\n\n\nF\nU\n136\n2440\n\n\nF\nH\n189\n2440\n\n\nF\nC\n763\n2440\n\n\nF\nG\n172\n2440"
  },
  {
    "objectID": "Stemp.html#exercises-and-answers",
    "href": "Stemp.html#exercises-and-answers",
    "title": "6  Causal Inference",
    "section": "6.2 Exercises and answers",
    "text": "6.2 Exercises and answers\n\n6.2.1 Find \\(P(eLevel = H)\\)\n\ned %&gt;%\n  filter(eLevel == \"H\") %&gt;%\n  mutate(p_H = sum(num)/total) %&gt;%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\np_H\n\n\n\n\nM\nH\n231\n2440\n0.1721311\n\n\nF\nH\n189\n2440\n0.1721311\n\n\n\n\n\n\n\n6.2.2 Find \\(P(eLevel = H\\ \\vee \\ Gender = F)\\)\n\ned %&gt;%\n  filter(Gender == \"F\" | eLevel == \"H\") %&gt;%\n  mutate(p_HorF = sum(num)/total) %&gt;%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\np_HorF\n\n\n\n\nM\nH\n231\n2440\n0.6110656\n\n\nF\nU\n136\n2440\n0.6110656\n\n\nF\nH\n189\n2440\n0.6110656\n\n\nF\nC\n763\n2440\n0.6110656\n\n\nF\nG\n172\n2440\n0.6110656\n\n\n\n\n\n\n\n6.2.3 Find \\(P(eLevel = H\\ |\\ Gender = F)\\)\n\ned %&gt;%\n  filter(Gender == \"F\") %&gt;%\n  mutate(tcond = sum(num)) %&gt;% \n  filter(eLevel == \"H\") %&gt;%\n  mutate(p_HgivenF = sum(num)/tcond) %&gt;%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\ntcond\np_HgivenF\n\n\n\n\nF\nH\n189\n2440\n1260\n0.15\n\n\n\n\n\n\n\n6.2.4 Find \\(P(Gender = F\\ | \\ eLevel = H)\\)\n\ned %&gt;%\n  filter(eLevel == \"H\") %&gt;%\n  mutate(tcond = sum(num)) %&gt;% \n  filter(Gender == \"F\") %&gt;%\n  mutate(p_FgivenH = sum(num)/tcond) %&gt;%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\ntcond\np_FgivenH\n\n\n\n\nF\nH\n189\n2440\n420\n0.45\n\n\n\n\n\n\n\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal Inference in Statistics: A Primer. Chichester: John Wiley & Sons. http://bayes.cs.ucla.edu/PRIMER/."
  },
  {
    "objectID": "Maps.html#how-heterogenous-is-uk-house-price-inflation",
    "href": "Maps.html#how-heterogenous-is-uk-house-price-inflation",
    "title": "7  Mapping regional house price inflation",
    "section": "7.1 How heterogenous is UK house price inflation?",
    "text": "7.1 How heterogenous is UK house price inflation?\nA simple enough question, and one that Bahaj, Foulis, and Pinter (2020) thought was best answered with a map – actually a referee asked for one. As I know how to draw a map in R they asked me if I could do it. Well yes, but there are some particular difficulties.\n\nThe UK (actually Great Britain) is an awkward (but not too awkward) shape.\nPopulation in the UK is heavily concentrated in a small number of centres, such as London or Manchester.\nThere are three different periods to compare.\nIt has to be in grayscale.\n\nBefore all of this we need some data, with boundaries that correspond to areas that we have data for. The regional inflation data is available at the level of the Land Registry, which almost by local authority but amalgamates a number of the areas. So a map at Local Authority level would be fine as long as we can amalgamate some of the regions.\nThe map data used here is available from the UK’s ONS geoportal, with a lot of administrative data available including local authority boundaries. The Local Authority data is specifically available from here, where I use the clipped full extent version. There are a number of possibilities, but in general high water mark, and enough but not too much detail is needed.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(sf)\n\nThe information in the map file is comprehensive, and by Local Authority as of December 2015.\n\nfle &lt;- \"LAD_Dec_2015_GCB_GB\"\nshape &lt;- read_sf(dsn=\".\", layer=fle)\n\nWe can look at the attributes using summary.\n\nsummary(shape)\n\n   lad15cd            lad15nm            lad15nmw           GlobalID        \n Length:380         Length:380         Length:380         Length:380        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n          geometry  \n MULTIPOLYGON :380  \n epsg:27700   :  0  \n +proj=tmer...:  0  \n\n\nThis can be plotted straightforwardly using ggplot.\n\nshape %&gt;%\n  ggplot() +\n  geom_sf(aes(geometry=geometry, fill=lad15nm), \n          color=NA, alpha=.66, show.legend=FALSE) +\n  theme_void()\n\n\n\n\n\n\n\n\nLooking at the read-out above, each of the 380 regions have some metadata associated, which are contained in each of the listed attributes. It should be obvious that objectid is just a sequence from 1 to 380. lad15nm turns out to be a list of names of the regions – I suspect lad for Local Authority District, 15 for 2015 and nm for name – and it is easy to specify this as the name to use for the region when using tidy.\nNow this can be plotted using ggplot, using geometry for the \\(x\\) and \\(y\\) coordinates. The choice of fill colour is determined by fill and we can set the colour of the lines by colour (or color). The two extra arguments are for a suitable blank style and to impose an appropriate ratio of height to width.\nImmediately, the awkward shape of the British Isles is apparent. (Note this is a plot of Great Britain, and there is no Northern Ireland.) The islands to the far north are somewhat unnecessary, although quite rightly the inhabitants get a bit tired of being left off maps! Nonetheless I’ll do exactly the same by filtering out the polygons associated with Orkney Islands and Shetland Islands.\nFewer Scottish Islands makes the graphs a lot clearer with little loss of information, paticularly given the tiny number of transactions in the Orkneys and the Shetlands, very far to the north.\nIn what follows we filter out the islands using\n\nshape &lt;- read_sf(dsn=\".\", layer=fle) %&gt;%\n  filter(!lad15nm %in% c(\"Shetland Islands\",\"Orkney Islands\")) %&gt;%\n  mutate(Country=str_sub(lad15cd, 1, 1), .after=1)\n\nwhere we also create an indicator of country using the first letter of the code string.\nSo the final country map is\n\nshape %&gt;%\n  group_by(Country) %&gt;%\n  summarise() %&gt;%\n  ggplot() +\n  geom_sf(aes(fill=Country), color=\"grey77\", linewidth=.25, alpha=.66) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroup and summarise can join geographical areas\n\n\n\nNote the really nice feature – if we group by something, in this case country, we can summarise to amalgamate the geometries!\n\n\nYou may have noticed, one thing that that’s missing on the LA graphs is the boundaries. They aren’t, they’re just invisible. That’s because I set colour = NA, so I can fix that by choosing a colour and making the lines very thin so they don’t swamp the map, as in the country one.\nOne further amendment, the fill is moved inside the aes() specification and made conditional. R now chooses unique colours for each of the regions.\nTwo things now need to be done to get the map colours right to illustrate regional inflation rates. First we need to amalgamate some of the Local Authority boundaries to the Land Registry definitions, and second we need to assign the inflation rate to each area."
  },
  {
    "objectID": "Maps.html#inflation-data-and-regions",
    "href": "Maps.html#inflation-data-and-regions",
    "title": "7  Mapping regional house price inflation",
    "section": "7.2 Inflation data and regions",
    "text": "7.2 Inflation data and regions\nWe have a map, and we have that data in a form that is easy to understand. If we can suitably attach an inflation rate to each area then we can fill the individual areas with a colour unique to each individual inflation rates.\nRecall that the Land Registry areas aren’t quite what we have, and will need amalgamating. Bahaj, Foulis, and Pinter (2020) supplied me the areas that needed amalgamating (and the inflation rates) using the ONS codes. This is contained in the metadata lad15cd above.\nThe data is structured in ‘wide’ format with one row for each Land Registry region. The details aren’t very important for us now, but what it means is I can manipulate it to get\n\n# Price data by Land Registry region, converted to long format\nhp_data &lt;- read_excel(\"house_price_data_figure_1.xls\")  %&gt;% \n  select(\"land_reg_region\", starts_with(\"e_\"), starts_with(\"av_\")) %&gt;% \n  pivot_longer(names_to  = \"name\", \n               values_to = \"lad15cd\", \n               cols      = c(-land_reg_region, -starts_with(\"av_\"))) %&gt;% \n  drop_na() %&gt;%\n  select(land_reg_region, lad15cd, starts_with(\"av_\")) \n\ncodes &lt;- hp_data %&gt;% \n  select(lad15cd, land_reg_region) \n\nThe important thing that the pivot_longer achieves is that for every land_reg_region I get a list of all the ONS codes that makes up the Local Authority level. So if I look at buckinghamshire as an example there are four ONS codes now associated with it.\n\nfilter(codes, land_reg_region == \"buckinghamshire\")\n\n# A tibble: 4 × 2\n  lad15cd   land_reg_region\n  &lt;chr&gt;     &lt;chr&gt;          \n1 E07000004 buckinghamshire\n2 E07000005 buckinghamshire\n3 E07000006 buckinghamshire\n4 E07000007 buckinghamshire\n\n\nJoin these together\n\n# Join polygons defined by Land Registry regions\ngg &lt;- shape %&gt;%\n  select(starts_with(c(\"lad\",\"C\"))) %&gt;% \n  left_join(codes, by=\"lad15cd\") %&gt;%\n  group_by(land_reg_region) %&gt;%\n  summarise() \n\nwhich produces a match between the Land Registry and the Local Authority areas, plus the inflation rates.\n\n7.2.1 Inflation in grayscale\nAll the information required to plot the Land Registry-based regional inflation rates is now available. As you can see from the buckinghamshire data above, there are three average rates in three different periods, so I’ll focus on one, 2002-2007 to begin with.\nFirst, augment the geographic data with the inflation data, and call them something better.\n\ngg %&gt;%\n  ggplot() +\n  geom_sf(aes(geometry=geometry, fill=land_reg_region), \n          color=NA, alpha=.66, show.legend = FALSE) +\n  theme_void()\n\n\n\n\n\n\n\n\nThen specify gray and put the legend at the bottom.\n\nnms &lt;- gsub(\"av_hp_growth\", \"HPI\", colnames(hp_data))\n\nhp_data %&gt;%\n  rename_all( ~ nms) %&gt;%\n  select(land_reg_region, starts_with(\"HPI\")) %&gt;%\n  distinct() %&gt;%\n  left_join(gg) %&gt;%\n  ggplot() +\n  geom_sf(aes(geometry=geometry, fill=HPI_02_07), \n          color=NA, alpha=.66, show.legend = TRUE) +\n  theme_void() +\n  scale_fill_gradient(low=grey(0.9), high=grey(0.05)) +\n  theme(legend.direction = \"horizontal\", \n        legend.position  = c(0.75,0.05),\n        legend.title     = element_blank())\n\nJoining with `by = join_by(land_reg_region)`\n\n\n\n\n\n\n\n\n\n\n\n\n\nBahaj, Saleem, Angus Foulis, and Gabor Pinter. 2020. “Home Values and Firm Behavior.” American Economic Review 110 (7): 2225–70. https://doi.org/10.1257/aer.20180649."
  },
  {
    "objectID": "BK.html#introduction",
    "href": "BK.html#introduction",
    "title": "6  Linear rational expectations models",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nHow do we solve rational expectations models? What does that even mean? Here I show how to implement versions of the Blanchard and Kahn (1980) and Klein (2000) solutions to linear rational expectations models in R. The implementation is fairly general, and copes with singular models. It is a very transparent implementation, with all the necessary code, and also shows how to calculate and plot impulse responses."
  },
  {
    "objectID": "BK.html#model",
    "href": "BK.html#model",
    "title": "6  Linear rational expectations models",
    "section": "6.2 Model",
    "text": "6.2 Model\nWe take a simple New Keynesian model \\[\n\\begin{align}\ny_t    &= y_{t+1}^e-\\frac{1}{\\sigma} (i_t - \\pi_{t+1}^e) + e_t^1 \\\\\n\\pi_t  &= \\beta \\pi_{t+1}^e + \\kappa y_t + e_t^2 \\\\\ni_t    &= \\gamma i_{t-1} + (1-\\gamma) \\delta \\pi_t + \\varepsilon_t^3 \\\\\ne_t^1  &= \\rho_1 e_{t-1}^1 + \\varepsilon_t^1 \\\\\ne_t^2  &= \\rho_2 e_{t-1}^2 + \\varepsilon_t^2\n\\end{align}\n\\] The model comprises a dynamic IS curve, a Phillips Curve and a policy rule with smoothing. There are three shocks, two of which are persistent. This we need to write in the general algebraic linear state-space form: \\[\nE\\begin{bmatrix} z_t \\\\ x_{t+1}^e \\end{bmatrix} = A \\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix} + B \\varepsilon_t  \n\\] We map our variables to their algebraic equivalent as (\\(z_t\\), \\(x_t\\)) \\(=\\) ((\\(e^1_t\\), \\(e^2_t\\), \\(i_t\\)), (\\(y_t\\), \\(\\pi_t\\))). Then the model in state-space form but including the matrix \\(E\\) is \\[\n\\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\\\\n                0 & 1 & 0 & 0 & 0 \\\\\n                0 & 0 & 1 & 0 & 0 \\\\\n                1 & 0 & -\\frac{1}{\\sigma} & 1 & \\frac{1}{\\sigma} \\\\\n                0 & 1 & 0 & 0 & \\beta\n\\end{bmatrix}\n\\begin{bmatrix} e^1_t \\\\ e^2_t \\\\ i_t \\\\ y^e_{t+1} \\\\ \\pi^e_{t+1} \\end{bmatrix}\n   =\n   \\begin{bmatrix} \\rho_1 & 0 & 0 & 0 & 0 \\\\\n                0 & \\rho_2 & 0 & 0 & 0 \\\\\n                0 & 0 & \\gamma & 0 & (1-\\gamma)\\delta \\\\\n                0 & 0 & 0 & 1 & 0 \\\\\n                0 & 0 & 0 & -\\kappa & 1\n   \\end{bmatrix}\n\\begin{bmatrix} e^1_{t-1} \\\\ e^2_{t-1} \\\\ i_{t-1} \\\\ y_t \\\\ \\pi_t \\end{bmatrix}    \n   +\n      \\begin{bmatrix}\n                1 & 0 & 0  \\\\\n                0 & 1 & 0 \\\\\n                0 & 0 & 1 \\\\\n                0 & 0 & 0 \\\\\n                0 & 0 & 0\n   \\end{bmatrix}\n   \\begin{bmatrix} \\varepsilon^1_t \\\\ \\varepsilon^2_t \\\\ \\varepsilon^3_t \\end{bmatrix}    \n\\] Anyone wanting to code up solutions should familiarize themselves with this before continuing.\n\n6.2.1 Coding up the model in R\nBefore we begin coding this in R, load the tidyverse libraries so we can do impulse responses with our usual tool kit and then we can forget about it.\n\nlibrary(tidyverse)\n\nSet the model parameters\n\nnf    <- 2\nns    <- 5\nne    <- 3\nnp    <- ns-nf\n\nbeta  <- 0.99   # Discount factor \nsigma <- 2.0    # Elas. substitution\nkappa <- 0.075  # Slope PC\ndelta <- 1.5    # Inflation feedback\ngamma <- 0.75   # Smoothing\nrho_1 <- 0.9    # AR1\nrho_2 <- 0.8    # AR1\nOmega <- diag(c(0.33,0.33,0.33)) # SE of 3 shocks\n\nNow define the model matrices ‘long hand’ and some variable names, which we put in labels.\n\nlabels <- c(\"e^1\",\"e^2\",\"i\",\"y\",\"pi\")\n\nE <- matrix(0,ns,ns)\nA <- matrix(0,ns,ns)\nB <- diag(1,ns,ne)\n\n# Now put the equations in matrix form\ndiag(E[1:2,1:2]) <- 1\ndiag(A[1:2,1:2]) <- c(rho_1, rho_2)\n\nE[3,3]             <- 1 \nE[4,c(1, 3, 4, 5)] <- c(1, -1/sigma, 1, 1/sigma)\nE[5,c(2, 5)]       <- c(1, beta)\n\nA[3,c(3, 5)]       <- c(gamma, (1-gamma)*delta)\nA[4,4]             <- 1\nA[5,c(4,5)]        <- c(-kappa, 1)\n\nwhere for example, \\(E\\) and \\(A\\) are \\[  \nE = \\left[\\begin{array}{r}1 &0 &0 &0 &0 \\\\0 &1 &0 &0 &0 \\\\0 &0 &1 &0 &0 \\\\1 &0 &-0.5 &1 &0.5 \\\\0 &1 &0 &0 &0.99 \\\\\\end{array}\\right]\n\\] \\[  \nA = \\left[\\begin{array}{r}0.9 &0 &0 &0 &0 \\\\0 &0.8 &0 &0 &0 \\\\0 &0 &0.75 &0 &0.375 \\\\0 &0 &0 &1 &0 \\\\0 &0 &0 &-0.075 &1 \\\\\\end{array}\\right]\n\\] Calculate the reduced form state-space model \\[\n\\begin{bmatrix} z_t \\\\ x_{t+1}^e \\end{bmatrix} = C \\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix} + D \\varepsilon_t  \n\\] which is done in R very simply as\n\nC <- solve(E,A)\nD <- solve(E,B)\n\nWhy can’t we solve this for impulse responses?\nThe following function simulates the impulse responses of a model in a loop within a loop1 and returns the time series in a suitably organised data frame.\n\nimpulse_responses <- function(P, Q, Omega, labels, T) {\n  s   <- matrix(0, ncol(Q), 1)\n  z   <- matrix(0, nrow(Q), T)\n  rownames(z) <- labels\n  dza <- NULL\n  for (j in 1:ncol(Q)) {\n    s[j]  <- Omega[j,j]\n    z[,1] <- Q %*% s\n    for (i in 1:(T-1)) {\n      z[,i+1] <- P %*% z[,i]\n    }\n    s[j] <- 0\n    dz <- as_tibble(t(z)) %>% \n      mutate(Period = 1:T, Shock = paste0(\"epsilon^\",j))\n    dza <- bind_rows(dza,dz)\n  }\n  return(dza)\n}\n\nA function to plot the impulses will be useful, so we create one.\n\nresponse_plot <- function(series, title) {\n  return(pivot_longer(series, cols = -c(Period,Shock), names_to=\"Var\", values_to = \"Val\") %>%\n           ggplot() +\n           geom_line(aes(x=Period, y=Val, group=Shock, colour=Var), show.legend=FALSE) +\n           facet_grid(Shock~Var, scales=\"free\", labeller=label_parsed) +\n           scale_x_continuous(expand=c(0,0)) +\n           theme_minimal() +\n           labs(title=title, x=\"\",y=\"\"))\n}\n\nCall the impulse response function using the model \\(C\\) and \\(D\\).\n\nT <- 25\nz <- impulse_responses(C, D, Omega, labels, T)\n\nand plot\n\nresponse_plot(z, \"Impulse responses: Taylor rule\")\n\n\n\n\n\n\n\n\nOh! That’s not looking good. Let’s try a few more periods.\n\nT <- 150\nz <- impulse_responses(C, D, Omega, labels, T)\nresponse_plot(z, \"Impulse responses: Taylor rule\")\n\n\n\n\n\n\n\n\nThis is clearly exploding. But it’s rational – we’re solving forward so expectations are always fulfilled. This is a key the insight of the early rational expectations modellers – rational isn’t enough, non-explosive is necessary too. Fortunately we know how to find this."
  },
  {
    "objectID": "BK.html#bk80",
    "href": "BK.html#bk80",
    "title": "6  Linear rational expectations models",
    "section": "6.3 Blanchard and Kahn (1980)",
    "text": "6.3 Blanchard and Kahn (1980)\nTo solve this model to give a unique stable rational expectations equilibrium, we appeal to the following. Consider the eigenvalue decomposition \\[\n  MC=\\Lambda M\n\\] where \\(\\Lambda\\) is a diagonal matrix of eigenvalues in increasing absolute value and \\(M\\) is a non-singular matrix of left eigenvectors. Note that computer routines (including the one in R) usually calculate right eigenvectors such that \\(CV=V\\Lambda\\) and that \\(M=V^{-1}\\), so be aware of this in what follows.\nWe can diagonalise \\(C\\) and write it as \\(C=M^{-1}\\Lambda M\\). So pre-multiplying the reduced form model by \\(M\\) gives \\[\nM \\begin{bmatrix} z_t \\\\ x_{t+1}^e \\end{bmatrix} = \\Lambda M \\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix} + M D \\varepsilon_t\n\\] Blanchard and Kahn (1980) (following Vaughan (1970)) show uniqueness requires as many unstable eigenvalues as jump variables. To see this, define \\[\n\\begin{bmatrix} \\xi_{t-1}^{s} \\\\  \\xi_t^{u} \\end{bmatrix}\n  =  \\begin{bmatrix} M_{11} & M_{12} \\\\  M_{21} & M_{22} \\end{bmatrix}\n      \\begin{bmatrix} z_{t-1} \\\\  x_t \\end{bmatrix}\n\\] Write the normalized model as \\[\n\\begin{bmatrix} \\xi_t^s \\\\ \\xi_{t+1}^u \\end{bmatrix}\n= \\begin{bmatrix} \\Lambda_s & 0 \\\\ 0 & \\Lambda_u \\end{bmatrix}\n\\begin{bmatrix} \\xi_{t-1}^s \\\\ \\xi_t^u \\end{bmatrix} +\n\\begin{bmatrix} M_1 \\\\ M_2 \\end{bmatrix} D\\varepsilon_t\n\\] where the eigenvalues are split into stable (\\(\\Lambda_s\\)) and unstable (\\(\\Lambda_u\\)). If we ignore the stochastic bit for a moment \\[\n\\begin{bmatrix} \\xi_t^s \\\\ \\xi_{t+1}^u \\end{bmatrix}\n= \\begin{bmatrix} \\Lambda_s & 0 \\\\ 0 & \\Lambda_u \\end{bmatrix}\n\\begin{bmatrix} \\xi_{t-1}^s \\\\ \\xi_t^u \\end{bmatrix}\n\\]\nWe seek a non-explosive solution, and this turns out to be easy to find using the following\n\nThe dynamics of \\(\\xi_t^u\\) are determined by \\(\\Lambda_u\\) and nothing else;\nIf they don’t start at \\(0\\) they must explode;\nThis implies they must start at \\(0\\) and are always \\(0\\).\n\nThus the definition of the canonical variables necessarily implies \\[\n\\begin{bmatrix} \\xi_{t-1}^s \\\\  0 \\end{bmatrix}\n  =  \\begin{bmatrix} M_{11} & M_{12} \\\\  M_{21} & M_{22} \\end{bmatrix}\n      \\begin{bmatrix} z_{t-1} \\\\  x_t \\end{bmatrix}\n\\]\nFrom this it is clear that the jump variables themselves are only on the saddle path if \\[\n   M_{21} z_{t-1} + M_{22} x_t = 0\n\\]\nThe rational solution implies that the jump variables are linearly related to the predetermined ones through \\[\n\\begin{aligned}\nx_t &= -M_{22}^{-1} M_{21}z_{t-1} \\\\\n    &= N z_{t-1}\n\\end{aligned}\n\\] We’ll deal with the shocks in a moment.\n\n6.3.1 R code\nHow do we do this in R? First, find the eigenvalue decomposition of \\(C\\) using\n\nm <- eigen(C, symmetric=FALSE)\n\nwhich yields\n\n\neigen() decomposition\n$values\n[1] 1.0715518+0.092734i 1.0715518-0.092734i 0.9000000+0.000000i\n[4] 0.8000000+0.000000i 0.6548762+0.000000i\n\n$vectors\n                      [,1]                  [,2]         [,3]           [,4]\n[1,]  0.0000000+0.0000000i  0.0000000+0.0000000i 0.2854942+0i  0.00000000+0i\n[2,]  0.0000000+0.0000000i  0.0000000+0.0000000i 0.0000000+0i  0.09783896+0i\n[3,]  0.1599159-0.5089425i  0.1599159+0.5089425i 0.7830500+0i  0.49622464+0i\n[4,] -0.6991064+0.0000000i -0.6991064+0.0000000i 0.4552131+0i -0.86012270+0i\n[5,]  0.2629802-0.3968579i  0.2629802+0.3968579i 0.3132200+0i  0.06616328+0i\n              [,5]\n[1,]  0.0000000+0i\n[2,]  0.0000000+0i\n[3,]  0.6351203+0i\n[4,] -0.7554249+0i\n[5,] -0.1611069+0i\n\n\nHowever this calculates right eigenvectors. We will need to invert it for left ones. Given the number of jump variables in the model satisfies the Blanchard-Kahn conditions of as many unstable roots (1.072+0.093i, 1.072-0.093i) as jump variables (2) we can calculate the reaction function from the eigenvectors\n\niz <- 1:np\nix <- (np+1):ns\nM  <- solve(m$vectors[,ns:1])        # Invert & reverse order for increasing abs value\nN  <- -Re(solve(M[ix,ix], M[ix,iz])) # Drop tiny complex bits (if any)\n\nwhere iz are the indices of the first np variables and ix those of the remaining nf ones.\n\n\n6.3.2 Stochastic part\nWhat about the shocks? Assume the stochastic reaction function is \\[\n  x_t = N z_{t-1} + G \\varepsilon_t\n\\] Following Blake (2004), note that \\(x_{t+1}^e = N z_t\\) as the expected value of \\(\\varepsilon_{t+1}=0\\), meaning we can write \\[\nNz_t = C_{21}z_{t-1} + C_{22} x_t + D_2 \\varepsilon_t\n\\] or \\[\nN\\left( C_{11}z_{t-1} + C_{12}x_t + D_1 \\varepsilon_t\\right) = C_{21}z_{t-1} + C_{22} x_t + D_2 \\varepsilon_t\n\\] Gathering terms we obtain \\[\n  (C_{22} - N C_{12}) x_t = (NC_{11} - C_{21}) z_{t-1} + (N D_1 - D_2) \\varepsilon_t\n\\] which implies \\[\nG=(C_{22} - N C_{12})^{-1}(N D_1 - D_2)\n\\] Notice it also implies \\(N = (C_{22} - N C_{12})^{-1}(NC_{11} - C_{21})\\). It is this fixed point nature of the solution for \\(N\\) – which in turn implies the quadratic matrix equation \\(C_{21} = NC_{11} - C_{22}N + N C_{12}N\\) – that means we need to use the Blanchard and Kahn (1980) method in the first place.\n\n\n6.3.3 R code\nAll of this means that\n\nG <- solve((C[ix,ix] - N %*% C[iz,ix]), (N %*% D[iz,]- D[ix,]))\n\nso for our model and parameters \\(N\\) and \\(G\\) are\n\nN\n\n        [,1]      [,2]       [,3]\n[1,] 4.85680 -2.758647 -1.1894200\n[2,] 1.79286  1.962790 -0.2536635\n\nG\n\n         [,1]      [,2]      [,3]\n[1,] 5.396445 -3.448309 -1.585893\n[2,] 1.992067  2.453488 -0.338218\n\n\nThe ‘fixed point’ check is that the following should be the same as \\(N\\)\n\nsolve((C[ix,ix] - N %*% C[iz,ix]), (N %*% C[iz,iz]- C[ix,iz]))\n\n        [,1]      [,2]       [,3]\n[1,] 4.85680 -2.758647 -1.1894200\n[2,] 1.79286  1.962790 -0.2536635\n\n\nwhich it is.\nThe solved model is finally \\[\n\\begin{align}\n\\begin{bmatrix} z_t \\\\ x_t \\end{bmatrix} &= \\begin{bmatrix} C_{11}+C_{12}N & 0 \\\\ N & 0 \\end{bmatrix} \\begin{bmatrix} z_{t-1} \\\\ x_{t-1} \\end{bmatrix} + \\begin{bmatrix} D_1+C_{12}G \\\\ G \\end{bmatrix} \\varepsilon_t \\\\\n&= P \\begin{bmatrix} z_{t-1} \\\\ x_{t-1} \\end{bmatrix} + Q \\varepsilon_t\n\\end{align}\n\\] which can be coded as\n\nP  <- cbind(rbind((C[iz,iz] + C[iz,ix] %*% N), N), matrix(0,ns,nf))\nQ  <- rbind(D[iz,] + C[iz,ix] %*% G, G)\n\n\n\n6.3.4 Digression – right eigenvector version\nIt turns out that we could use the output from the standard eigenvalue/vector routine directly by exploiting the following. This time, let \\(M\\) be the matrix of right eigenvectors so \\[\n  C M = M \\Lambda \\text{  or  } C = M\\Lambda M^{-1}\n\\]\nand \\[\n\\begin{bmatrix} M_{11} & M_{12} \\\\ M_{21} & M_{22} \\end{bmatrix}\n\\begin{bmatrix} \\xi_{t-1}^s \\\\ \\xi_t^u \\end{bmatrix}\n=\n\\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix}\n\\]\nWritten this way around, if \\(\\xi_t^{u}=0\\) \\(\\forall\\ t\\) then (again ignoring stochastics)\n\\[\n  M_{11} \\xi_{t-1}^s = z_{t-1}, \\ M_{21}\\xi_t^s = x_t\n\\]\n\\[\n   \\Rightarrow x_t = M_{21} M_{11}^{-1} z_t\n\\] so\n\nM <- m$vectors[,ns:1]            # Don't invert as already right vectors, but reorder\nRe(M[ix,iz] %*% solve(M[iz,iz])) # Again, drop tiny complex bits\n\n        [,1]      [,2]       [,3]\n[1,] 4.85680 -2.758647 -1.1894200\n[2,] 1.79286  1.962790 -0.2536635\n\n\nThe result is identical. This method is particularly useful if there are fewer predetermined variables than jumps as the matrix we need to invert is of the same dimension as the predetermined variables this way round.\n\n\n6.3.5 Impulse responses\nWe now call the impulse response function using the model solved for rational expectations.\n\nT <- 25\nz <- impulse_responses(P, Q, Omega, labels, T)\n\nNow plot these responses\n\nresponse_plot(z, \"Impulse responses: Taylor rule\")\n\n\n\n\n\n\n\n\nNow, that looks better! It is no longer explosive. It also makes complete economic sense, which you can verify by going through the dynamics of the different demand, supply and monetary shocks."
  },
  {
    "objectID": "BK.html#generalized-solution",
    "href": "BK.html#generalized-solution",
    "title": "6  Linear rational expectations models",
    "section": "6.4 Generalized solution",
    "text": "6.4 Generalized solution\nSometimes for a model \\(E\\) is singular. A more general solution was proposed by Klein (2000), that doesn’t require \\(E\\) to be non-singular. This uses a generalized Schur decomposition instead of an eigenvalue one and is applied to the structural model represented by the matrix pencil \\((A,E)\\), and is considered much more numerically stable (see Pappas, Laub, and Sandell (1980)). The generalized Schur form of \\((A,E)\\) is \\((QTZ', QSZ')\\), so we can write the model as \\[\nE \\begin{bmatrix} z_t \\\\ x_{t+1}^e \\end{bmatrix} \\equiv QTZ' \\begin{bmatrix} z_t \\\\ x_{t+1}^e \\end{bmatrix} \\equiv QT \\begin{bmatrix} \\xi_t^s \\\\ \\xi_{t+1}^u \\end{bmatrix}\n\\] and \\[\nA \\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix} \\equiv QSZ' \\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix} \\equiv QS\\begin{bmatrix} \\xi_{t-1}^s \\\\ \\xi_t^u \\end{bmatrix}\n\\] so the model pre-multiplied by \\(Q'\\) is \\[\nT \\begin{bmatrix} \\xi_{t+1}^s \\\\ \\xi_{t+1}^u \\end{bmatrix} = S \\begin{bmatrix} \\xi_t^s \\\\ \\xi_t^u \\end{bmatrix} + Q'B\\varepsilon_t\n\\]\nWe use the function gqz from the library geigen for this\n\nd <- geigen::gqz(A, E, sort=\"S\") # Option \"S\" puts the stable roots first\n\nWe can check that this is actually saddle path using gevalues() to get all the eigenvalues from the generalized Schur decomposition, and the unstable ones are\n\ne <- geigen::gevalues(d)\ne[abs(e) > 1]\n\n[1] 1.071552+0.092734i 1.071552-0.092734i\n\n\nThe number of stable roots is returned in d$sdim which is 3.\nWe then modify our solution function to calculate Ns and Gs using the matrix Z and a generalized version of the formula for \\(G\\) and calculate the reduced form model Ps and `Q which are \\[\n\\begin{align}\n    N_s &= Z_{21} Z_{11}^{-1} \\\\\n    H   &= (E_{11} + E_{12} N_s)^{-1} \\\\\n    W   &= (E_{21} + E_{22} N_s) H\\\\\n    G_s &= (A_{22} - W A_{12})^{-1} (W B_1 - B_2) \\\\\n    P_s &= H (A_{11} + A_{12} N_s) \\\\\n    Q_s &= H (B_1 + A_{12} G_s)\n\\end{align}\n\\] Verify this yourself with a bit of matrix algebra!\nThe R code for this is\n\nsolveGenBK <- function(E,A,B,n) {\n  d  <- geigen::gqz(A, E, sort=\"S\") \n  np <- d$sdim\n  ns <- nrow(E)\n  print(paste(\"Number of unstable roots is\", ns-np))\n  if (n == np) {\n    iz <- 1:n\n    ix <- (n+1):ns\n    Ns <- d$Z[ix,iz] %*% solve(d$Z[iz,iz])\n    H  <- solve(E[iz,iz] + E[iz,ix] %*% Ns)\n    W  <- (E[ix,iz] + E[ix,ix] %*% Ns) %*% H\n    Gs <- solve((A[ix,ix] - W %*% A[iz,ix]), (W %*% B[iz,] - B[ix,]))\n    As <- H %*% (A[iz,iz] + A[iz,ix] %*% Ns)\n    Bs <- H %*% (B[iz,] + A[iz,ix] %*% Gs)\n    return(list(P=cbind(rbind(As,Ns),matrix(0,ns,ns-n)), Q=rbind(Bs, Gs)))\n    } \n  else { \n    return(-1) \n    }\n}\n\nUsing this on our original model gives\n\nS  <- solveGenBK(E,A,B,np)\n\n[1] \"Number of unstable roots is 2\"\n\nPs <- S$P\nQs <- S$Q\n\nand comparing Ps and Qs with P and Q obtained using Blanchard-Kahn we find\n\nround(max(abs(P-Ps), abs(Q-Qs)), 12)\n\n[1] 0\n\n\nThey are, as expected, the same – at least up to 12 decimal places, which should be enough."
  },
  {
    "objectID": "BK.html#singular-models-optimal-policy",
    "href": "BK.html#singular-models-optimal-policy",
    "title": "6  Linear rational expectations models",
    "section": "6.5 Singular models: optimal policy",
    "text": "6.5 Singular models: optimal policy\nHowever, this is an easy test. What we need is to use a model that can’t be solved using the BK method. Under optimal policy, the interest rate instrument rule is replaced with a targeting rule, so that \\[\n  \\pi_t = -\\mu \\Delta y_t - \\varepsilon^3_t\n\\] for some value of \\(\\mu\\) that reflects the optimal trade-off between output (gap) growth and inflation, and we’ve included a disturbance which we can loosely describe as a monetary policy shock. We modify the model above by dropping the Taylor rule in favour of the targeting rule. This requires a lagged value of \\(y\\) to be created. The following does the trick\n\nnf <- 2\nne <- 3\nns <- 6      # One extra state\nnp <- ns-nf\nmu <- 0.75   # Representative trade-off\n\nlabels <- c(\"e^1\",\"e^2\",\"ylag\",\"i\",\"y\",\"pi\") # New variable order\n\nE <- matrix(0,ns,ns)\nA <- E\nB <- matrix(0,ns,ne)\nB[1,1] <- 1\nB[2,2] <- 1\nB[4,3] <- -1\n\ndiag(E[1:3,1:3]) <- 1\ndiag(A[1:2,1:2]) <- c(rho_1, rho_2)\nA[3,5]           <- 1\n\nE[4,3]           <- 1\nA[4,c(3, 6)]     <- c(1, -1/mu)\n\nE[5,c(1, 4, 5, 6)] <- c(1, -1/sigma, 1, 1/sigma)\nA[5,5]           <- 1\n\nE[6,c(2, 6)]     <- c(1, beta)\nA[6,c(5, 6)]     <- c(-kappa, 1)\n\nThe new \\(E\\) and \\(A\\) system matrices are then \\[  \nE = \\left[\\begin{array}{r}1 &0 &0 &0 &0 &0 \\\\0 &1 &0 &0 &0 &0 \\\\0 &0 &1 &0 &0 &0 \\\\0 &0 &1 &0 &0 &0 \\\\1 &0 &0 &-0.5 &1 &0.5 \\\\0 &1 &0 &0 &0 &0.99 \\\\\\end{array}\\right]\n\\] \\[  \nA = \\left[\\begin{array}{r}0.9 &0 &0 &0 &0 &0 \\\\0 &0.8 &0 &0 &0 &0 \\\\0 &0 &0 &0 &1 &0 \\\\0 &0 &1 &0 &0 &-1.333 \\\\0 &0 &0 &0 &1 &0 \\\\0 &0 &0 &0 &-0.075 &1 \\\\\\end{array}\\right]\n\\] Now we have a singular model. The matrix \\(E\\) is clearly singular as rows 3 and 4 are identical. But we have a problem using the code above. To use it we need the matrices \\(H\\) and \\((A_{22} - W A_{21})\\) to be non-singular. What to do?\nThere are two ways out. Klein (2000) gives a solution that depends on the decomposed matrix pencil, which is what is typically implemented, but you don’t actually need it although it is easiest. Instead, all you need to do is reorder the equations.\nThe real problem is that with a targeting rule that doesn’t include the interest rate, and the interest rate is now only determined by the IS curve. But we can swap the location of any two rows of the model arbitrarily. If we swap the positions of the equations for the IS curve and the targeting rule (rows 4 and 5) using the following\n\nE[4:5,] <- E[5:4,]\nA[4:5,] <- A[5:4,]\nB[4:5,] <- B[5:4,]\n\nthen the model is unchanged but now we have \\[  \nE_{11} = \\left[\\begin{array}{r}1 &0 &0 &0 \\\\0 &1 &0 &0 \\\\0 &0 &1 &0 \\\\1 &0 &0 &-0.5 \\\\\\end{array}\\right]\n\\] so \\(E_{11} + E_{12}N\\) is likely non-singular (it is). Also, note after the re-ordering \\(A_{22}\\) is \\[  \nA_{22} = \\left[\\begin{array}{r}0 &-1.33 \\\\-0.07 &1 \\\\\\end{array}\\right]\n\\] which is guaranteed non-singular for zero \\(W\\). We can now proceed as before. First, check for saddle path stability\n\ne <- geigen::gevalues(geigen::gqz(A, E, sort=\"S\"))\ne[abs(e) > 1]\n\n[1] 1.378195      Inf\n\n\nwhich confirms that it has a unique saddle path stable solution. This is\n\nSo <- solveGenBK(E,A,B,np)\n\n[1] \"Number of unstable roots is 2\"\n\nPo <- So$P\nQo <- So$Q\n\nThe solved model is then\n\nPo\n\n     [,1]      [,2]          [,3] [,4] [,5] [,6]\n[1,]  0.9  0.000000  0.000000e+00    0    0    0\n[2,]  0.0  0.800000  6.986592e-17    0    0    0\n[3,]  0.0 -1.863455  7.329156e-01    0    0    0\n[4,]  1.8 -1.241330 -2.446879e-01    0    0    0\n[5,]  0.0 -1.863455  7.329156e-01    0    0    0\n[6,]  0.0  1.397591  2.003133e-01    0    0    0\n\nQo\n\n     [,1]      [,2]          [,3]\n[1,]    1  0.000000  0.000000e+00\n[2,]    0  1.000000 -6.986592e-17\n[3,]    0 -2.329318 -7.329156e-01\n[4,]    2 -1.551663  2.446879e-01\n[5,]    0 -2.329318 -7.329156e-01\n[6,]    0  1.746989 -2.003133e-01\n\n\n\n6.5.1 Optimal impulse responses\nWe can now simulate the model under optimal policy and plot using\n\nzo <- impulse_responses(Po, Qo, Omega, labels, T) %>%\n  select(-ylag) # Drop duplicate series\nresponse_plot(zo, \"Impulse responses: Optimal policy\")"
  },
  {
    "objectID": "BK.html#dummy-jumps",
    "href": "BK.html#dummy-jumps",
    "title": "6  Linear rational expectations models",
    "section": "6.6 Dummy jumps",
    "text": "6.6 Dummy jumps\nBut this isn’t the only way to get this to work. Effectively what we just did was create an extra predetermined variable and reorder the system to give us non-singularity. What if instead of including an unused \\(i_{t-1}\\) on the right hand side, we instead include an unused \\(i^e_{t+1}\\) on the left hand side? So we swap to having one more jump variable, one less predetermined one?\nCompare the following to the previous model. When we pick out the interest rate we do so on the right hand side of the matrix equation, not the left as before.\n\nns <- 6      # One extra state\nnf <- 3      # And one extra jump\nnp <- ns-nf\nlabels <- c(\"e^1\",\"e^2\",\"ylag\",\"i\",\"y\",\"pi\") # New variable order\n\nE <- matrix(0,ns,ns)\nA <- E\nB <- matrix(0,ns,ne)\nB[1,1] <- 1\nB[2,2] <- 1\nB[4,3] <- -1\n\ndiag(E[1:3,1:3]) <- 1\ndiag(A[1:2,1:2]) <- c(rho_1, rho_2)\nA[3,5]           <- 1\n\nE[4,3]           <- 1\nA[4,c(3, 6)]     <- c(1, -1/mu)\n\nE[5,c(1, 5, 6)]  <- c(1, 1, 1/sigma) # One less coefficient\nA[5,c(4, 5)]     <- c(1/sigma, 1)    # One more - nothing else changes\n\nE[6,c(2, 6)]     <- c(1, beta)\nA[6,c(5, 6)]     <- c(-kappa, 1)\n\nThis is still a singular model, as we can see from \\[  \nE = \\left[\\begin{array}{r}1 &0 &0 &0 &0 &0 \\\\0 &1 &0 &0 &0 &0 \\\\0 &0 &1 &0 &0 &0 \\\\0 &0 &1 &0 &0 &0 \\\\1 &0 &0 &0 &1 &0.5 \\\\0 &1 &0 &0 &0 &0.99 \\\\\\end{array}\\right]\n\\] with column 4 all zeros. Is this model saddle path stable?\n\ne <- geigen::gevalues(geigen::gqz(A, E, sort=\"S\") )\ne[abs(e) > 1]\n\n[1]     -Inf 1.378195      Inf\n\n\nAgain, it is with an extra unstable root for the extra jump variable. We could simplify the solution. As that top left 3 by 3 block, \\(E_{11}\\), is the identity matrix and \\(E_{12}\\) is all zeros this Ei is always an identity matrix. However, here we simply re-use solveGenBG\n\nSo2 <- solveGenBK(E,A,B,np)\n\n[1] \"Number of unstable roots is 3\"\n\nPo2 <- So2$P\nQo2 <- So2$Q\n\nNow the solved model is\n\nPo2\n\n     [,1]      [,2]       [,3] [,4] [,5] [,6]\n[1,]  0.9  0.000000  0.0000000    0    0    0\n[2,]  0.0  0.800000  0.0000000    0    0    0\n[3,]  0.0 -1.863455  0.7329156    0    0    0\n[4,]  1.8 -1.241330 -0.2446879    0    0    0\n[5,]  0.0 -1.863455  0.7329156    0    0    0\n[6,]  0.0  1.397591  0.2003133    0    0    0\n\nQo2\n\n     [,1]      [,2]       [,3]\n[1,]    1  0.000000  0.0000000\n[2,]    0  1.000000  0.0000000\n[3,]    0 -2.329318 -0.7329156\n[4,]    2 -1.551663  0.2446879\n[5,]    0 -2.329318 -0.7329156\n[6,]    0  1.746989 -0.2003133\n\n\nwhich is actually identical to our previous solution. This is because I have preserved the order of the solved-out variables, and shows that the swap from a predetermined to a jump variable is completely arbitrary."
  },
  {
    "objectID": "BK.html#substituting-out",
    "href": "BK.html#substituting-out",
    "title": "6  Linear rational expectations models",
    "section": "6.7 Substituting out",
    "text": "6.7 Substituting out\nBut even this doesn’t exhaust the possible re-parametrisations of the model. We can reduce the number of jump variables to 1 and find the same solution. There exist formal methods for reducing models (see King and Watson (2002)) but there is an obvious way to proceed here. From the targeting rule, it must be that \\[\n  y^e_{t+1} = y_t - \\frac{1}{\\mu}\\pi^e_{t+1}\n\\] as the expected shock is zero. This means the IS curve can be rewritten \\[\ny_t = y_t - \\frac{1}{\\mu}\\pi^e_{t+1} - \\frac{1}{\\sigma} \\left (i_t - \\pi_{t+1}^e \\right ) + e_t^1\n\\] implying \\[\ni_t =  \\left (1 - \\frac{\\sigma}{\\mu} \\right )\\pi_{t+1}^e + \\sigma e_t^1\n\\] This is the required interest rate consistent with the targeting rule holding. Now the only jump variable is the inflation rate as we have eliminated the expected output gap. \\[\n\\begin{aligned}\ny_t    &= y_{t-1} -\\frac{1}{\\mu} \\pi_t  + \\frac{1}{\\mu} \\varepsilon^3_t \\\\\n\\pi_t  &= \\beta \\pi_{t+1}^e + \\kappa y_t + e_t^2 \\\\\ni_t    &= \\left (1 - \\frac{\\sigma}{\\mu} \\right ) \\pi_{t+1}^e + \\sigma e_t^1 \\\\\ne_t^1  &= \\rho_1 e_{t-1}^1 + \\varepsilon_t^1 \\\\\ne_t^2  &= \\rho_2 e_{t-1}^2 + \\varepsilon_t^2\n\\end{aligned}\n\\]\nWe can code this\n\nns <- 5      # Back to 5 states\nnf <- 1      # Now only one jump\nnp <- ns-nf\n\nlabels <- c(\"e^1\",\"e^2\",\"i\",\"y\",\"pi\") # Lose a y\n\nE <- matrix(0,ns,ns)\nA <- E\nB <- matrix(0,ns,ne)\nB[1,1] <- 1\nB[2,2] <- 1\nB[4,3] <- -1\n\ndiag(E[1:4,1:4]) <- 1\ndiag(A[1:2,1:2]) <- c(rho_1, rho_2)\n\nE[3,c(1, 3, 5)]  <- c(-sigma, 1, sigma/mu-1)\n\nA[4,c(4,5)]      <- c(1, -1/mu)\n\nE[5,c(2, 4, 5)]  <- c(1, kappa, beta)\nA[5,5]           <- 1\n\nand solve it using\n\nSs <- solveGenBK(E,A,B,np)\n\n[1] \"Number of unstable roots is 1\"\n\nPs <- Ss$P\nQs <- Ss$Q\n\nCompare the realized of Ps\n\nPs\n\n     [,1]      [,2] [,3]       [,4] [,5]\n[1,]  0.9  0.000000    0  0.0000000    0\n[2,]  0.0  0.800000    0  0.0000000    0\n[3,]  1.8 -1.241330    0 -0.2446879    0\n[4,]  0.0 -1.863455    0  0.7329156    0\n[5,]  0.0  1.397591    0  0.2003133    0\n\n\nwith Po above, say. This is the most ‘efficient’ way of programming the model, in that we have only five states, and indeed the repeated behavioural equations we had before have disappeared in the reduced form solution. Just to confirm this, simulating and plotting this version gives\n\nresponse_plot(impulse_responses(Ps,Qs,Omega,labels,T), \"Optimal, substituted out\")\n\n\n\n\n\n\n\n\nwhich are identical results to those above. But of course \\(E\\) is now invertible so we could solve this using the simplest Blanchard-Kahn variant. Try it!\n\n\n\n\nBlake, Andrew P. 2004. “Analytic Derivatives in Linear Rational Expectations Models.” Computational Economics 24 (1): 77–96.\n\n\nBlanchard, O., and C. Kahn. 1980. “The Solution of Linear Difference Models Under Rational Expectations.” Econometrica 48 (5): 1305–11.\n\n\nKing, Robert G., and Mark W. Watson. 2002. “System Reduction and Solution Algorithms for Singular Linear Difference Systems Under Rational Expectations.” Computational Economics 20 (1–2): 57–86.\n\n\nKlein, Paul. 2000. “Using the Generalized Schur Form to Solve a Multivariate Linear Rational Expectations Model.” Journal of Economic Dynamics and Control 24 (10): 1405–23.\n\n\nPappas, T., A. J. Laub, and N. R. Sandell. 1980. “On the Numerical Solution of the Discrete-Time Algebraic Riccati Equation.” IEEE Transaction on Automatic Control AC-25.4: 631–41.\n\n\nVaughan, D. R. 1970. “A Non Recursive Algorithm Solution for the Discrete Riccati Equation.” IEEE Transactions on Automatic Control AC-15.5: 597–99."
  },
  {
    "objectID": "BVAR.html#estimating-bvars-using-us-data",
    "href": "BVAR.html#estimating-bvars-using-us-data",
    "title": "7  BVAR with dummies",
    "section": "7.1 Estimating BVARs using US data",
    "text": "7.1 Estimating BVARs using US data\nWe will the Fed Funds rate, annual GDP growth and annual CPI inflation data from FRED, retrieved 2023-05-23. These are:\n\n\n\n\n\n\n\n\n\nWe will build a variety and two and three variable BVARs. More details on the data are given below."
  },
  {
    "objectID": "BVAR.html#bvars-with-dummy-variable-priors",
    "href": "BVAR.html#bvars-with-dummy-variable-priors",
    "title": "7  BVAR with dummies",
    "section": "7.2 BVARs with dummy variable priors",
    "text": "7.2 BVARs with dummy variable priors\nRather than combine a prior distribution with a likelihood and draw from the resulting joint posterior distribution there is another convenient way of parameterizing the problem. We can instead add some ‘dummy variables’ that have the same properties of the prior so we have a single modified likelihood that incorporates the prior information. This approach was most obviously adopted by Banbura, Giannone, and Reichlin (2010). Further discussion of this can be found in Giannone, Lenza, and Primiceri (2015). In general this is a version of the Theil and Goldberger (1961) mixed estimator given a Bayesian interpretation.\n\n7.2.1 VAR model\nSimple bi-variate two-lag VAR model: \\[\n  \\left[\\matrix{g_t \\cr \\pi_t}\\right] =\n  \\left[\\matrix{c_1 \\cr c_2}\\right] +\n  \\left[\\matrix{b_{11} & b_{12} \\cr\n    b_{21} & b_{22}}\\right]\n\\left[\\matrix{g_{t-1} \\cr \\pi_{t-1} }\\right] +\n   \\left[\\matrix{d_{11} & d_{12} \\cr\n    d_{21} & d_{22}}\\right]\n\\left[\\matrix{g_{t-2} \\cr \\pi_{t-2} }\\right] +\n  \\left[\\matrix{\\nu_{g,t} \\cr \\nu_{\\pi,t}}\\right]\n\\] \\[\n  \\left[\\matrix{\\nu_{g,t} \\cr \\nu_{\\pi,t}}\\right] \\sim N(0, \\Sigma)\n\\]"
  },
  {
    "objectID": "BVAR.html#bvar-hyperparameters",
    "href": "BVAR.html#bvar-hyperparameters",
    "title": "7  BVAR with dummies",
    "section": "7.3 BVAR hyperparameters",
    "text": "7.3 BVAR hyperparameters\nWe will (similarly to the straightforward Minnesota prior) need some control parameters:\n\n\\(\\tau\\) controls the overall tightness of the prior for the AR coefficients\n\\(d\\) controls the prior on higher lags;\n\\(\\lambda\\) controls the prior on constants;\n\\(\\gamma\\) controls the prior on the sum of coefficients;\n\\(\\delta\\) controls the cointegration prior;\n\nwhere\n\n\\(\\sigma_i\\) standard deviation of error terms from individual OLS regressions;\n\\(\\mu_i\\) sample means of the data.\n\n\n7.3.0.1 First lag\nNow consider the following artificial data for the first lag. We construct some dummy observations of the dependent and explanatory variables that look like: \\[\n  Y_{D,1} = \\left[\\matrix{\\frac{1}{\\tau}\\sigma_1 & 0 \\cr\n    0 & \\frac{1}{\\tau}\\sigma_2}\\right]\n\\] and \\[\n  X_{D,1} = \\left [ \\matrix{0 & \\frac{1}{\\tau}\\sigma_1 & 0 & 0 & 0\\cr\n    0 & 0 & \\frac{1}{\\tau}\\sigma_2 & 0 & 0}\\right]\n\\] Intuition: \\[\n  \\left[\\matrix{\\frac{\\sigma_1}{\\tau} & 0 \\cr\n    0 & \\frac{\\sigma_2}{\\tau}}\\right] =\n  \\left[\\matrix{0 & \\frac{\\sigma_1}{\\tau} & 0 & 0 & 0\\cr\n   0 & 0 & \\frac{\\sigma_2}{\\tau} & 0 & 0} \\right]\n\\left[\\matrix{c_1    & c_2 \\cr\n  b_{11} & b_{21} \\cr\n  b_{12} & b_{22} \\cr\n  d_{11} & d_{21} \\cr\n  d_{12} & d_{22}}\\right]\n+\n  \\left[\\matrix{\\xi_{11} & \\xi_{12} \\cr \\xi_{21} & \\xi_{22} }\\right]\n\\] Multiplying out we get: \\[\n  \\left[\\matrix{\\frac{\\sigma_1}{\\tau} & 0 \\cr\n    0 & \\frac{\\sigma_2}{\\tau}}\\right]\n=\n  \\left[\\matrix{\\frac{\\sigma_1}{\\tau}b_{11} &   \\frac{\\sigma_1}{\\tau}b_{21}\\cr\n  \\frac{\\sigma_2}{\\tau}b_{12} &  \\frac{\\sigma_2}{\\tau}b_{22}} \\right]\n+\n  \\left[\\matrix{\\xi_{11} & \\xi_{12} \\cr \\xi_{21} & \\xi_{22} }\\right]\n\\] Concentrating on the first row, notice: \\[\n  \\frac{\\sigma_1}{\\tau} = \\frac{\\sigma_1}{\\tau}b_{11} + \\xi_{11}\n\\] implying: \\[\n  b_{11} = 1 - \\frac{\\tau}{\\sigma_1}\\xi_{11}\n\\] so we can write: \\[\n  b_{11} \\sim N\\left(1, \\frac{\\tau^2var(\\xi_{11})}{\\sigma^2_1}\\right)\n\\] as \\(E[b_{11}] = 1 - \\frac{\\tau}{\\sigma_1}E[\\xi_{11}] = 1\\) and the variance is easily derived. Similarly: \\[\n  b_{12} = - \\frac{\\tau}{\\sigma_1}\\xi_{12}\n\\] which is clearly zero in expectation.\n\n\n7.3.1 Further priors\n\n7.3.1.1 Higher lags\nRather than derive the implications we state the rest of the dummy priors. Consider the following artificial data for the second lag: \\[\nY_{D,2} = \\left[\\matrix{0 & 0 \\cr 0 & 0}\\right]\n\\] and: \\[\nX_{D,2} = \\left [ \\matrix{0 & 0 & 0 & \\frac{\\sigma_1 2^d}{\\tau} & 0 \\cr\n    0 & 0 & 0 & 0 & \\frac{\\sigma_2 2^d}{\\tau} }\\right]\n\\] We can multiply these out and check the properties, in particular we can verify in the same way as for the first lag that: \\[\n  b_{ji} \\sim N\\left(0, \\frac{1}{4}\\frac{\\tau^2var(\\xi_{ji})} {2^d\\sigma^2_j}\\right)\n\\] for \\(j=1,...N\\), \\(i=1,...l\\).\n\n\n7.3.1.2 Constant\nConsider the following artificial data for the constant: \\[\n  Y_{D,3} = \\left[\\matrix{0 & 0 }\\right]\n\\] \\[\n  X_{D,3} = \\left [ \\matrix{\\lambda & 0 & 0 & 0 & 0 }\\right]\n\\] so \\(\\lambda c_1 = \\varepsilon_1\\) and \\(\\lambda c_2 = \\varepsilon_2\\). As \\(\\lambda \\rightarrow \\infty\\) the prior is implemented more tightly.\n\n\n7.3.1.3 Covariances\nDummy observations to implement the prior on the error covariance matrix are: \\[\n  Y_{D,4} = \\left[\\matrix{\\sigma_1 & 0 \\cr 0 & \\sigma_2}\\right]\n\\] and \\[\n  X_{D,4} = \\left [ \\matrix{0 & 0 & 0 & 0 & 0 \\cr\n    0 & 0 & 0 & 0 & 0 }\\right]\n\\] with the magnitude of the diagonal elements of \\(\\Sigma\\) controlled by the scale of the diagonal elements of \\(Y_{D,4}\\), as larger diagonal elements implement the prior belief that the variance of \\(\\nu_1\\) and \\(\\nu_2\\) is larger.\nBanbura, Giannone, and Reichlin (2010) stop here, but there are additional priors that could be added.\n\n\n7.3.1.4 Sum of coefficients\nWe could add a prior that reflects the belief that the sum of coefficients on ‘own’ lags add up to 1. This is an additional ‘unit root’-style prior. Consider: \\[\n  Y_{D,5} = \\left[\\matrix{\\gamma\\mu_1 & 0\\cr 0 & \\gamma\\mu_2}\\right]\n\\] and \\[\n  X_{D,5} = \\left [ \\matrix{0 & \\gamma\\mu_1 & 0 & \\gamma\\mu_1 & 0\\cr\n    0 & 0 & \\gamma\\mu_2 & 0 & \\gamma\\mu_2}\\right]\n\\] where \\(\\mu_1\\) is the sample mean of \\(y_t\\) and \\(\\mu_2\\) is the sample mean of \\(x_t\\). Note that these dummy observations imply prior means of the form \\(b_{ii} + d_{ii} = 1\\) where \\(i = 1, 2\\) and \\(\\gamma\\) controls the tightness of the prior. As \\(\\gamma \\rightarrow \\infty\\) the prior is implemented more tightly. Forecast growth rates eventually converge to their sample averages.\n\n\n7.3.1.5 Trends\nWe can also specify common stochastic trend dummies: \\[\n  Y_{D,6} = \\left[\\matrix{\\delta\\mu_1 & \\delta\\mu_2 }\\right]\n\\] and \\[\n  X_{D,6} = \\left [ \\matrix{\\delta & \\delta\\mu_1 & \\delta\\mu_2 & \\delta\\mu_1 & \\delta\\mu_2 }\\right]\n\\] where this imposes that the coefficients are consistent with limiting the amount of drift between the predictions at their average values.\n\n\n\n7.3.2 Implementation\nThe data and the artificial data are now stacked: \\[\n  Y^* = \\left[\\matrix{ g_3 & \\pi_3 \\cr\n    \\vdots & \\vdots \\cr\n    g_T & \\pi_T \\cr\n    \\frac{1}{\\tau}\\sigma_1 & 0 \\cr\n    0 & \\frac{1}{\\tau}\\sigma_2\\cr\n    0 & 0 \\cr\n    0 & 0 \\cr\n    0 & 0 \\cr\n    \\sigma_1 & 0 \\cr\n    0 & \\sigma_2 \\cr\n    \\gamma\\mu_1 & 0 \\cr\n    0 & \\gamma\\mu_2 \\cr\n    \\delta\\mu_1 & \\delta\\mu_2 }\\right], \\quad\nX^* = \\left [ \\matrix{1 & g_2 & \\pi_2 & g_1 & \\pi_1 \\cr\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\cr\n  1 & g_{T-1} & \\pi_{T-1} & g_{T-2} & \\pi_{T-2} \\cr\n  0 & \\frac{1}{\\tau}\\sigma_1 & 0 & 0 & 0\\cr\n  0 & 0 & \\frac{1}{\\tau}\\sigma_2 & 0 & 0\\cr\n  0 & 0 & 0 & \\frac{\\sigma_1 2^d}{\\tau} & 0 \\cr\n  0 & 0 & 0 & 0 & \\frac{\\sigma_2 2^d}{\\tau} \\cr\n  \\lambda & 0 & 0 & 0 & 0 \\cr\n  0 & 0 & 0 & 0 & 0 \\cr\n  0 & 0 & 0 & 0 & 0 \\cr\n  0 & \\gamma\\mu_1 & 0 & \\gamma\\mu_1 & 0 \\cr\n  0 & 0 & \\gamma\\mu_2 & 0 & \\gamma\\mu_2 \\cr\n  \\delta & \\delta\\mu_1 & \\delta\\mu_2 & \\delta\\mu_1 & \\delta\\mu_2 }\\right]\n\\] Estimation via Gibbs sampling now proceeds in a very straightforward way. There is no need to draw for the prior separately."
  },
  {
    "objectID": "BVAR.html#examples",
    "href": "BVAR.html#examples",
    "title": "7  BVAR with dummies",
    "section": "7.4 Examples",
    "text": "7.4 Examples\nFirst we use quarterly US Growth (FRED series A191RO1Q156NBEA) and CPI (FRED series CPALTT01USQ661S) expressed as the annual inflation rate from 1961-01-01 to 2023-01-01 in a bi-variate BVAR. The last ten observations are:\n\n\n\n\n\nDate\nGrowth\nInflation\n\n\n\n\n2020-10-01\n-1.5\n1.224176\n\n\n2021-01-01\n1.2\n1.905310\n\n\n2021-04-01\n12.5\n4.776278\n\n\n2021-07-01\n5.0\n5.264633\n\n\n2021-10-01\n5.7\n6.765892\n\n\n2022-01-01\n3.7\n8.023109\n\n\n2022-04-01\n1.8\n8.556077\n\n\n2022-07-01\n1.9\n8.284860\n\n\n2022-10-01\n0.9\n7.110821\n\n\n2023-01-01\n1.6\n5.769521\n\n\n\n\n\nWe specify a VAR with two lags, and use it to forecast 12 periods ahead. The BVAR are specified using the names above, with only tau particularly binding in this case. We set the total number of iterations in each case to 20000 and discard the first half. The parameter nb is used to set how much back data should appear in a fan chart.\n\n#########\n# Options\n#########\nnf <- 12 # Max forecast horizon\nnb <- 21 # No. back periods plotted in graphs\nl  <- 2  # Number of lags in VAR\n\n# specify parameters of the Minnesota-type prior\ntau    <- .1   # controls prior on own 1st lags (1 makes wibbly)\nd      <- 1    # decay for higher lags\nlambda <- 1    # prior for the constant\ngamma  <- 1    # sum of coefficients unit roots\ndelta  <- 1    # cointegration prior\n\n# Gibbs control\nreps <- 20000 # total numbers of Gibbs iterations\nburn <- 10000 # number of burn-in iterations\n\nIn what follows we vary tau and the lag length to illustrate their effects. To do this we create the augmented data and then run the Gibbs sampler, using:\n\n# Create augmented data\nYplus <- augmentData(Y, l, tau, d, lambda, gamma, delta)\n\n# Run Gibbs sampler\nout   <- Gibbs_estimate(Yplus[[1]], Yplus[[2]], reps, burn, 1, nf)\n\nwhere Y contains the data in a dataframe/tibble with the date in the first column as in the data example above. The code strips out the date and then uses the remaining \\(N\\) columns in the BVAR. See the Code Appendix for the details of the functions.\n\n\n\nThe output contains any forecast draws from the Gibbs sampler in the third list element from the Gibbs_estimate() function. The first two elements are coefficient draws. Two further functions plots the fan charts using the Gibbs draws:\n\n# String to put in subtitle\ncontrols <- paste0(\"Lag length \", l, \": tau=\", tau, \", d=\", d,\n                   \", lambda=\", lambda, \", gamma=\", gamma, \", delta=\", delta)\n\n# Plots\nfan_chart(Y, out[[3]], controls, nb)\np           <- coeff_plot(Y, l, out[[1]], out[[2]], 333, controls)\npnum        <- pnum+1\npce[[pnum]] <- p[[1]]\n\nwhere the string controls is put in the chart subtitle and the coefficient densities. It can be anything but is a good place to remind yourself of how you specified the model. Notice we save the coefficient plots for later use.\n\n7.4.0.1 Example 1: BVAR(2) with \\(\\tau=0.1\\)\n\n\n\n\n\n\n\n\n\n\n\n7.4.0.2 Example 2: BVAR(2) with \\(\\tau=1\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.4.0.3 Example 3: BVAR(6) with \\(\\tau=0.1\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.4.0.4 Example 4: BVAR(6) with \\(\\tau=1\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.4.0.5 Coefficient estimates\nAll of these have underlying parameters. Their estimated posterior densities are:"
  },
  {
    "objectID": "BVAR.html#tri-variate-bvar",
    "href": "BVAR.html#tri-variate-bvar",
    "title": "7  BVAR with dummies",
    "section": "7.5 Tri-variate BVAR",
    "text": "7.5 Tri-variate BVAR\nNow we add the FedFunds rate (FRED series FEDFUNDS), so the last ten periods of the data set is now:\n\n\n\n\n\nDate\nGrowth\nInflation\nFedFunds\n\n\n\n\n2020-10-01\n-1.5\n1.224176\n0.09\n\n\n2021-01-01\n1.2\n1.905310\n0.09\n\n\n2021-04-01\n12.5\n4.776278\n0.07\n\n\n2021-07-01\n5.0\n5.264633\n0.10\n\n\n2021-10-01\n5.7\n6.765892\n0.08\n\n\n2022-01-01\n3.7\n8.023109\n0.08\n\n\n2022-04-01\n1.8\n8.556077\n0.33\n\n\n2022-07-01\n1.9\n8.284860\n1.68\n\n\n2022-10-01\n0.9\n7.110821\n3.08\n\n\n2023-01-01\n1.6\n5.769521\n4.33\n\n\n\n\n\nTwo more examples follow.\n\n7.5.0.1 Example 5: BVAR(4) with \\(\\tau=.1\\), 3 variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.5.0.2 Example 6: BVAR(6) with \\(\\tau=1\\), 3 variables"
  },
  {
    "objectID": "BVAR.html#code-appendix",
    "href": "BVAR.html#code-appendix",
    "title": "7  BVAR with dummies",
    "section": "7.6 Code appendix",
    "text": "7.6 Code appendix\nYou can download the program and functions used for the estimates above from the links below. Put them in the same directory and they should recreate exactly (within sampling error) the same graphs as above. Ensure you have all the libraries available that are loaded at the top of BVARdum.R.\nMain program:\n\n\nDownload BVARdum.R\n\n\nFunctions:\n\n\nDownload BVARdumFUNCs.R\n\n\n\n\n\n\nBanbura, M., D. Giannone, and L. Reichlin. 2010. “Large Bayesian Vector Auto Regressions.” Journal of Applied Econometrics 25 (1): 71–92.\n\n\nGiannone, Domenico, Michele Lenza, and Giorgio E. Primiceri. 2015. “Prior Selection for Vector Autoregressions.” The Review of Economics and Statistics 97 (2): 436–51.\n\n\nTheil, Henri, and Arthur S. Goldberger. 1961. “On Pure and Mixed Statistical Estimation in Economics.” International Economic Review 2: 317–32."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "10  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever…"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bahaj, Saleem, Angus Foulis, and Gabor Pinter. 2020. “Home Values\nand Firm Behavior.” American Economic Review 110 (7):\n2225–70. https://doi.org/10.1257/aer.20180649.\n\n\nBanbura, M., D. Giannone, and L. Reichlin. 2010. “Large\nBayesian Vector Auto Regressions.” Journal of\nApplied Econometrics 25 (1): 71–92.\n\n\nBlake, Andrew P. 2004. “Analytic Derivatives in Linear Rational\nExpectations Models.” Computational Economics 24 (1):\n77–96.\n\n\nBlake, Andrew P, and Haroon Mumtaz. 2017. Applied Bayesian\nEconometrics for Central Bankers. Revised. Technical Books. Centre\nfor Central Banking Studies, Bank of England. https://www.bankofengland.co.uk/-/media/boe/files/ccbs/resources/applied-bayesian-econometrics-for-central-bankers-updated-2017.pdf.\n\n\nBlanchard, O., and C. Kahn. 1980. “The Solution of Linear\nDifference Models Under Rational Expectations.”\nEconometrica 48 (5): 1305–11.\n\n\nBoehmke, Brad, and Brandon M. Greenwell. 2019. Hands-on Machine\nLearning with R. The R Series. Boca\nRaton: Chapman & Hall/CRC. https://bradleyboehmke.github.io/HOML/.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. New\nHaven & London: Yale University Press. https://mixtape.scunning.com/.\n\n\nDurbin, J., and S. J. Koopman. 2001. Time Series Analysis by State\nSpace Methods. Oxford: Oxford University Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2019. Regression and\nOther Stories. Cambridge: Cambridge University Press. http://www.stat.columbia.edu/~gelman/regression.\n\n\nGiannone, Domenico, Michele Lenza, and Giorgio E. Primiceri. 2015.\n“Prior Selection for Vector\nAutoregressions.” The Review of Economics and\nStatistics 97 (2): 436–51.\n\n\nHamilton, J. D. 1994. Time Series Analysis. Princeton, NJ:\nPrinceton University Press.\n\n\nHarvey, Andrew C. 1989. Forecasting, Structural Time Series Models\nand the Kalman Filter. Cambridge: Cambridge University Press.\n\n\nHarvey, Andrew C., and Richard G. Pierse. 1984. “Estimating\nMissing Observations in Economic Time Series.” Journal of the\nAmerican Statistical Association 79 (385): 125–31.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The\nElements of Statistical Learning: Data Mining, Inference, and\nPrediction. 2nd ed. New York, NY: Springer. https://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning\nfor Text Analysis in R. Chapman & Hall: CRC Press.\nhttps://smltar.com/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical Learning: With Applications in\nR. 2nd ed. Springer Texts in Statistics. New York, NY:\nSpringer. https://www.statlearning.com/.\n\n\nJazwinski, Andrew H. 1970. Stochastic Processes and Filtering\nTheory. Mineola, N.Y.: Dover Publications Inc.\n\n\nKalman, R. E. 1960. “A New Approach to Linear Filtering and\nPrediction Problems.” Transactions of the ASME Journal of\nBasic Engineering 82 (Series D): 35–45.\n\n\nKim, Chang-Jin, and Charles R. Nelson. 1999. State-Space Models with\nRegime Switching: Classical and Gibbs-Sampling Approaches with\nApplications. MIT Press.\n\n\nKing, Robert G., and Mark W. Watson. 2002. “System Reduction and\nSolution Algorithms for Singular Linear Difference Systems Under\nRational Expectations.” Computational Economics 20\n(1–2): 57–86.\n\n\nKlein, Paul. 2000. “Using the Generalized Schur Form\nto Solve a Multivariate Linear Rational Expectations Model.”\nJournal of Economic Dynamics and Control 24 (10): 1405–23.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019.\nGeocomputation with R. 1st ed. The R\nSeries. Boca Raton: Chapman & Hall/CRC. https://geocompr.robinlovelace.net/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in R and Stan. 2nd ed. Abingdon,\nOxfordshire: CRC Press. https://github.com/rmcelreath/rethinking.\n\n\nPappas, T., A. J. Laub, and N. R. Sandell. 1980. “On the Numerical\nSolution of the Discrete-Time Algebraic Riccati Equation.”\nIEEE Transaction on Automatic Control AC-25.4: 631–41.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal\nInference in Statistics: A Primer. Chichester: John Wiley &\nSons. http://bayes.cs.ucla.edu/PRIMER/.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with\nR: A Tidy Approach. O’Reilly. https://www.tidytextmining.com/.\n\n\nTaddy, Matt. 2019. Business Data Science: Combining Machine Learning\nand Economics to Optimize, Automate, and Accelerate Business\nDecisions. New York, NY: McGraw-Hill Education. https://github.com/TaddyLab/BDS.\n\n\nTheil, Henri, and Arthur S. Goldberger. 1961. “On Pure and Mixed\nStatistical Estimation in Economics.” International Economic\nReview 2: 317–32.\n\n\nTriantafyllopoulos, Kostas. 2021. Bayesian Inference of State Space\nModels: Kalman Filtering and Beyond. Springer Texts in Statistics.\nCham, Switzerland: Springer.\n\n\nVaughan, D. R. 1970. “A Non Recursive Algorithm Solution for the\nDiscrete Riccati Equation.” IEEE Transactions on Automatic\nControl AC-15.5: 597–99.\n\n\nYong, Laurel Harbridge, Jon A. Krosnick, and Jeffrey M. Wooldridge.\n2016. “Presidential Approval and Gas Prices: Sociotropic or\nPocketbook Influence?” In Political Psychology, edited\nby Jon A. Krosnick, I-Chant A. Chiang, and Tobias H. Stark, 246–75.\nTaylor; Francis Inc."
  },
  {
    "objectID": "Appendix1.html#ggplot2",
    "href": "Appendix1.html#ggplot2",
    "title": "Appendix A — Appendix: ggplot2",
    "section": "A.1 ggplot2",
    "text": "A.1 ggplot2\n\nKey part of the tidyverse – for many the only part\nBuilds a grammar of graphics\nSimple rules that drive you mad until you get it\nProcess\n\nInitiate a plot using ggplot\nSpecify aesthetics which indicate what you want to plot\nCall a geom to say how you want to plot it\nAdd modifiers to change how it looks"
  },
  {
    "objectID": "Appendix1.html#example",
    "href": "Appendix1.html#example",
    "title": "Appendix A — Appendix: ggplot2",
    "section": "A.2 Example",
    "text": "A.2 Example\n\nTake the Wooldridge data set approval from Yong, Krosnick, and Wooldridge (2016)\nDo a little wrangling\n\nProduce some quite nice plots\n\nStart with libraries and retrieve data.\n\nlibrary(tidyverse)\nlibrary(wooldridge)\ndata(\"approval\")\n\nThis looks like:\n\nhead(approval) \n\n   id month year   sp500   cpi cpifood approve gasprice unemploy katrina\n1 302     2 2001 1239.94 184.4   171.8   59.24    148.4      4.6       0\n2 303     3 2001 1160.33 185.3   172.2   57.01    144.7      4.5       0\n3 304     4 2001 1249.46 185.6   172.4   60.31    156.4      4.2       0\n4 305     5 2001 1255.82 185.5   172.9   55.82    172.9      4.1       0\n5 306     6 2001 1224.42 185.9   173.4   54.93    164.0      4.7       0\n6 307     7 2001 1211.23 186.2   174.0   56.36    148.2      4.7       0\n  rgasprice lrgasprice X11.Sep iraqinvade   lsp500 lcpifood\n1  80.47723   4.387974       0          0 7.122818 5.146331\n2  78.08958   4.357857       0          0 7.056460 5.148656\n3  84.26724   4.433993       0          0 7.130467 5.149817\n4  93.20755   4.534829       0          0 7.135544 5.152713\n5  88.21947   4.479828       0          0 7.110222 5.155601\n6  79.59184   4.376912       0          0 7.099391 5.159055\n\n\n\nA.2.1 Scatter plot\nA first scatter plot, using geom_point of food against gas (petrol) prices\n\nggplot(approval, aes(x=lcpifood, y=lrgasprice)) +    # Initiate, set aesthetics\n  geom_point()                                       # Display as points\n\n\n\n\nOK, I guess, a bit dull – so I add some colour. This time I specify aes in the geom - either is fine, some advantages either way.\n\nggplot(approval) +\n  geom_point(aes(x=lcpifood, y=lrgasprice, color=month))  # Colours by month\n\n\n\n\nBetter, but how about…\n\nggplot(approval) +\n  geom_point(aes(x=lcpifood, y=lrgasprice, color=approve), size=2, shape=17) + # Colours by popularity!\n  scale_color_gradient(low=\"red\", high=\"green\") \n\n\n\n\nA categorical variable (a factor) is needed to get different actual colours, otherwise for a continuous variable I get shades of one color or a continuous change.\n\nggplot(approval) +\n  geom_point(aes(x=lcpifood, y=lrgasprice, color=as.factor(month), size=as.factor(year)))\n\nWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n\nA.2.2 Time series plots\nOur time index is a bit odd as the data set has year and month separately. Create a proper date series using:\n\napproval %<>% \n  unite(date, year, month, sep=\"/\") %>% \n  mutate(date = as.Date(paste0(date,\"/01\"), \"%Y/%m/%d\"))\n\nI’ve used the %<>% pipe operator to send and get back approval so this is now\n\n\n   id       date   sp500   cpi cpifood approve gasprice unemploy katrina\n1 302 2001-02-01 1239.94 184.4   171.8   59.24    148.4      4.6       0\n2 303 2001-03-01 1160.33 185.3   172.2   57.01    144.7      4.5       0\n3 304 2001-04-01 1249.46 185.6   172.4   60.31    156.4      4.2       0\n4 305 2001-05-01 1255.82 185.5   172.9   55.82    172.9      4.1       0\n5 306 2001-06-01 1224.42 185.9   173.4   54.93    164.0      4.7       0\n6 307 2001-07-01 1211.23 186.2   174.0   56.36    148.2      4.7       0\n  rgasprice lrgasprice X11.Sep iraqinvade   lsp500 lcpifood\n1  80.47723   4.387974       0          0 7.122818 5.146331\n2  78.08958   4.357857       0          0 7.056460 5.148656\n3  84.26724   4.433993       0          0 7.130467 5.149817\n4  93.20755   4.534829       0          0 7.135544 5.152713\n5  88.21947   4.479828       0          0 7.110222 5.155601\n6  79.59184   4.376912       0          0 7.099391 5.159055\n\n\nThen I can plot a couple of series using two calls to geom_line\n\nggplot(approval) +\n  geom_line(aes(x=date, y=unemploy), colour=\"red\") +\n  geom_line(aes(x=date, y=cpi), colour=\"blue\") \n\n\n\n\nBut this is pretty inefficient, as I would need a call to geom_line for every series I wanted to plot and even then scales are unsuitable. Plus the labels are not right.\nThis is where things really get interesting. I pivot_longer all the variables into a single column.\n\ndf <- pivot_longer(approval, cols=-c(date, id), names_to= \"Var\", values_to = \"Val\")\nhead(df)\n\n# A tibble: 6 × 4\n     id date       Var         Val\n  <int> <date>     <chr>     <dbl>\n1   302 2001-02-01 sp500    1240. \n2   302 2001-02-01 cpi       184. \n3   302 2001-02-01 cpifood   172. \n4   302 2001-02-01 approve    59.2\n5   302 2001-02-01 gasprice  148. \n6   302 2001-02-01 unemploy    4.6\n\n\nGreat! Now I can plot Val using one call to geom_line. This time, put the graph object into p and then explicitly plot it.\n\np  <- ggplot(df) +\n  geom_line(aes(x=date, y=Val))\nplot(p)\n\n\n\n\nOops! I need to tell ggplot2 to separate out the variables which are stored in Var. For this, use group:\n\np  <- ggplot(df) +\n  geom_line(aes(x=date, y=Val, group=Var))\nplot(p)\n\n\n\n\nBut this could better be done by using an aesthetic like colour which implies group\n\np  <- ggplot(df) +\n  geom_line(aes(x=date, y=Val, colour=Var))\nplot(p)\n\n\n\n\nOK, but can I plot them so we can see what’s going on, like in a grid? This is where facet comes in.\n\np  <- p +\n  facet_wrap(~Var, scales = \"free\")\nplot(p)\n\n\n\n\nA bit more formatting…\n\np  <- p +\n  theme_minimal() + \n  labs(title=\"Facet plots\", x=\"\", y=\"\")\nplot(p)\n\n\n\n\nFinally all in one go, dropping the dummies, don’t store as an object. Also no legend, as series labelled in the facets. And I call a rather handy little function geom_smooth which fits (by default) a Loess smoothing line.\n\napproval %>% \n  select(-iraqinvade, -katrina, -X11.Sep) %>%\n  pivot_longer(cols=-c(date, id), names_to=\"Var\", values_to=\"Val\") %>%\n  ggplot(aes(x=date, y=Val, group=Var, colour=Var)) +\n  geom_line() +\n  geom_smooth() + # Smoother\n  facet_wrap(~Var, scales = \"free\") +\n  theme_minimal() + \n  theme(legend.position = \"none\") +\n  labs(title=\"Facet plots\", x=\"\", y=\"\")\n\n\n\n\nCool, huh?\n\n\n\n\nYong, Laurel Harbridge, Jon A. Krosnick, and Jeffrey M. Wooldridge. 2016. “Presidential Approval and Gas Prices: Sociotropic or Pocketbook Influence?” In Political Psychology, edited by Jon A. Krosnick, I-Chant A. Chiang, and Tobias H. Stark, 246–75. Taylor; Francis Inc."
  },
  {
    "objectID": "index.html#sec-genesis",
    "href": "index.html#sec-genesis",
    "title": "Quantiles, Networks, Time",
    "section": "Genesis",
    "text": "Genesis\nThis book owes it origin to an initiative the Bank of England began in the early 1990s, and a prescient one. This was at a major historical turning point, one that signalled a burgeoning new world order, as the Iron Curtain crumbled, the European experiment gathered momentum, and industrial might continued an inexorable shift eastwards.\n\n\n\n\n\n\nPuppet, Yogyakarta\n\n\n\n\n\n\n\nAbu Dhabi, United Arab Emirates\n\n\n\n\n\n\n\n\n\nVilla Sterne, Pretoria\n\n\n\n\n\n\n\nDowntown, Seoul\n\n\n\n\n\n\n\n\nMontevideo\n\n\nPlaceholders abound.\n\n\n\n\n\n\nDisclaimer\n\n\n\nThe Bank of England does not accept any liability for misleading or inaccurate information or omissions in the information provided. The subject matter reflects the views of the author and not the wider Bank of England or its Policy Committees."
  },
  {
    "objectID": "index.html#sec-disclaimer",
    "href": "index.html#sec-disclaimer",
    "title": "Quantiles, Networks, Time",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe Bank of England does not accept any liability for misleading or inaccurate information or omissions in the information provided. The subject matter reflects the views of the individual presenter and not the wider Bank of England or its Policy Committees."
  },
  {
    "objectID": "Kalman.html#literature",
    "href": "Kalman.html#literature",
    "title": "3  Kalman filtering",
    "section": "3.1 Literature",
    "text": "3.1 Literature\nAlternatives to what follows can be found in Harvey (1989), Hamilton (1994), Kim and Nelson (1999), Durbin and Koopman (2001) or Triantafyllopoulos (2021). There are many books devoted to the Kalman filter as a causal Amazon search mostly from an engineering perspective. However econometricians since Harvey and Pierse (1984) have used it in a way somewhat different from standard engineering applications. We will cover the filter and then look at a simple example if filtering, then develop a maximum likelihood estimation approach."
  },
  {
    "objectID": "Kalman.html#reminder-of-a-state-space-model",
    "href": "Kalman.html#reminder-of-a-state-space-model",
    "title": "3  Kalman filtering",
    "section": "3.2 Reminder of a state-space model",
    "text": "3.2 Reminder of a state-space model\nConsider the trend-cycle model \\[\n  y_t = \\chi_t + \\tau_t + \\varepsilon_t\n\\] where the cycle equation is \\[\n\\chi_t = c+\\rho_1 \\chi_{t-1} + \\rho_2 \\chi_{t-2}+v_{1t}\n\\] and the trend equation is \\[\n\\tau_t = \\tau_{t-1} + v_{2t}\n\\] In state space this can be written \\[\n\\begin{align}\ny_t &=\n\\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix}\n\\begin{bmatrix} \\chi_t \\\\ \\chi_{t-1} \\\\ \\tau_t \\end{bmatrix} + [1] \\varepsilon_t\\\\\n\\begin{bmatrix} \\chi_t \\\\ \\chi_{t-1} \\\\ \\tau_t \\end{bmatrix} &=\n\\begin{bmatrix} c \\\\ 0 \\\\ 0 \\end{bmatrix} +\n\\begin{bmatrix} \\rho_1 & \\rho_2 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n\\begin{bmatrix} \\chi_{t-1} \\\\ \\chi_{t-2} \\\\ \\tau_{t-1} \\end{bmatrix} +\n\\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\\\ 0 & 1 \\end{bmatrix}\n\\begin{bmatrix} v_{1t} \\\\ v_{2t} \\end{bmatrix}\n\\end{align}\n\\]\n\n3.2.1 A useful class of models\nWe need a framework that nests this type of model (and many more). State-space models are one such framework, amenable to classical (and Bayesian) estimation. Quite a lot of apparatus required before we can apply maximum likelihood.\nKey points:\n\nMany quantities routinely used to build models and analyse policy are unobservable.\nEconometricians face a major problem estimating such models: everything unobserved needs estimating simultaneously (states and parameters).\nFortunately there is a method we can use: the Kalman filter (Kalman (1960)).\nHas the useful spin-off that we can also use it to calculate the value of the likelihood function."
  },
  {
    "objectID": "Kalman.html#the-filter",
    "href": "Kalman.html#the-filter",
    "title": "3  Kalman filtering",
    "section": "3.4 The filter",
    "text": "3.4 The filter\nFiltering is specifically this: perhaps we have some estimates already of \\(\\beta_t\\) for \\(t=1...t-1\\), then given the new period-\\(t\\) observation of \\(y_t\\) how should we estimate a new value of \\(\\beta_t\\)? This immediately implies a recursive structure to estimation problems, consistent with on-line (or real-time) estimation. The Kalman filter uses the period \\(t\\) news available from observed \\(y_t\\) to update our estimates of \\(\\beta_t\\) using the regression lemma.\n\n3.4.1 Forecasting\nConsider the simple first-order VAR model \\[\n  \\beta_t = F\\beta_{t-1}+v_{t},\\quad v_t\\sim N(0,Q)\n\\] We can use to make the conditional forecast \\[\n  \\beta_{t|t-1} = F\\beta_{t-1}\n\\] where \\(\\beta_{t|t-1}=E[\\beta_t|\\psi_{t-1}]\\) and \\(\\psi_{t-1}\\) is the information set available at time \\(t-1\\).\nIf \\(\\psi_{t-1}\\) includes \\(\\beta_{t-1}\\) we can straightforwardly forecast next period (and the next-but-one period etc) using the model. This is a standard forecasting exercise given any estimated (or even calibrated) economic model.\n\n\n3.4.2 Uncertainty\nHow can we assess the associated forecast uncertainty? The forecast covariance \\(P_{t|t-1} = var(\\beta_t|\\psi_{t-1})\\) is given by \\[\n\\begin{align}\nP_{t|t-1} &= E\\left[ (\\beta_t - \\beta_{t|t-1})(\\beta_t-\\beta_{t|t-1})'\\right]   \\\\\n  &= E\\left[ (F\\beta_{t-1}+v_t-F\\beta_{t-1|t-1}) \\left(\\beta_{t-1}'F'+v_t'-\\beta_{t-1|t-1}'F'\\right) \\right]  \\notag \\\\\n&= E\\left[ F\\left( \\beta _{t-1}-\\beta _{t-1|t-1}\\right) \\left( \\beta_{t-1}-\\beta _{t-1|t-1}\\mu_z \\right) ' F' \\right] + E[v_t v_t']  \\notag \\\\\n&= FP_{t-1|t-1}F' + Q\n\\end{align}\n\\] where \\(P_{t-1|t-1} = var(\\beta_{t-1}|\\psi_{t-1})\\)\n\nThe forecast error variance depends on the previous error variance; that value depends on the information set\n\n\n\n3.4.3 Uncertainty\nIf \\(\\beta_{t-1}\\) forms part of the information set \\(\\psi_{t-1}\\) then \\[\nP_{t-1|t-1} = var \\left(\\beta_{t-1}|\\psi_{t-1}\\right) = 0\n\\] and there is no uncertainty other than from the disturbance terms and \\(P_t=Q\\).\nIf \\(\\beta_{t-1}\\) does not form part of the information set \\(\\psi_{t-1}\\) but \\(\\beta _{t-2}\\) does then \\(P_{t-1|t-1}=Q\\) and \\(P_{t|t-1}=FQF'+Q\\). This can be continued backwards; the unconditional (steady-state) covariance of \\(\\beta_t\\) is the limit \\(P=FPF'+Q\\). We can easily calculate error bands for \\(\\beta_t\\) using the appropriate information set.\n\n\n3.4.4 Prediction error\nWe can turn this around, as it must be the prediction errors are given by \\[\n\\begin{aligned}\n\\eta_{t|t-1} &= y_t - E[y_t|\\psi_{t-1}]  \\\\\n             &= y_t - E[H_t\\beta_t+e_t|\\psi_{t-1}] \\\\\n             &= y_t-H_t\\beta_{t|t-1}\n\\end{aligned}            \n\\] where \\(\\eta_{t|t-1}\\) is uncorrelated with \\(\\psi_{t-1}\\) . So the ‘news’ over that contained in \\(y_t\\) above \\(\\psi_{t-1}\\) is captured by \\(\\eta_{t|t-1}\\). It will be that \\(\\eta_{t|t-1}\\sim N(0,\\Sigma_{\\eta\\eta})\\); we need to find an expression for the covariance.\n\n\n3.4.5 Current-data predictions\n\nNow we find \\(E [\\beta_t | \\psi_t]\\) – the best prediction of the unknown coefficient vector given current information\nUsing the regression lemma we know that\n\n\\[\n\\begin{align}\nE[\\beta_t | \\psi_t ] &= E[\\beta_t | \\psi_{t-1}, \\eta_{t|t-1} ]  \\\\\n&= E[\\beta_t | \\psi_{t-1}] +\\Sigma_{\\beta\\eta} \\Sigma_{\\eta\\eta}^{-1}\\eta_{t|t-1}  \\\\\n&= \\beta_{t|t-1} + \\Sigma_{\\beta\\eta}\\Sigma_{\\eta\\eta}^{-1} \\eta_{t|t-1}\n\\end{align}\n\\] because \\(\\psi_{t-1}\\) and \\(\\eta_{t|t-1}\\) are uncorrelated and \\(\\eta_{t|t-1}\\) is mean zero.\nSimilarly, we can find \\(P_{t|t}\\) as the best prediction of the variance of \\(\\beta_t\\) given \\(\\psi_t\\). Using the regression lemma we know that \\[\n\\begin{align}\nP_{t|t} &= E[(\\beta_t-\\beta_{t|t})(\\beta_t - \\beta_{t|t})'|\\psi_{t-1}, \\eta_{t|t-1}] \\\\\n        &= E[(\\beta_t - \\beta_{t|t})(\\beta_t-\\beta_{t|t})'|\\psi_{t-1}] - \\Sigma_{\\beta\\eta}\\Sigma_{\\eta\\eta}^{-1}\\Sigma_{\\eta\\beta} \\\\\n        &=P_{t|t-1}-\\Sigma_{\\beta\\eta} \\Sigma_{\\eta\\eta}^{-1} \\Sigma_{\\eta\\beta}\n\\end{align}\n\\]\n\n\n3.4.6 Estimated model covariances\nWhat is \\(\\Sigma_{\\beta\\eta}\\)? \\[\n\\begin{align}\n\\Sigma_{\\beta\\eta} &= E[(\\beta_t-\\beta_{t|t-1}) \\eta_{t|t-1}'] \\\\\n&= E\\left[(\\beta_t-\\beta_{t|t-1}) (y_t-H_t\\beta_{t|t-1})'\\right] \\\\\n&= E\\left[(\\beta_t-\\beta_{t|t-1}) (H_t\\beta_t+e_t-H_t\\beta_{t|t-1})'\\right] \\\\\n&= E[(\\beta_t-\\beta_{t|t-1}) (\\beta_t-\\beta_{t|t-1})'H_t'] + E[(\\beta_t - \\beta_{t|t-1}) e_t'] \\\\\n&= P_{t|t-1} H_t' \\tag{$\\Sigma_{\\beta\\eta}$}\n\\end{align}\n\\] as \\(E[(\\beta_t - \\beta_{t|t-1})e_t']=0\\).\n\n\n3.4.7 Estimated model covariances\nWhat is \\(\\Sigma_{\\eta\\eta}\\)? \\[\n\\begin{align}\n\\Sigma_{\\eta\\eta} &= E[(y_t-H_t \\beta_{t|t-1})(y_t-H_t\\beta_{t|t-1})'] \\\\\n                  &= E[(H_t\\beta_t+e_t-H_t\\beta_{t|t-1})(H_t\\beta_t+e_t-H_t\\beta_{t|t-1})'] \\\\\n                  &= E[(H_t\\beta_t-H_t \\beta_{t|t-1})(H_t\\beta_t-H_t\\beta_{t|t-1})'] + E[e_t e_t'] \\\\\n                  &= E[H_t(\\beta_t-\\beta_{t|t-1})(\\beta_t-\\beta_{t|t-1})'H_t'] + R \\\\\n                  &= H_t P_{t|t-1} H_t' + R \\\\\n                  &= f_{t|t-1} \\tag{$\\Sigma_{\\eta\\eta}$} \\label{svv}\n\\end{align}\n\\] where we define \\(f_{t|t-1}=E[\\eta_{t|t-1}\\eta_{t|t-1}']\\)"
  },
  {
    "objectID": "Kalman.html#the-filter-1",
    "href": "Kalman.html#the-filter-1",
    "title": "3  Kalman filtering",
    "section": "3.4 The filter",
    "text": "3.4 The filter\nFiltering is specifically this: perhaps we have some estimates already of \\(\\beta_t\\) for \\(t=1...t-1\\), then given the new period-\\(t\\) observation of \\(y_t\\) how should we estimate a new value of \\(\\beta_t\\)? This immediately implies a recursive structure to estimation problems, consistent with on-line (or real-time) estimation. The Kalman filter uses the period \\(t\\) news available from observed \\(y_t\\) to update our estimates of \\(\\beta_t\\) using the regression lemma.\n\n3.4.1 Forecasting\nConsider the simple first-order VAR model\n$$\nt = F{t-1}+v_{t},v_tN(0,Q)\n$$\n\nWe can use to make the conditional forecast\n\n\\[\n  \\beta_{t|t-1} = F\\beta_{t-1}\n\\]\nwhere \\(\\beta_{t|t-1}=E[\\beta_t|\\psi_{t-1}]\\) and \\(\\psi_{t-1}\\) is the information set available at time \\(t-1\\)\n\nIf \\(\\psi_{t-1}\\) includes \\(\\beta_{t-1}\\) we can straightforwardly forecast next period (and the next-but-one period etc) using the model\nThis is a standard forecasting exercise given any estimated (or even calibrated) economic model\n\n\n\n3.4.2 Uncertainty\n\nHow can we assess the associated forecast uncertainty?\nThe forecast covariance \\(P_{t|t-1} = var(\\beta_t|\\psi_{t-1})\\) is given by\n\n\\[\n\\begin{align}\nP_{t|t-1} &= E\\left[ (\\beta_t - \\beta_{t|t-1})(\\beta_t-\\beta_{t|t-1})'\\right]   \\\\\n  &= E\\left[ (F\\beta_{t-1}+v_t-F\\beta_{t-1|t-1}) \\left(\\beta_{t-1}'F'+v_t'-\\beta_{t-1|t-1}'F'\\right) \\right]  \\notag \\\\\n&= E\\left[ F\\left( \\beta _{t-1}-\\beta _{t-1|t-1}\\right) \\left( \\beta_{t-1}-\\beta _{t-1|t-1}\\mu_z \\right) ' F' \\right] + E[v_t v_t']  \\notag \\\\\n&= FP_{t-1|t-1}F' + Q\n\\end{align}\n\\] where \\(P_{t-1|t-1} = var(\\beta_{t-1}|\\psi_{t-1})\\)\n\nThe forecast error variance depends on the previous error variance; that value depends on the information set\n\n\n\n3.4.3 Uncertainty\nIf \\(\\beta_{t-1}\\) forms part of the information set \\(\\psi_{t-1}\\) then \\[\nP_{t-1|t-1} = var \\left(\\beta_{t-1}|\\psi_{t-1}\\right) = 0\n\\] and there is no uncertainty other than from the disturbance terms and \\(P_t=Q\\).\nIf \\(\\beta_{t-1}\\) does not form part of the information set \\(\\psi_{t-1}\\) but \\(\\beta _{t-2}\\) does then \\(P_{t-1|t-1}=Q\\) and \\(P_{t|t-1}=FQF'+Q\\). This can be continued backwards; the unconditional (steady-state) covariance of \\(\\beta_t\\) is the limit \\(P=FPF'+Q\\). We can easily calculate error bands for \\(\\beta_t\\) using the appropriate information set.\n\n\n3.4.4 Prediction error\nWe can turn this around, as it must be the prediction errors are given by \\[\n\\begin{aligned}\n\\eta_{t|t-1} &= y_t - E[y_t|\\psi_{t-1}]  \\\\\n             &= y_t - E[H_t\\beta_t+e_t|\\psi_{t-1}] \\\\\n             &= y_t-H_t\\beta_{t|t-1}\n\\end{aligned}            \n\\] where \\(\\eta_{t|t-1}\\) is uncorrelated with \\(\\psi_{t-1}\\) . So the ‘news’ over that contained in \\(y_t\\) above \\(\\psi_{t-1}\\) is captured by \\(\\eta_{t|t-1}\\). It will be that \\(\\eta_{t|t-1}\\sim N(0,\\Sigma_{\\eta\\eta})\\); we need to find an expression for the covariance.\n\n\n3.4.5 Current-data predictions\n\nNow we find \\(E [\\beta_t | \\psi_t]\\) – the best prediction of the unknown coefficient vector given current information\nUsing the regression lemma we know that\n\n\\[\n\\begin{align}\nE[\\beta_t | \\psi_t ] &= E[\\beta_t | \\psi_{t-1}, \\eta_{t|t-1} ]  \\\\\n&= E[\\beta_t | \\psi_{t-1}] +\\Sigma_{\\beta\\eta} \\Sigma_{\\eta\\eta}^{-1}\\eta_{t|t-1}  \\\\\n&= \\beta_{t|t-1} + \\Sigma_{\\beta\\eta}\\Sigma_{\\eta\\eta}^{-1} \\eta_{t|t-1}\n\\end{align}\n\\] because \\(\\psi_{t-1}\\) and \\(\\eta_{t|t-1}\\) are uncorrelated and \\(\\eta_{t|t-1}\\) is mean zero.\nSimilarly, we can find \\(P_{t|t}\\) as the best prediction of the variance of \\(\\beta_t\\) given \\(\\psi_t\\). Using the regression lemma we know that \\[\n\\begin{align}\nP_{t|t} &= E[(\\beta_t-\\beta_{t|t})(\\beta_t - \\beta_{t|t})'|\\psi_{t-1}, \\eta_{t|t-1}] \\\\\n        &= E[(\\beta_t - \\beta_{t|t})(\\beta_t-\\beta_{t|t})'|\\psi_{t-1}] - \\Sigma_{\\beta\\eta}\\Sigma_{\\eta\\eta}^{-1}\\Sigma_{\\eta\\beta} \\\\\n        &=P_{t|t-1}-\\Sigma_{\\beta\\eta} \\Sigma_{\\eta\\eta}^{-1} \\Sigma_{\\eta\\beta}\n\\end{align}\n\\]\n\n\n3.4.6 Estimated model covariances\nWhat is \\(\\Sigma_{\\beta\\eta}\\)? \\[\n\\begin{align}\n\\Sigma_{\\beta\\eta} &= E[(\\beta_t-\\beta_{t|t-1}) \\eta_{t|t-1}'] \\\\\n&= E\\left[(\\beta_t-\\beta_{t|t-1}) (y_t-H_t\\beta_{t|t-1})'\\right] \\\\\n&= E\\left[(\\beta_t-\\beta_{t|t-1}) (H_t\\beta_t+e_t-H_t\\beta_{t|t-1})'\\right] \\\\\n&= E[(\\beta_t-\\beta_{t|t-1}) (\\beta_t-\\beta_{t|t-1})'H_t'] + E[(\\beta_t - \\beta_{t|t-1}) e_t'] \\\\\n&= P_{t|t-1} H_t' \\tag{$\\Sigma_{\\beta\\eta}$}\n\\end{align}\n\\] as \\(E[(\\beta_t - \\beta_{t|t-1})e_t']=0\\).\n\n\n3.4.7 Estimated model covariances\nWhat is \\(\\Sigma_{\\eta\\eta}\\)? \\[\n\\begin{align}\n\\Sigma_{\\eta\\eta} &= E[(y_t-H_t \\beta_{t|t-1})(y_t-H_t\\beta_{t|t-1})'] \\\\\n                  &= E[(H_t\\beta_t+e_t-H_t\\beta_{t|t-1})(H_t\\beta_t+e_t-H_t\\beta_{t|t-1})'] \\\\\n                  &= E[(H_t\\beta_t-H_t \\beta_{t|t-1})(H_t\\beta_t-H_t\\beta_{t|t-1})'] + E[e_t e_t'] \\\\\n                  &= E[H_t(\\beta_t-\\beta_{t|t-1})(\\beta_t-\\beta_{t|t-1})'H_t'] + R \\\\\n                  &= H_t P_{t|t-1} H_t' + R \\\\\n                  &= f_{t|t-1} \\tag{$\\Sigma_{\\eta\\eta}$} \\label{svv}\n\\end{align}\n\\] where we define \\(f_{t|t-1}=E[\\eta_{t|t-1}\\eta_{t|t-1}']\\)"
  },
  {
    "objectID": "Kalman.html#the-kalman-filter",
    "href": "Kalman.html#the-kalman-filter",
    "title": "3  Kalman filtering",
    "section": "3.4 The Kalman filter",
    "text": "3.4 The Kalman filter\nThe equations of the filter are\n\nthe conditional expectation depending on \\(\\psi_{t-1}\\)\nan update that uses \\(\\eta_{t|t-1}\\) to obtain the best \\(t\\)-period prediction\n\nThese must be of the form \\[\n\\begin{aligned}\nE[\\beta_t | \\psi_{t-1}] &= \\mu + F E [\\beta_t | \\psi_{t-1}] \\\\\nE[P_t|\\psi_{t-1}]       &= F E[P_{t-1}|\\psi_{t-1}] F' + Q \\\\\nE[\\beta_t | \\psi_t]     &= E[\\beta_t|\\psi_{t-1}] + \\Sigma_{\\beta\\eta} \\Sigma_{\\eta\\eta}^{-1}\\eta_{t|t-1} \\\\\nE[P_t|\\psi_t]           &= E[P_t|\\psi_{t-1}] -\\Sigma_{\\beta\\eta}\\Sigma_{\\eta\\eta}^{-1}\\Sigma_{\\eta \\beta}\n\\end{aligned}\n\\] These are specifically \\[\n\\begin{align}\n\\beta_{t|t-1} &= \\mu +F\\beta_{t-1|t-1}                                  \\tag{Predicted $\\beta$}\\\\\nP_{t|t-1}     &= F P_{t-1|t-1}F' + Q                                    \\tag{Predicted $P$}\\\\\n\\eta_{t|t-1}  &= y_t-H_t\\beta_{t|t-1}                                   \\tag{Prediction error}\\\\\nf_{t|t-1}     &= H_t P_{t|t-1}H_t' + R                                  \\tag{Pred. err. variance}\\\\\n\\beta_{t|t}   &= \\beta_{t|t-1}+P_{t|t-1}H_t' f_{t|t-1}^{-1}\\eta_{t|t-1} \\tag{Updated $\\beta$}\\\\\nP_{t|t}       &= P_{t|t-1} - P_{t|t-1}H_t' f_{t|t-1}^{-1}H_tP_{t|t-1}   \\tag{Updated $P$}\n\\end{align}\n\\] The filter evaluates these recursively, beginning from \\(\\beta_0\\), \\(P_0\\)\nTreatment of these initial condition reflects knowledge/model - Stationary models can use steady-sate - Non-stationary models use something which is often (confusingly) called a diffuse prior (zero mean, large variance)\n\n3.4.1 Kalman filter trick\nFor known initial conditions – say \\(\\beta_{0}\\sim N(\\mu_0, P_0)\\) – the likelihood of a state-space model with \\(T\\) observations of \\(m\\) variables is \\[\n\\begin{eqnarray*}\n\\log L(y|\\theta) &=&\\sum_{t=1}^T \\log p \\left( y_t|\\psi_{t-1}, \\theta \\right) \\\\\n  &=& -\\Phi\n  - \\frac{1}{2}\\sum_{t=1}^T \\left( \\log \\det f_{t|t-1} + \\eta_{t|t-1}' f_{t|t-1}^{-1} \\eta_{t|t-1} \\ | \\ \\theta \\right)\n\\end{eqnarray*}\n\\] where \\(\\Phi =\\frac{Tm}{2}\\log \\left( 2\\pi \\right)\\) and \\(\\theta\\) are all the non-state parameters to be estimated. We can use the Kalman filter to obtain \\(\\eta_{t|t-1}\\) and \\(f_{t|t-1}\\) as they are the prediction error and its variance. This is the prediction error decomposition of the log-likelihood.\nA maximum likelihood estimate maximizes \\(\\log L(y|\\theta)\\) by choice of \\(\\theta\\).\n\n\n\n\nDurbin, J., and S. J. Koopman. 2001. Time Series Analysis by State Space Methods. Oxford: Oxford University Press.\n\n\nHamilton, J. D. 1994. Time Series Analysis. Princeton, NJ: Princeton University Press.\n\n\nHarvey, Andrew C. 1989. Forecasting, Structural Time Series Models and the Kalman Filter. Cambridge: Cambridge University Press.\n\n\nHarvey, Andrew C., and Richard G. Pierse. 1984. “Estimating Missing Observations in Economic Time Series.” Journal of the American Statistical Association 79 (385): 125–31.\n\n\nJazwinski, Andrew H. 1970. Stochastic Processes and Filtering Theory. Mineola, N.Y.: Dover Publications Inc.\n\n\nKalman, R. E. 1960. “A New Approach to Linear Filtering and Prediction Problems.” Transactions of the ASME Journal of Basic Engineering 82 (Series D): 35–45.\n\n\nKim, Chang-Jin, and Charles R. Nelson. 1999. State-Space Models with Regime Switching: Classical and Gibbs-Sampling Approaches with Applications. MIT Press.\n\n\nTriantafyllopoulos, Kostas. 2021. Bayesian Inference of State Space Models: Kalman Filtering and Beyond. Springer Texts in Statistics. Cham, Switzerland: Springer."
  },
  {
    "objectID": "Kalman.html#a-filter",
    "href": "Kalman.html#a-filter",
    "title": "3  Kalman filtering",
    "section": "3.3 A filter",
    "text": "3.3 A filter\nImagine we had a time-varying parameter model where we estimate \\(\\beta_t\\); this becomes a time series so we have a lot of parameters to estimate: we outline an estimation method here, the Kalman Filter.\nSimple observation and transition equations \\[\n\\begin{align}\ny_t     &= H_t\\beta_t + e_t, &var(e_t) = R \\\\\n\\beta_t &= \\mu + F \\beta_{t-1} + v_t, &var(v_t)=Q\n\\end{align}\n\\] where \\(\\beta_t\\) is a vector of stochastic variables, \\(y_t\\) a vector of measurements and the data forms an information set such that \\(\\psi_T = \\{y_T,y_{T-1},...,y_1\\}\\).\nJazwinski (1970) defines three type of estimation problem\n\nSmoothing is the problem of estimating \\(\\beta_k\\) for any \\(k<T\\)\nFiltering is the problem of estimating \\(\\beta_k\\) for \\(k=T\\)\nPrediction is the problem of estimating \\(\\beta_k\\) for any \\(k>T\\)\n\n\n“The object of filtering is to update our knowledge of the system each time a new observation \\(y_t\\) is brought in.” (Durbin and Koopman (2001))\n\nFiltering is specifically this: perhaps we have some estimates already of \\(\\beta_t\\) for \\(t=1...t-1\\), then given the new period-\\(t\\) observation of \\(y_t\\) how should we estimate a new value of \\(\\beta_t\\)? This immediately implies a recursive structure to estimation problems, consistent with on-line (or real-time) estimation. The Kalman filter uses the period \\(t\\) news available from observed \\(y_t\\) to update our estimates of \\(\\beta_t\\) using the regression lemma.\n\n3.3.1 Forecasting\nConsider the simple first-order VAR model \\[\n  \\beta_t = F\\beta_{t-1}+v_{t},\\quad v_t\\sim N(0,Q)\n\\] We can use to make the conditional forecast \\[\n  \\beta_{t|t-1} = F\\beta_{t-1}\n\\] where \\(\\beta_{t|t-1}=E[\\beta_t|\\psi_{t-1}]\\) and \\(\\psi_{t-1}\\) is the information set available at time \\(t-1\\).\nIf \\(\\psi_{t-1}\\) includes \\(\\beta_{t-1}\\) we can straightforwardly forecast next period (and the next-but-one period etc) using the model. This is a standard forecasting exercise given any estimated (or even calibrated) economic model.\n\n\n3.3.2 Uncertainty\nHow can we assess the associated forecast uncertainty? The forecast covariance \\(P_{t|t-1} = var(\\beta_t|\\psi_{t-1})\\) is given by \\[\n\\begin{align}\nP_{t|t-1} &= E\\left[ (\\beta_t - \\beta_{t|t-1})(\\beta_t-\\beta_{t|t-1})'\\right]   \\\\\n  &= E\\left[ (F\\beta_{t-1}+v_t-F\\beta_{t-1|t-1}) \\left(\\beta_{t-1}'F'+v_t'-\\beta_{t-1|t-1}'F'\\right) \\right]  \\notag \\\\\n&= E\\left[ F\\left( \\beta _{t-1}-\\beta _{t-1|t-1}\\right) \\left( \\beta_{t-1}-\\beta _{t-1|t-1}\\mu_z \\right) ' F' \\right] + E[v_t v_t']  \\notag \\\\\n&= FP_{t-1|t-1}F' + Q\n\\end{align}\n\\] where \\(P_{t-1|t-1} = var(\\beta_{t-1}|\\psi_{t-1})\\)\n\nThe forecast error variance depends on the previous error variance; that value depends on the information set\n\n\n\n3.3.3 Uncertainty\nIf \\(\\beta_{t-1}\\) forms part of the information set \\(\\psi_{t-1}\\) then \\[\nP_{t-1|t-1} = var \\left(\\beta_{t-1}|\\psi_{t-1}\\right) = 0\n\\] and there is no uncertainty other than from the disturbance terms and \\(P_t=Q\\).\nIf \\(\\beta_{t-1}\\) does not form part of the information set \\(\\psi_{t-1}\\) but \\(\\beta _{t-2}\\) does then \\(P_{t-1|t-1}=Q\\) and \\(P_{t|t-1}=FQF'+Q\\). This can be continued backwards; the unconditional (steady-state) covariance of \\(\\beta_t\\) is the limit \\(P=FPF'+Q\\). We can easily calculate error bands for \\(\\beta_t\\) using the appropriate information set.\n\n\n3.3.4 Prediction error\nWe can turn this around, as it must be the prediction errors are given by \\[\n\\begin{aligned}\n\\eta_{t|t-1} &= y_t - E[y_t|\\psi_{t-1}]  \\\\\n             &= y_t - E[H_t\\beta_t+e_t|\\psi_{t-1}] \\\\\n             &= y_t-H_t\\beta_{t|t-1}\n\\end{aligned}            \n\\] where \\(\\eta_{t|t-1}\\) is uncorrelated with \\(\\psi_{t-1}\\) . So the ‘news’ over that contained in \\(y_t\\) above \\(\\psi_{t-1}\\) is captured by \\(\\eta_{t|t-1}\\). It will be that \\(\\eta_{t|t-1}\\sim N(0,\\Sigma_{\\eta\\eta})\\); we need to find an expression for the covariance.\n\n\n3.3.5 Current-data predictions\nNow we find \\(E [\\beta_t | \\psi_t]\\) – the best prediction of the unknown coefficient vector given current information. Using the regression lemma we know that \\[\n\\begin{align}\nE[\\beta_t | \\psi_t ] &= E[\\beta_t | \\psi_{t-1}, \\eta_{t|t-1} ]  \\\\\n&= E[\\beta_t | \\psi_{t-1}] +\\Sigma_{\\beta\\eta} \\Sigma_{\\eta\\eta}^{-1}\\eta_{t|t-1}  \\\\\n&= \\beta_{t|t-1} + \\Sigma_{\\beta\\eta}\\Sigma_{\\eta\\eta}^{-1} \\eta_{t|t-1}\n\\end{align}\n\\] because \\(\\psi_{t-1}\\) and \\(\\eta_{t|t-1}\\) are uncorrelated and \\(\\eta_{t|t-1}\\) is mean zero.\nSimilarly, we can find \\(P_{t|t}\\) as the best prediction of the variance of \\(\\beta_t\\) given \\(\\psi_t\\). Using the regression lemma we know that \\[\n\\begin{align}\nP_{t|t} &= E[(\\beta_t-\\beta_{t|t})(\\beta_t - \\beta_{t|t})'|\\psi_{t-1}, \\eta_{t|t-1}] \\\\\n        &= E[(\\beta_t - \\beta_{t|t})(\\beta_t-\\beta_{t|t})'|\\psi_{t-1}] - \\Sigma_{\\beta\\eta}\\Sigma_{\\eta\\eta}^{-1}\\Sigma_{\\eta\\beta} \\\\\n        &=P_{t|t-1}-\\Sigma_{\\beta\\eta} \\Sigma_{\\eta\\eta}^{-1} \\Sigma_{\\eta\\beta}\n\\end{align}\n\\]\n\n\n3.3.6 Estimated model covariances\nWhat is \\(\\Sigma_{\\beta\\eta}\\)? \\[\n\\begin{align}\n\\Sigma_{\\beta\\eta} &= E[(\\beta_t-\\beta_{t|t-1}) \\eta_{t|t-1}'] \\\\\n&= E\\left[(\\beta_t-\\beta_{t|t-1}) (y_t-H_t\\beta_{t|t-1})'\\right] \\\\\n&= E\\left[(\\beta_t-\\beta_{t|t-1}) (H_t\\beta_t+e_t-H_t\\beta_{t|t-1})'\\right] \\\\\n&= E[(\\beta_t-\\beta_{t|t-1}) (\\beta_t-\\beta_{t|t-1})'H_t'] + E[(\\beta_t - \\beta_{t|t-1}) e_t'] \\\\\n&= P_{t|t-1} H_t' \\tag{$\\Sigma_{\\beta\\eta}$}\n\\end{align}\n\\] as \\(E[(\\beta_t - \\beta_{t|t-1})e_t']=0\\).\n\n\n3.3.7 Estimated model covariances\nWhat is \\(\\Sigma_{\\eta\\eta}\\)? \\[\n\\begin{align}\n\\Sigma_{\\eta\\eta} &= E[(y_t-H_t \\beta_{t|t-1})(y_t-H_t\\beta_{t|t-1})'] \\\\\n                  &= E[(H_t\\beta_t+e_t-H_t\\beta_{t|t-1})(H_t\\beta_t+e_t-H_t\\beta_{t|t-1})'] \\\\\n                  &= E[(H_t\\beta_t-H_t \\beta_{t|t-1})(H_t\\beta_t-H_t\\beta_{t|t-1})'] + E[e_t e_t'] \\\\\n                  &= E[H_t(\\beta_t-\\beta_{t|t-1})(\\beta_t-\\beta_{t|t-1})'H_t'] + R \\\\\n                  &= H_t P_{t|t-1} H_t' + R \\\\\n                  &= f_{t|t-1} \\tag{$\\Sigma_{\\eta\\eta}$} \\label{svv}\n\\end{align}\n\\] where we define \\(f_{t|t-1}=E[\\eta_{t|t-1}\\eta_{t|t-1}']\\)"
  },
  {
    "objectID": "Kalman.html#what-is-a-filter",
    "href": "Kalman.html#what-is-a-filter",
    "title": "3  Kalman filtering",
    "section": "3.3 What is a filter?",
    "text": "3.3 What is a filter?\nImagine we had a time-varying parameter model where we estimate \\(\\beta_t\\); this becomes a time series so we have a lot of parameters to estimate: we outline an estimation method here, which we will then characterise as the Kalman Filter.\nSimple observation and transition equations \\[\n\\begin{align}\ny_t     &= H_t\\beta_t + e_t, &var(e_t) = R \\\\\n\\beta_t &= \\mu + F \\beta_{t-1} + v_t, &var(v_t)=Q\n\\end{align}\n\\] where \\(\\beta_t\\) is a vector of stochastic variables, \\(y_t\\) a vector of measurements and the data forms an information set such that \\(\\psi_T = \\{y_T,y_{T-1},...,y_1\\}\\).\nJazwinski (1970) defines three type of estimation problem\n\nSmoothing is the problem of estimating \\(\\beta_k\\) for any \\(k<T\\)\nFiltering is the problem of estimating \\(\\beta_k\\) for \\(k=T\\)\nPrediction is the problem of estimating \\(\\beta_k\\) for any \\(k>T\\)\n\n\n“The object of filtering is to update our knowledge of the system each time a new observation \\(y_t\\) is brought in.” (Durbin and Koopman (2001))\n\nFiltering is specifically this: perhaps we have some estimates already of \\(\\beta_t\\) for \\(t=1...t-1\\), then given the new period-\\(t\\) observation of \\(y_t\\) how should we estimate a new value of \\(\\beta_t\\)? This immediately implies a recursive structure to estimation problems, consistent with on-line (or real-time) estimation. The Kalman filter uses the period \\(t\\) news available from observed \\(y_t\\) to update our estimates of \\(\\beta_t\\) using the regression lemma.\n\n3.3.1 Forecasting\nConsider the simple first-order VAR model \\[\n  \\beta_t = F\\beta_{t-1}+v_{t},\\quad v_t\\sim N(0,Q)\n\\] We can use to make the conditional forecast \\[\n  \\beta_{t|t-1} = F\\beta_{t-1}\n\\] where \\(\\beta_{t|t-1}=E[\\beta_t|\\psi_{t-1}]\\) and \\(\\psi_{t-1}\\) is the information set available at time \\(t-1\\).\nIf \\(\\psi_{t-1}\\) includes \\(\\beta_{t-1}\\) we can straightforwardly forecast next period (and the next-but-one period etc) using the model. This is a standard forecasting exercise given any estimated (or even calibrated) economic model.\n\n\n3.3.2 Uncertainty\nHow can we assess the associated forecast uncertainty? The forecast covariance \\(P_{t|t-1} = var(\\beta_t|\\psi_{t-1})\\) is given by \\[\n\\begin{align}\nP_{t|t-1} &= E\\left[ (\\beta_t - \\beta_{t|t-1})(\\beta_t-\\beta_{t|t-1})'\\right]   \\\\\n  &= E\\left[ (F\\beta_{t-1}+v_t-F\\beta_{t-1|t-1}) \\left(\\beta_{t-1}'F'+v_t'-\\beta_{t-1|t-1}'F'\\right) \\right]  \\notag \\\\\n&= E\\left[ F\\left( \\beta _{t-1}-\\beta _{t-1|t-1}\\right) \\left( \\beta_{t-1}-\\beta _{t-1|t-1}\\mu_z \\right) ' F' \\right] + E[v_t v_t']  \\notag \\\\\n&= FP_{t-1|t-1}F' + Q\n\\end{align}\n\\] where \\(P_{t-1|t-1} = var(\\beta_{t-1}|\\psi_{t-1})\\)\n\nThe forecast error variance depends on the previous error variance; that value depends on the information set\n\n\n\n3.3.3 Uncertainty\nIf \\(\\beta_{t-1}\\) forms part of the information set \\(\\psi_{t-1}\\) then \\[\nP_{t-1|t-1} = var \\left(\\beta_{t-1}|\\psi_{t-1}\\right) = 0\n\\] and there is no uncertainty other than from the disturbance terms and \\(P_t=Q\\).\nIf \\(\\beta_{t-1}\\) does not form part of the information set \\(\\psi_{t-1}\\) but \\(\\beta _{t-2}\\) does then \\(P_{t-1|t-1}=Q\\) and \\(P_{t|t-1}=FQF'+Q\\). This can be continued backwards; the unconditional (steady-state) covariance of \\(\\beta_t\\) is the limit \\(P=FPF'+Q\\). We can easily calculate error bands for \\(\\beta_t\\) using the appropriate information set.\n\n\n3.3.4 Prediction error\nWe can turn this around, as it must be the prediction errors are given by \\[\n\\begin{aligned}\n\\eta_{t|t-1} &= y_t - E[y_t|\\psi_{t-1}]  \\\\\n             &= y_t - E[H_t\\beta_t+e_t|\\psi_{t-1}] \\\\\n             &= y_t-H_t\\beta_{t|t-1}\n\\end{aligned}            \n\\] where \\(\\eta_{t|t-1}\\) is uncorrelated with \\(\\psi_{t-1}\\) . So the ‘news’ over that contained in \\(y_t\\) above \\(\\psi_{t-1}\\) is captured by \\(\\eta_{t|t-1}\\). It will be that \\(\\eta_{t|t-1}\\sim N(0,\\Sigma_{\\eta\\eta})\\); we need to find an expression for the covariance.\n\n\n3.3.5 Current-data predictions\nNow we find \\(E [\\beta_t | \\psi_t]\\) – the best prediction of the unknown coefficient vector given current information. Using the regression lemma we know that \\[\n\\begin{align}\nE[\\beta_t | \\psi_t ] &= E[\\beta_t | \\psi_{t-1}, \\eta_{t|t-1} ]  \\\\\n&= E[\\beta_t | \\psi_{t-1}] +\\Sigma_{\\beta\\eta} \\Sigma_{\\eta\\eta}^{-1}\\eta_{t|t-1}  \\\\\n&= \\beta_{t|t-1} + \\Sigma_{\\beta\\eta}\\Sigma_{\\eta\\eta}^{-1} \\eta_{t|t-1}\n\\end{align}\n\\] because \\(\\psi_{t-1}\\) and \\(\\eta_{t|t-1}\\) are uncorrelated and \\(\\eta_{t|t-1}\\) is mean zero.\nSimilarly, we can find \\(P_{t|t}\\) as the best prediction of the variance of \\(\\beta_t\\) given \\(\\psi_t\\). Using the regression lemma we know that \\[\n\\begin{align}\nP_{t|t} &= E[(\\beta_t-\\beta_{t|t})(\\beta_t - \\beta_{t|t})'|\\psi_{t-1}, \\eta_{t|t-1}] \\\\\n        &= E[(\\beta_t - \\beta_{t|t})(\\beta_t-\\beta_{t|t})'|\\psi_{t-1}] - \\Sigma_{\\beta\\eta}\\Sigma_{\\eta\\eta}^{-1}\\Sigma_{\\eta\\beta} \\\\\n        &=P_{t|t-1}-\\Sigma_{\\beta\\eta} \\Sigma_{\\eta\\eta}^{-1} \\Sigma_{\\eta\\beta}\n\\end{align}\n\\]\n\n\n3.3.6 Estimated model covariances\nWhat is \\(\\Sigma_{\\beta\\eta}\\)? \\[\n\\begin{align}\n\\Sigma_{\\beta\\eta} &= E[(\\beta_t-\\beta_{t|t-1}) \\eta_{t|t-1}'] \\\\\n&= E\\left[(\\beta_t-\\beta_{t|t-1}) (y_t-H_t\\beta_{t|t-1})'\\right] \\\\\n&= E\\left[(\\beta_t-\\beta_{t|t-1}) (H_t\\beta_t+e_t-H_t\\beta_{t|t-1})'\\right] \\\\\n&= E[(\\beta_t-\\beta_{t|t-1}) (\\beta_t-\\beta_{t|t-1})'H_t'] + E[(\\beta_t - \\beta_{t|t-1}) e_t'] \\\\\n&= P_{t|t-1} H_t' \\tag{$\\Sigma_{\\beta\\eta}$}\n\\end{align}\n\\] as \\(E[(\\beta_t - \\beta_{t|t-1})e_t']=0\\).\n\n\n3.3.7 Estimated model covariances\nWhat is \\(\\Sigma_{\\eta\\eta}\\)? \\[\n\\begin{align}\n\\Sigma_{\\eta\\eta} &= E[(y_t-H_t \\beta_{t|t-1})(y_t-H_t\\beta_{t|t-1})'] \\\\\n                  &= E[(H_t\\beta_t+e_t-H_t\\beta_{t|t-1})(H_t\\beta_t+e_t-H_t\\beta_{t|t-1})'] \\\\\n                  &= E[(H_t\\beta_t-H_t \\beta_{t|t-1})(H_t\\beta_t-H_t\\beta_{t|t-1})'] + E[e_t e_t'] \\\\\n                  &= E[H_t(\\beta_t-\\beta_{t|t-1})(\\beta_t-\\beta_{t|t-1})'H_t'] + R \\\\\n                  &= H_t P_{t|t-1} H_t' + R \\\\\n                  &= f_{t|t-1} \\tag{$\\Sigma_{\\eta\\eta}$} \\label{svv}\n\\end{align}\n\\] where we define \\(f_{t|t-1}=E[\\eta_{t|t-1}\\eta_{t|t-1}']\\)"
  },
  {
    "objectID": "CK.html",
    "href": "CK.html",
    "title": "5  Carter-Kohn",
    "section": "",
    "text": "6 Kalman filter in econometrics"
  },
  {
    "objectID": "CK.html#maximum-likelihood",
    "href": "CK.html#maximum-likelihood",
    "title": "5  Carter-Kohn",
    "section": "5.2 Maximum likelihood",
    "text": "5.2 Maximum likelihood\n\n5.2.1 Classical Maximum Likelihood Estimation\nThe principle of maximum likelihood is that the parameters should be chosen so that the probability of observing a given sample is maximized\n\nFor time series models the joint density of \\(\\psi_T = \\{y_T, y_{T-1},\\ldots ,y_1 \\}\\) and parameters \\(\\theta\\) in conditional form is \\[\np(\\psi_T|\\theta) = \\prod\\nolimits_{t=1}^T p (y_t|\\psi_{t-1},\\theta)\n\\] emphasizing the serial dependence of observations\nInterpret this as the likelihood for a particular sample\nAssuming (conditional) normality, the likelihood of any particular \\(n\\)-vector of observations is\n\n\\[\np(y_t) = (2\\pi)^{-\\frac{n}{2}}|var(y_t)|^{-\\frac{1}{2}}e^{\\left\\{ -\\frac{1}{2}(y_t-\\mu )' var(y_t)^{-1}(y_t-\\mu)\\right\\} }\n\\]\n\nDepends on the observations and the parameters\nIt can be multivariate and for any underlying density\nA maximum likelihood (ML) estimate of \\(\\theta\\) maximizes the likelihood of the observed sample\n\n\n\n5.2.2 Poisson example\n\nGreene (1997) constructs a Poisson distribution example where the density for each observation is \\[\np(y_i,\\ \\theta )=\\frac{e^{-\\theta }\\theta ^{y_i}}{y_i!}\n\\] for \\(y&gt;0\\), zero otherwise and property \\(E[Y]=var(Y)=\\theta\\)\n\nCount variables often modeled as a random Poisson process: numbers of road traffic accidents, sales, telephone calls, electron emissions. Greene’s example is to find the most likely value of \\(\\theta\\) given observations. \\[\n5,\\ 0,\\ 1,\\ 1,\\ 0,\\ 3,\\ 2,\\ 3,\\ 4,\\ 1\n\\] For independent observations the joint density is \\[\np(y,\\ \\theta) =\\prod_{i=1}^{10}p(y_i,\\ \\theta )\n  = \\frac{e^{-10\\theta}\\theta^{\\sum_i y_i}}{\\prod_i (y_i!)}\n  = \\frac{e^{-10\\theta}\\theta^{20}}{207,360}\n\\] We can plot this function to see if it has a maximum\n\n\n\n\n\n\n\n\n\n\nWe can also find this by calculus\nThe log function is monotonic so convenient to take logs\n\n\\[\n   \\ln L(\\theta) = -10\\theta + 20\\ln\\theta -\\ln(207,360)\n\\]\nFirst order conditions are \\[\n  \\frac{\\partial \\ln L(\\theta )}{\\partial\\theta} = -10+\\frac{20}{\\theta}\n\\Rightarrow \\theta =\\frac{20}{10}=2\n\\] Check for maximum \\[\n   \\frac{\\partial^2\\ln L(\\theta )}{\\partial\\theta^2} = -\\frac{20}{\\theta^2} &lt; 0\n\\]\n\n\n5.2.3 ML and regression\nLinear regression problem is \\[\nL(y) = \\frac{1}{(2\\pi \\sigma^2)^{n/2}}\\exp \\left[ -\\frac{1}{2\\sigma^2}(y-X\\beta)'(y-X\\beta)\\right]\n\\] Log-likelihood \\[\n   \\ln L = -\\frac{n}{2}\\ln (2\\pi)-\\frac{n}{2}\\ln (\\sigma^2) - \\frac{1}{2\\sigma^2}(y-X\\beta)'(y-X\\beta)\n\\] Find the extremum by calculus; yields likelihood equations \\[\n\\begin{align}\n\\frac{\\partial \\ln L}{\\partial \\beta} & = - \\frac{2}{2\\sigma^2} (X'y-X'X\\beta) = 0 \\\\\n& \\Rightarrow \\hat{\\beta}_{ml} = (X'X)^{-1}X'y\n\\end{align}\n\\] and \\[\n\\begin{align}\n\\frac{\\partial \\ln L}{\\partial \\sigma^2} &=\n   - \\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4}   \n     (y - X\\beta)'(y - X\\beta) = 0 \\\\\n& \\Rightarrow -n + \\sigma^{-2} (\\epsilon'\\epsilon) = 0 \\\\\n& \\Rightarrow \\hat{\\sigma}_{ml}^2 = \\frac{\\hat{\\epsilon}'\\hat{\\epsilon}}{n}\n\\end{align}\n\\] ML estimate of \\(\\sigma^2\\) divided by \\(n\\) (not \\(n-k\\)) so biased in small samples but not asymptotically"
  },
  {
    "objectID": "CK.html#kalman-filter-tricks",
    "href": "CK.html#kalman-filter-tricks",
    "title": "5  Carter-Kohn",
    "section": "5.3 Kalman filter tricks",
    "text": "5.3 Kalman filter tricks\n\nFor some initial condition – say \\(\\beta_0 \\sim N(\\mu_0,P_0))\\) – the conditional log-likelihood for sample \\(1\\) to \\(T\\)\n\n$$ \\[\\begin{align}\n\\log L(\\psi_t|\\theta) &= \\sum\\nolimits_{t=1}^T\\log p(y_t|\\psi_{t-1},\\theta) \\\\\n&\\propto -\\sum\\nolimits_{t=1}^T\\left( \\log \\left\\vert f_{t|t-1}\\right\\vert +\\eta_{t|t-1}' f_{t|t-1}^{-1}\\eta_{t|t-1} |\\ \\theta \\right)\n\n\\end{align}\\]\n$$\n\nNote we could obtain \\(\\eta_{t|t-1}\\) and \\(f_{t|t-1}\\) from the Kalman filter, i.e.\n\n$$\nf_{t|t-1} = (H_tP_{t|t-1}H_t’ + Q) = _{}\n$$\n\nThis is the prediction error decomposition of the log-likelihood\nFor a classical approach we estimate \\(\\theta\\) by numerically maximizing \\(\\log L(\\psi_T|\\theta)\\)\nThis gives a point estimate for the value of \\(\\theta\\) and we typically apply classical inference using the estimated standard errors\nNote to do this we need to evaluate the best estimate of the state as well as maximize the likelihood: the Kalman Filter is a key ingredient in both\nA suitable numerical maximization routine will (in principle) maximize the likelihood straightforwardly\n\nOften use Chris Sims’ csminwel in Matlab as well-suited to this type of problem\n\nCan show (Cramer-Rao) that\n\n\\[\n\\widehat{\\theta}\\sim N\\left(\\theta, -\\frac{\\partial^2 \\log L(\\psi_T|\\theta)}{\\partial\\theta\\partial\\theta'} \\right)\n\\]"
  },
  {
    "objectID": "CK.html#full-sample-estimates-of-beta_t",
    "href": "CK.html#full-sample-estimates-of-beta_t",
    "title": "5  Carter-Kohn",
    "section": "5.4 Full sample estimates of \\(\\beta_t\\)",
    "text": "5.4 Full sample estimates of \\(\\beta_t\\)\n\n5.4.1 The regression lemma again\nYou may recall that for any \\[\n\\begin{bmatrix} z \\\\ y \\\\ \\varepsilon \\end{bmatrix}\n\\sim N\\left(\\begin{bmatrix} \\mu_z \\\\ \\mu_y \\\\ 0 \\end{bmatrix},\n\\begin{bmatrix}\n\\Sigma_{zz} & \\Sigma_{zy} & \\Sigma_{z\\varepsilon } \\\\\n\\Sigma_{yz} & \\Sigma_{yy} & 0 \\\\\n\\Sigma_{\\varepsilon z} & 0 & \\Sigma_{\\varepsilon\\varepsilon}\n\\end{bmatrix}\\right)\n\\] then it must be that \\[\n\\begin{align}\nE[z|y,\\varepsilon] &= \\mu_z + \\Sigma_{zy}\\Sigma_{yy}^{-1}(y-\\mu_y) + \\Sigma_{z\\varepsilon}\\Sigma_{\\varepsilon\\varepsilon}^{-1}\\varepsilon \\\\\n&= E[z|y] + \\Sigma_{z\\varepsilon}\\Sigma_{\\varepsilon\\varepsilon }^{-1}\\varepsilon\n\\end{align}\n\\]\n\nWe use this to derive a recursive update to smooth our estimates\nWe will also derive an appropriate conditional expectation which we can use in Gibbs sampling"
  },
  {
    "objectID": "CK.html#smoothing",
    "href": "CK.html#smoothing",
    "title": "5  Carter-Kohn",
    "section": "5.5 Smoothing",
    "text": "5.5 Smoothing\n\nThe Kalman filter estimates \\(\\beta_t\\) recursively: it only uses information available up until time \\(t\\)\nThis means that the estimate of \\(\\beta_{T|T}\\) uses all available information, but any previous estimate doesn’t\nIndeed there must be some values of \\(\\eta_{i|t-1}\\)\n\n\\[\n   \\beta_{t|T} = E(\\beta_t | \\psi_{t-1}, \\eta_{t|t-1}, \\eta_{t+1|t-1},...,\\eta_{T|t-1})\n\\] where the ‘news’ is relative to period \\(t\\)\n\nWe could update \\(\\beta_{t|t-1}\\) using the (uncorrelated) future innovations\n\n\\[\n  \\beta_{t|T}=\\beta_{t|t}+\\sum_{j=t}^T\\Sigma_{\\beta_t\\eta_j}\\Sigma_{\\eta_j\\eta_j}^{-1}\\eta_{j|t-1}\n\\] and recalling \\(\\beta_{t|t}=E(\\beta_t|\\psi_{t-1},\\eta_{t|t-1})\\)\n\nThis is a fixed interval smoother; often used for full sample estimates of \\(\\beta_t\\)\nRemember we already have an estimate of \\(\\beta_{T|T}\\) from the Kalman filter so smoothers work backwards; we sketch a derivation here\nIn the last but one period we have a different prediction error\n\n\\[\n   \\varsigma_{T|T-1} = \\beta_{T|T} - F\\beta_{T-1|T-1} - \\mu\n\\] which is the error in predicting \\(\\beta_T\\) using \\(\\psi_{T-1}\\)\n\nAn ‘update’ has to be of the form \\[\n\\beta_{T-1|T}=\\beta_{T-1|T-1} + \\Sigma_{\\beta\\varsigma}\\Sigma_{\\varsigma\\varsigma}^{-1}\\varsigma_{T|T-1}\n\\] where \\(\\Sigma_{\\varsigma\\varsigma} = var\\left[ \\varsigma_T|\\psi_{T-1}\\right]\\) and \\(\\Sigma_{\\beta \\varsigma}=cov\\left[\\beta_{T-1},\\varsigma_T|\\psi_{T-1}\\right]\\)\n\nThese are \\[\n\\begin{align}\n\\Sigma_{\\varsigma\\varsigma} &= var(\\beta_T - F\\beta_{T-1|T-1}-\\mu) \\\\\n  &= var\\left(F(\\beta_{T-1}-\\beta_{T-1|T-1}) + e_t \\right) \\\\\n  &= F P_{T-1|T-1}F' + Q\n\\end{align}\n\\] and \\[\n\\begin{align}\n\\Sigma_{\\beta\\varsigma} &= E\\left[ (\\beta_{T-1}-\\beta_{T-1|T-1}) \\left( \\beta_T - F\\beta_{T-1|T-1}-\\mu \\right)'\n\\right] \\\\\n&= E\\left[ (\\beta_{T-1}-\\beta_{T-1|T-1}) (\\beta_T-\\beta_{T-1|T-1})'\\right] F' \\\\\n&= P_{T-1|T-1}F'\n\\end{align}\n\\] Plugging these definitions in gives us \\[\n\\beta_{T-1|T} = \\beta_{T-1|T-1} + P_{T-1|T-1}F' P_{T|T-1}^{-1} (\\beta_{T|T}-F\\beta_{T-1|T-1}-\\mu)\n\\] Applying the argument backward in time gives the recursion \\[\n\\begin{eqnarray}\n\\beta_{t|T} &=& \\beta_{t|t}+P_{t|t} F'P_{t+1|t}^{-1} (\\beta_{t+1|T}-F\\beta_{t|t}-\\mu) \\\\\n&=& \\beta_{t|t} - K_{t|T} (\\beta_{t+1|T}-F\\beta_{t|t}-\\mu) \\tag{smooth}\n\\end{eqnarray}\n\\] All these quantities are outputs of the Kalman filter so smoothing is easy to implement\nThe smoothed variance of \\(\\beta_{t|T}\\) found by multiplying out (smooth). To do this use \\(\\beta_{t+1|t} = \\mu + F\\beta_{t|t}\\) so rearranging gives \\[\n  \\widetilde{\\beta}_{t|T} + K_{t|T}\\beta_{t+1|T} = \\widetilde{\\beta}_{t|t}+K_{t|T}\\beta_{t+1|t}\n\\] where \\(\\widetilde{\\beta}_{t|t}=\\beta_t-\\beta_{t|t}\\). Now square both sides and take expectations \\[\n  P_{t|T} + K_{t|T} E\\left[\\beta_{t+1|T}\\beta_{t+1|T}' \\right] K_{t|T}' = P_{t|t} + K_{t|T} E\\left[ \\beta_{t+1|t} \\beta_{t+1|t}' \\right] K_{t|T}'\n\\] Adding and subtracting \\(E[\\beta_{t+1}\\beta_{t+1}']\\) we can show that \\[\n-E\\left[\\beta_{t+1|T}\\beta_{t+1|T}'\\right] + E\\left[\\beta_{t+1|t}\\beta_{t+1|t}'\\right] = P_{t+1|T}-P_{t+1|t}\n\\] to obtain \\[\n  P_{t|T} = P_{t|t} + K_{t|T} (P_{t+1|T}-P_{t+1|t}) K_{t|T}'.\n\\]"
  },
  {
    "objectID": "CK.html#smoothing-1",
    "href": "CK.html#smoothing-1",
    "title": "5  Carter-Kohn",
    "section": "6.5 Smoothing",
    "text": "6.5 Smoothing\n\nThis is a fixed interval smoother; often used for full sample estimates of \\(\\beta_t\\)\nRemember we already have an estimate of \\(\\beta_{T|T}\\) from the Kalman filter so smoothers work backwards; we sketch a derivation here\nIn the last but one period we have a different prediction error\n\n$$\n{T|T-1} = {T|T} - F_{T-1|T-1} - \n$$\nwhich is the error in predicting \\(\\beta_T\\) using \\(\\psi_{T-1}\\)\n\nAn ‘update’ has to be of the form\n\n$$\n{T-1|T}={T-1|T-1} + {}{}^{-1}_{T|T-1}\n$$\nwhere \\(\\Sigma_{\\varsigma\\varsigma} = var\\left[ \\varsigma_T|\\psi_{T-1}\\right]\\) and \\(\\Sigma_{\\beta \\varsigma}=cov\\left[\\beta_{T-1},\\varsigma_T|\\psi_{T-1}\\right]\\)\nThese are \\[\n\\begin{align}\n\\Sigma_{\\varsigma\\varsigma} &= var(\\beta_T - F\\beta_{T-1|T-1}-\\mu) \\\\\n  &= var\\left(F(\\beta_{T-1}-\\beta_{T-1|T-1}) + e_t \\right) \\\\\n  &= F P_{T-1|T-1}F' + Q\n\\end{align}\n\\] and \\[\n\\begin{align}\n\\Sigma_{\\beta\\varsigma} &= E\\left[ (\\beta_{T-1}-\\beta_{T-1|T-1}) \\left( \\beta_T - F\\beta_{T-1|T-1}-\\mu \\right)'\n\\right] \\\\\n&= E\\left[ (\\beta_{T-1}-\\beta_{T-1|T-1}) (\\beta_T-\\beta_{T-1|T-1})'\\right] F' \\\\\n&= P_{T-1|T-1}F'\n\\end{align}\n\\] Plugging these definitions in gives us \\[\n\\beta_{T-1|T} = \\beta_{T-1|T-1} + P_{T-1|T-1}F' P_{T|T-1}^{-1} (\\beta_{T|T}-F\\beta_{T-1|T-1}-\\mu)\n\\] Applying the argument backward in time gives the recursion \\[\n\\begin{eqnarray}\n\\beta_{t|T} &=& \\beta_{t|t}+P_{t|t} F'P_{t+1|t}^{-1} (\\beta_{t+1|T}-F\\beta_{t|t}-\\mu) \\\\\n&=& \\beta_{t|t} - K_{t|T} (\\beta_{t+1|T}-F\\beta_{t|t}-\\mu) \\tag{smooth}\n\\end{eqnarray}\n\\] All these quantities are outputs of the Kalman filter so smoothing is easy to implement\nSmoothed variance of \\(\\beta_{t|T}\\) found by multiplying out (smooth)\nRearranging and using \\(\\beta_{t+1|t} = \\mu + F\\beta_{t|t}\\) gives\n\\[\n  \\widetilde{\\beta}_{t|T} + K_{t|T}\\beta_{t+1|T} = \\widetilde{\\beta}_{t|t}+K_{t|T}\\beta_{t+1|t}\n\\] where \\(\\widetilde{\\beta}_{t|t}=\\beta_t-\\beta_{t|t}\\)\nSquare both sides and take expectations \\[\n  P_{t|T} + K_{t|T} E\\left[\\beta_{t+1|T}\\beta_{t+1|T}' \\right] K_{t|T}' = P_{t|t} + K_{t|T} E\\left[ \\beta_{t+1|t} \\beta_{t+1|t}' \\right] K_{t|T}'\n\\] Adding and subtracting \\(E[\\beta_{t+1}\\beta_{t+1}']\\) we can show that \\[\n-E\\left[\\beta_{t+1|T}\\beta_{t+1|T}'\\right] + E\\left[\\beta_{t+1|t}\\beta_{t+1|t}'\\right] = P_{t+1|T}-P_{t+1|t}\n\\] to obtain \\[\n  P_{t|T} = P_{t|t} + K_{t|T} (P_{t+1|T}-P_{t+1|t}) K_{t|T}'\n\\]"
  },
  {
    "objectID": "CK.html#classical-approach",
    "href": "CK.html#classical-approach",
    "title": "5  Carter-Kohn",
    "section": "6.1 Classical approach",
    "text": "6.1 Classical approach\n\nFormulate state-space model\nEstimate the model by maximum likelihood\nCondition on the parameters to retrieve the (usually smoothed) state estimates and standard errors\nUse Cramer-Rao to calculate the standard errors of any other parameter estimates\nFor this the Kalman filter is a useful tool, as it allows a great deal of flexibility in the estimation of a variety of models\nIt is the appropriate tool for models with unobserved components\nIt must be used with care: it is easy to try to estimate models that are essentially unidentified\nFurther useful tools\n\nThe Extended Kalman filter linearises the filter at every step and can be used for nonlinear models (such as ones where you need to estimate \\(B_T\\) and \\(\\theta\\) simultaneously)\nIncreasingly non-Gaussian non-linear models are estimated using the particle filter"
  },
  {
    "objectID": "CK.html#bayesian-approach",
    "href": "CK.html#bayesian-approach",
    "title": "5  Carter-Kohn",
    "section": "6.2 Bayesian approach",
    "text": "6.2 Bayesian approach\n\nBayesian approach is to generate the entire distribution of the model parameters\nNow no longer just look for the point estimate obtained by maximum likelihood\nUse Gibbs sampling or some other appropriate method applied to the state space model\nIn particular we treat the states and the parameters as jointly determined by the data\nAs the state is estimated we need a way to draw the states conditional on the other estimates to do Gibbs sampling\nSeek a conditional updating algorithm that replicates the Gibbs sampling approach we have used before\nWe require a procedure such that:\n\nStep 1 Conditional on \\(\\theta\\) and the data, generate the sequence \\(B_T = (\\beta_1, \\beta_2, \\ldots, \\beta_T)\\)\nStep 2 Conditional on \\(B_T\\) and the data, generate values of \\(\\theta\\)\nStep 3 Iterate previous two steps until convergence\n\nIn this way the joint distribution of the two can be obtained from the resulting simulation"
  },
  {
    "objectID": "CK.html#carter-kohn-equations",
    "href": "CK.html#carter-kohn-equations",
    "title": "5  Carter-Kohn",
    "section": "7.1 Carter-Kohn equations",
    "text": "7.1 Carter-Kohn equations\n\nStep 2 above relatively easy but how do we generate a sequence of states?\n\nThe state estimates depend on the parameter value through the Kalman filter\n\nAppropriate algorithm designed by Carter and Kohn (1994)\nTakes the form of a modified Kalman smoother\nKnown as multimove Gibbs sampling"
  },
  {
    "objectID": "CK.html#carter-kohn-equations-1",
    "href": "CK.html#carter-kohn-equations-1",
    "title": "5  Carter-Kohn",
    "section": "7.2 Carter-Kohn equations",
    "text": "7.2 Carter-Kohn equations\n\nSimilar to above, define\n\n\\[\n  B_t = \\begin{bmatrix} \\beta_1 & \\beta_2 & \\ldots & \\beta_t \\end{bmatrix}\n\\] so in particular \\[\nB_{T-1} = \\begin{bmatrix} \\beta_1 & \\beta_2 & \\ldots & \\beta_{T-1} \\end{bmatrix}\n\\] consistent with out earlier definition of \\(\\psi_t\\)\n\nMultimove Gibbs sampling generates the whole vector of states (\\(B_T\\)) at once\nWe therefore need to generate a realization of \\(B_T\\) given the probability distribution \\(p( B_T|\\psi_T)\\)\nWe want to generate an appropriate conditional probability distribution \\(p(\\beta_t|B_{j\\neq t}, \\psi_T)\\) to sample from for our Gibbs sampler\nJust as for the Kalman smoother we use the outputs of the Kalman filter and a separate backward recursion to obtain the conditional distribution"
  },
  {
    "objectID": "CK.html#joint-distribution",
    "href": "CK.html#joint-distribution",
    "title": "5  Carter-Kohn",
    "section": "5.8 Joint distribution",
    "text": "5.8 Joint distribution\n\nDeriving the appropriate distributions is easy if we know what to condition on\nThe joint probability density function can be split into a sequence of conditional distributions: \\(p(B_T|\\psi_T)\\) can be written recursively\n\n\\[\n\\begin{align}\np(B_T|\\psi_T) &= p(\\beta_T|\\psi_T) \\times p(B_{T-1}|\\beta_T,\\psi_T) \\\\\n&= p(\\beta_T|\\psi_T) \\times p(\\beta_{T-1}|\\beta_T,\\psi_T) \\times p(B_{T-2}|\\beta_{T-1},\\beta_T,\\psi_T) \\\\\n&= p(\\beta_T|\\psi_T) \\times p(\\beta_{T-1}|\\beta_T,\\psi_T) \\times p(B_{T-2}|\\beta_{T-1},\\psi_T)\n\\end{align}\n\\]\n\nFinal simplification follows as the state vector is a Markov chain so there is no information in \\(\\beta_T\\) not contained in \\(\\beta_{T-1}\\) and \\(\\psi_T\\)\nFurther as soon as we know \\(\\beta_{T-1}\\) there is no information contained in \\(\\psi_T\\) so we can drop that, so\n\n\\[\n\\begin{align}\np(B_T|\\psi_T) &= p(\\beta_T|\\psi_T)\\times p(\\beta_{T-1}|\\beta_T, \\psi_T) \\times  p(B_{T-2}|\\beta_{T-1},\\psi_T) \\\\\n&= p(\\beta_T|\\psi_T)\\times p(\\beta_{T-1}|\\beta_T, \\psi_{T-1}) \\times p(B_{T-2}|\\beta_{T-1},\\psi_{T-2}) \\\\\n&= p(\\beta_T|\\psi_T) \\times \\prod_{t=1}^{T-1} p(\\beta_t|\\beta_{t+1}, \\psi_t)\n\\end{align}\n\\]"
  },
  {
    "objectID": "CK.html#the-carter-kohn-equations",
    "href": "CK.html#the-carter-kohn-equations",
    "title": "5  Carter-Kohn",
    "section": "5.9 The Carter-Kohn equations",
    "text": "5.9 The Carter-Kohn equations\n\nThe estimated \\(\\beta\\) variables are distributed\n\n$$\n\\[\\begin{align}\n\n\\beta_{T|\\psi_T} &\\sim N(\\beta_{T|T}, P_{T|T}) \\\\\n\n\\beta_{t|\\psi_t, \\beta_{t+1}} &\\sim N(\\beta_{t|t,\\beta_{t+1}},P_{t|t,\\beta_{t+1}})\n\n\\end{align}\\]\n$$\nwhere\n\\[\n\\begin{align}\n\\beta_{t|t,\\beta_{t+1}} &= E\\left[\\beta_t|\\psi_t,\\beta_{t+1}\\right]\n= E\\left[\\beta_t|\\beta_{t|t},\\beta_{t+1}\\right] \\\\\nP_{t|t,\\beta_{t+1}} &= cov\\left[\\beta_t|\\psi_t,\\beta_{t+1}\\right] = cov\\left[\\beta_t |{\\beta_{t|t},\\beta_{t+1}}\\right]\n\\end{align}\n\\] Carter-Kohn derive appropriate recursions so that, for example, we update the state estimate conditioning on some known value of \\(\\beta_{t+1}\\) \\[\n  \\beta_{t|t,\\beta_{t+1}} = \\beta_{t|t}-K_{t|t+1} (\\beta_{t+1}-F\\beta_{t|t}-\\mu)\n\\]\n\nDefine\n\n\\[\n   \\varsigma_{t+1|t} = \\beta_{t+1}-F\\beta_{t|t}-\\mu\n\\] as the `innovation’ in predicted \\(\\beta_{t+1|t}\\) where we have some realized \\(\\beta_{t+1}\\) drawn from its probability distribution\n\nCarter-Kohn smoother comprises updates to the conditional expectations that use this news\n\n\\[\n\\begin{align}\nE[\\beta_t|\\psi_t,\\beta_{t+1}] &= E[\\beta_t|\\psi_t] + \\Sigma_{\\beta\\varsigma} \\Sigma_{\\varsigma\\varsigma}^{-1}\\varsigma_{t+1|t} \\\\\n&= \\beta_{t|t} + \\Sigma_{\\beta\\varsigma}\\Sigma_{\\varsigma\\varsigma}^{-1}\\varsigma_{t+1|t} \\\\\nvar[\\beta_t|\\psi_t,\\beta_{t+1}] &= var[\\beta_t|\\psi_t] -\\Sigma_{\\beta\\varsigma} \\Sigma_{\\varsigma\\varsigma}^{-1} \\Sigma_{\\varsigma\\beta} \\\\\n&= P_{t|t} - \\Sigma_{\\beta\\varsigma} \\Sigma_{\\varsigma\\varsigma}^{-1} \\Sigma_{\\varsigma\\beta} \\\\\n&= P_{t|t,\\beta_{t+1}}\n\\end{align}\n\\]\n\nBoth \\(\\beta_{t|t}\\) and \\(P_{t|t}\\) are outputs of the Kalman filter"
  },
  {
    "objectID": "CK.html#deriving-sigma_betavarsigma-and-sigma_varsigmavarsigma",
    "href": "CK.html#deriving-sigma_betavarsigma-and-sigma_varsigmavarsigma",
    "title": "5  Carter-Kohn",
    "section": "5.10 Deriving \\(\\Sigma_{\\beta\\varsigma}\\) and \\(\\Sigma_{\\varsigma\\varsigma}\\)",
    "text": "5.10 Deriving \\(\\Sigma_{\\beta\\varsigma}\\) and \\(\\Sigma_{\\varsigma\\varsigma}\\)\nAs before we just plug in the definitions so\n\\[\n\\begin{align}\n\\Sigma_{\\varsigma\\varsigma} &= var[\\beta_{t+1}-F\\beta_{t|t}-\\mu] \\\\\n&= var\\left[ F\\beta_t+\\mu +v_{t+1}-F\\beta_{t|t}-\\mu \\right] \\\\\n&= var\\left[ F(\\beta_t-\\beta_{t|t})+v_{t+1}\\right] \\\\\n&= F P_{t|t}F' + Q\n\\end{align}\n\\] and \\[\n\\begin{align}\n\\Sigma_{\\beta\\varsigma} &= E\\left[ (\\beta_t - \\beta_{t|t}) (\\beta_{t+1}-F\\beta_{t|t}-\\mu)'\\right] \\\\\n&= E\\left[ (\\beta_t-\\beta_{t|t})\\left( F(\\beta_t-\\beta_{t|t})+v_{t+1}\\right)'\\right] \\\\\n&= P_{t|t}F'\n\\end{align}\n\\]"
  },
  {
    "objectID": "CK.html#kalman-gain-again",
    "href": "CK.html#kalman-gain-again",
    "title": "5  Carter-Kohn",
    "section": "5.11 ‘Kalman gain’ again",
    "text": "5.11 ‘Kalman gain’ again\nSo using the definitions of the covariances and the regression lemma we get \\[\n\\begin{align}\n\\beta_{t|t,\\beta_{t+1}} &= \\beta_{t|t} + \\Sigma_{s\\eta} \\Sigma_{\\eta\\eta}^{-1}\\varsigma_t \\\\\n&= \\beta_{t|t} + P_{t|t}F' \\left( FP_{t|t}F' + Q\\right)^{-1} (\\beta_{t+1}-F\\beta_{t|t}-\\mu) \\\\\n&= \\beta_{t|t} - K_{t|t}(\\beta_{t+1}-F\\beta_{t|t}-\\mu)\n\\end{align}\n\\] where \\[\n   K_{t|t+1} = - P_{t|t}F' (FP_{t|t}F' + Q)^{-1}\n\\]\nLike the Kalman smoother, this uses the filter’s estimate of \\(P_{t|t}\\) and updates \\(\\beta_t\\) using the error in predicting \\(\\beta_{t+1}\\) not \\(y_t\\)."
  },
  {
    "objectID": "CK.html#conditional-mean-and-variance-of-the-state",
    "href": "CK.html#conditional-mean-and-variance-of-the-state",
    "title": "5  Carter-Kohn",
    "section": "5.12 Conditional mean and variance of the state",
    "text": "5.12 Conditional mean and variance of the state\n\nUpdating equations for the state and variance obtained directly from the regression lemma\n\n\\[\n\\begin{align}\n\\beta_{t|t,\\beta_{t+1}} &= \\beta_{t|t} - K_{t|t+1}\\varsigma_{t+1|t} \\\\\n    P_{t|t,\\beta_{t+1}} &= P_{t|t}-P_{t|t}F'(FP_{t|t}F' + Q)^{-1}F P_{t|t}\n\\end{align}\n\\]\n\nCK equations recursively evaluate these quantities backwards beginning from \\(s_T\\), \\(P_T\\) obtained from the Kalman filter\nGenerate appropriate conditional samples using\n\n\\[\n  \\beta_{t|t,\\beta_{t+1}}\\sim N\\left(\\beta_{t|t,\\beta_{t+1}},P_{t|t,\\beta_{t+1}}\\right)\n\\] to give \\(B_T|\\psi_T\\)\n\nThis is a conditional sample that depends on a given parameter vector to use in a Gibbs sampling scheme that draws those parameters in turn from distributions conditioned on the states"
  },
  {
    "objectID": "CK.html#state-space-gibbs-sampling-in-practice",
    "href": "CK.html#state-space-gibbs-sampling-in-practice",
    "title": "5  Carter-Kohn",
    "section": "5.13 State-space Gibbs sampling in practice",
    "text": "5.13 State-space Gibbs sampling in practice\n\nAbove approach cannot be used explicitly if \\(\\Sigma_{\\varsigma\\varsigma}\\) is singular, for example if we have more states than shocks (which is not uncommon)\n\nSimple modification given in KN can deal with this; we treat only those states that are shocked as observed\n\nIn general we need conditional distributions for all the other parameters to be estimated\nNeed to store the complete sequence of states and covariances to implement the Gibbs sampler\nWe will investigate the exact implementation of Gibbs sampling for state-space models in the exercises"
  },
  {
    "objectID": "CK.html#comparing-the-filters-and-smoothers",
    "href": "CK.html#comparing-the-filters-and-smoothers",
    "title": "5  Carter-Kohn",
    "section": "5.14 Comparing the filters and smoothers",
    "text": "5.14 Comparing the filters and smoothers\n\\[\n\\begin{alignat*}{3}\n&\\text{Filter} & &\\text{Innovation} & & \\text{Gain and state covariance} & \\\\\n& \\\\\n&KF &\\qquad\\ &\\eta_t = y_t-H_t\\beta_{t|t-1} && K_{t|t} =-P_{t|t-1}H_t' (H_t P_{t|t-1} H_t' + R)^{-1} \\\\\n&   &&      && P_{t|t} = P_{t|t-1}-P_{t|t-1} H_t'(H_t P_{t|t-1} H_t' +R)^{-1} H_t P_{t|t-1} \\\\\n&KS & &\\varsigma_t = \\beta_{t+1|T}-F\\beta_{t|t}-\\mu &\\qquad\\ & K_{t|T}=-P_{t|t}F' P_{t+1|t}^{-1} \\\\\n&   &&      && P_{t|T} = P_{t|t}+K_{t|T}(P_{t+1|T}-P_{t+1|t}) K_{t|T}' \\\\\n&CK & &\\varsigma_t = \\beta_{t+1}-F\\beta_{t|t}-\\mu && K_{t|t,\\beta_{t+1}} = -P_{t|t}F' P_{t+1|t}^{-1} \\\\ &   &&   && P_{t|t,\\beta_{t+1}} = P_{t|t}-P_{t|t}F'(FP_{t|t}F' + Q)^{-1}FP_{t|t}\n\\end{alignat*}\n\\]\n\n\n\n\nCarter, C. K., and R. Kohn. 1994. “On Gibbs sampling for state space models.” Biometrika 81 (3): 541–53. https://doi.org/10.1093/biomet/81.3.541.\n\n\nDurbin, J., and S. J. Koopman. 2001. Time Series Analysis by State Space Methods. Oxford: Oxford University Press.\n\n\nGreene, William H. 1997. Econometric Analysis. Third. McGraw Hill.\n\n\nHamilton, J. D. 1994. Time Series Analysis. Princeton, NJ: Princeton University Press.\n\n\nHarvey, Andrew C. 1989. Forecasting, Structural Time Series Models and the Kalman Filter. Cambridge: Cambridge University Press.\n\n\nKim, Chang-Jin, and Charles R. Nelson. 1999. State-Space Models with Regime Switching: Classical and Gibbs-Sampling Approaches with Applications. MIT Press."
  },
  {
    "objectID": "CK.html#using-the-kalman-filter",
    "href": "CK.html#using-the-kalman-filter",
    "title": "5  Carter-Kohn",
    "section": "5.1 Using the Kalman Filter",
    "text": "5.1 Using the Kalman Filter\nEstablish the usefulness of the Kalman Filter (and not just for state estimation). Refresh idea of maximum likelihood estimation in the context of state space models.\n\nIntroduce smoothing\nDevelop Gibbs sampling by the Carter-Kohn method\nAll of these use the Kalman Filter to develop conceptually different tools\n\nFollow Kim and Nelson (1999); also see Harvey (1989), Hamilton (1994), Durbin and Koopman (2001)"
  },
  {
    "objectID": "CK.html#carter-kohn-algorithm",
    "href": "CK.html#carter-kohn-algorithm",
    "title": "5  Carter-Kohn",
    "section": "5.7 Carter-Kohn algorithm",
    "text": "5.7 Carter-Kohn algorithm\n\nStep 2 above relatively easy but how do we generate a sequence of states?\n\nThe state estimates depend on the parameter value through the Kalman filter\n\nAppropriate algorithm designed by Carter and Kohn (1994)\nTakes the form of a modified Kalman smoother\nKnown as multimove Gibbs sampling\nSimilar to above, define\n\n\\[\n  B_t = \\begin{bmatrix} \\beta_1 & \\beta_2 & \\ldots & \\beta_t \\end{bmatrix}\n\\] so in particular \\[\nB_{T-1} = \\begin{bmatrix} \\beta_1 & \\beta_2 & \\ldots & \\beta_{T-1} \\end{bmatrix}\n\\] consistent with out earlier definition of \\(\\psi_t\\)\n\nMultimove Gibbs sampling generates the whole vector of states (\\(B_T\\)) at once\nWe therefore need to generate a realization of \\(B_T\\) given the probability distribution \\(p( B_T|\\psi_T)\\)\nWe want to generate an appropriate conditional probability distribution \\(p(\\beta_t|B_{j\\neq t}, \\psi_T)\\) to sample from for our Gibbs sampler\nJust as for the Kalman smoother we use the outputs of the Kalman filter and a separate backward recursion to obtain the conditional distribution"
  },
  {
    "objectID": "CK.html#kalman-filter-in-econometrics",
    "href": "CK.html#kalman-filter-in-econometrics",
    "title": "5  Carter-Kohn",
    "section": "5.6 Kalman filter in econometrics",
    "text": "5.6 Kalman filter in econometrics\n\n5.6.1 Classical approach\nThe typical procedure is some variation on:\n\nFormulate state-space model\nEstimate the model by maximum likelihood\nCondition on the parameters to retrieve the (usually smoothed) state estimates and standard errors\nUse Cramer-Rao to calculate the standard errors of any other parameter estimates\n\nFor this the Kalman filter is a useful tool, as it allows a great deal of flexibility in the estimation of a variety of models, as is is an appropriate tool for models with unobserved components. However, it must be used with care: it is easy to try to estimate models that are essentially unidentified.\nFurther useful tools\n\nThe Extended Kalman filter linearises the filter at every step and can be used for nonlinear models (such as ones where you need to estimate \\(B_T\\) and \\(\\theta\\) simultaneously)\nIncreasingly non-Gaussian non-linear models are estimated using the particle filter\n\n\n\n5.6.2 Bayesian approach\nBayesian approach is to generate the entire distribution of the model parameters.\n\nNow no longer just look for the point estimate obtained by maximum likelihood\nUse Gibbs sampling or some other appropriate method applied to the state space model\nIn particular we treat the states and the parameters as jointly determined by the data\nAs the state is estimated we need a way to draw the states conditional on the other estimates to do Gibbs sampling\nSeek a conditional updating algorithm that replicates the Gibbs sampling approach we have used before\n\nWe require a procedure such that:\n\nStep 1 Conditional on \\(\\theta\\) and the data, generate the sequence \\(B_T = (\\beta_1, \\beta_2, \\ldots, \\beta_T)\\)\nStep 2 Conditional on \\(B_T\\) and the data, generate values of \\(\\theta\\)\nStep 3 Iterate previous two steps until convergence\n\nIn this way the joint distribution of the two can be obtained from the resulting simulation."
  }
]