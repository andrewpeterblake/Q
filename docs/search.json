[
  {
    "objectID": "Kalman.html#literature",
    "href": "Kalman.html#literature",
    "title": "5  Kalman filtering",
    "section": "5.1 Literature",
    "text": "5.1 Literature\nAlternatives to what follows can be found in Harvey (1989), Hamilton (1994), Kim and Nelson (1999), Durbin and Koopman (2001) or Triantafyllopoulos (2021). There are many books devoted to the Kalman filter as a casual Amazon search mostly from an engineering perspective. However econometricians since Harvey and Pierse (1984) have used it in a way somewhat different from standard engineering applications. We will cover the filter and then look at a simple example if filtering, then develop a maximum likelihood estimation approach.\n\n5.1.1 Reminder of a state-space model\nConsider the trend-cycle model \\[\\begin{equation}\n  y_t = \\chi_t + \\tau_t + \\varepsilon_t\n\\end{equation}\\] where the cycle equation is \\[\\begin{equation}\n  \\chi_t = c+\\rho_1 \\chi_{t-1} + \\rho_2 \\chi_{t-2}+v_{1t}\n\\end{equation}\\] and the trend equation is \\[\\begin{equation}\n  \\tau_t = \\tau_{t-1} + v_{2t}\n\\end{equation}\\]\nIn state space this can be written \\[\\begin{align}\ny_t &=\n\\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix}\n\\begin{bmatrix} \\chi_t \\\\ \\chi_{t-1} \\\\ \\tau_t \\end{bmatrix} + [1] \\varepsilon_t\\\\\n\\begin{bmatrix} \\chi_t \\\\ \\chi_{t-1} \\\\ \\tau_t \\end{bmatrix} &=\n\\begin{bmatrix} c \\\\ 0 \\\\ 0 \\end{bmatrix} +\n\\begin{bmatrix} \\rho_1 & \\rho_2 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n\\begin{bmatrix} \\chi_{t-1} \\\\ \\chi_{t-2} \\\\ \\tau_{t-1} \\end{bmatrix} +\n\\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\\\ 0 & 1 \\end{bmatrix}\n\\begin{bmatrix} v_{1t} \\\\ v_{2t} \\end{bmatrix}\n\\end{align}\\]"
  },
  {
    "objectID": "Kalman.html#a-useful-class-of-models",
    "href": "Kalman.html#a-useful-class-of-models",
    "title": "5  Kalman filtering",
    "section": "5.2 A useful class of models",
    "text": "5.2 A useful class of models\nWe need a framework that nests this type of model (and many more). State-space models are one such framework, amenable to classical (and Bayesian) estimation. Quite a lot of apparatus required before we can apply maximum likelihood.\nKey points:\n\nMany quantities routinely used to build models and analyse policy are unobservable.\nEconometricians face a major problem estimating such models: everything unobserved needs estimating simultaneously (states and parameters).\nFortunately there is a method we can use: the Kalman filter (Kalman (1960)).\nHas the useful spin-off that we can also use it to calculate the value of the likelihood function."
  },
  {
    "objectID": "Kalman.html#what-is-a-filter",
    "href": "Kalman.html#what-is-a-filter",
    "title": "5  Kalman filtering",
    "section": "5.3 What is a filter?",
    "text": "5.3 What is a filter?\nImagine we had a time-varying parameter model where we estimate \\(\\beta_t\\); this becomes a time series so we have a lot of parameters to estimate: we outline an estimation method here, which we will then characterise as the Kalman Filter.\nSimple observation and transition equations \\[\\begin{align}\ny_t     &= H_t\\beta_t + e_t, &var(e_t) = R \\\\\n\\beta_t &= \\mu + F \\beta_{t-1} + v_t, &var(v_t)=Q\n\\end{align}\\] where \\(\\beta_t\\) is a vector of stochastic variables, \\(y_t\\) a vector of measurements and the data forms an information set such that \\(\\psi_T = \\{y_T,y_{T-1},...,y_1\\}\\).\nNote that written this way the shocks are potentially reduced form ones. Typically there will be less shocks then either states (or observables) or it may be that structural shocks enter into one or more relationships. We would then write \\(e_t = B\\eta_t\\) and \\(v_t = G\\nu_t\\) where \\(B\\) and \\(G\\) are matrices of appropriate dimension that feed the structural shocks into the right equations. If we assume both \\(eta_t\\) and \\(\\nu_t\\) only contains independent unit variance shocks, then \\(R = B\\mathbb{E}(\\eta_t\\eta_t')B' = BB'\\) and similarly \\(Q=GG'\\).\nThe reason for using the reduced form shocks is toreduce the amount of notation. We will also (to further reduce notation) mostly set \\(\\mu = 0\\), but it is straightforward to have a vector of constants, which we include in the final formulae.\n\n5.3.1 Three estimation problems\nJazwinski (1970) defines three type of estimation problem\n\nSmoothing is the problem of estimating \\(\\beta_k\\) for any \\(k&lt;T\\)\nFiltering is the problem of estimating \\(\\beta_k\\) for \\(k=T\\)\nPrediction is the problem of estimating \\(\\beta_k\\) for any \\(k&gt;T\\)\n\n\n“The object of filtering is to update our knowledge of the system each time a new observation \\(y_t\\) is brought in.” (Durbin and Koopman (2001))\n\n\n\n\n\n\n\nNotation\n\n\n\nIf we understand the notation we are a long way to understanding the solution. \\[\n\\begin{align}\n\\beta_{t|t-1} &= \\mathbb{E}[\\beta_t | \\psi_{t-1}] && \\hbox{Estimate of } \\beta_t \\hbox{ conditional on } \\psi_{t-1} \\hbox{ (predicted)} \\\\\n\\beta_{t|t}   &= \\mathbb{E}[\\beta_t | \\psi_t]     && \\hbox{Current } (\\psi_t) \\hbox{ sample estimate} \\hbox{ (filtered)}\\\\\n\\beta_{t|T}   &= \\mathbb{E}[\\beta_t | \\psi_T]     && \\hbox{Full } (\\psi_T) \\hbox{ sample estimate} \\hbox{ (smoothed)}\\\\\nP_{t|j}       &= \\mathbb{E}\\left[(\\beta_t - \\beta_{t|j}) (\\beta_t - \\beta _{t|j})'\\right]  && \\hbox{Conditional cov. based on } \\psi_j,\\ j=[t-1,t,T]\\\\\ny_{t|t-1}     &= \\mathbb{E}[y_t | \\psi_{t-1}] = H_t \\beta_{t|t-1}  && \\hbox{Prediction of}\\ y_t \\hbox{ given } \\psi_{t-1}\\\\\n\\eta_{t|t-1}  &= y_t - y_{t|t-1} = y_t - H_t \\beta_{t|t-1}  && \\hbox{Prediction error given }\\psi_{t-1} \\\\\nf_{t|t-1}     &= \\mathbb{E}[\\eta_{t|t-1}\\eta_{t|t-1}']      && \\hbox{Conditional } (\\psi_{t-1}) \\hbox{ variance of prediction error}\n\\end{align}\n\\]\n\n\nIn what follows we will calculate some quantities that are consequences of the state space formulation of our problem that will allow us to apply the regression lemma to estimate the unobserved state variables.\n\n\n5.3.2 Forecasting\nConsider the simple first-order VAR model \\[\\begin{equation}\n  \\beta_t = F\\beta_{t-1}+v_{t},\\quad v_t\\sim N(0,Q)\n\\end{equation}\\] We can use to make the conditional forecast \\[\\begin{equation}\n  \\beta_{t|t-1} = F\\beta_{t-1}\n\\end{equation}\\] where \\(\\beta_{t|t-1}= \\mathbb{E}[\\beta_t|\\psi_{t-1}]\\) and \\(\\psi_{t-1}\\) is the information set available at time \\(t-1\\).\nIf \\(\\psi_{t-1}\\) includes \\(\\beta_{t-1}\\) we can straightforwardly forecast next period (and the next-but-one period etc) using the model. This is a standard forecasting exercise given any estimated (or even calibrated) economic model.\n\n\n5.3.3 Uncertainty\nHow can we assess the associated forecast uncertainty? The forecast covariance \\(P_{t|t-1} = var(\\beta_t|\\psi_{t-1})\\) is given by \\[\\begin{align*}\nP_{t|t-1} &= \\mathbb{E}\\left[ (\\beta_t - \\beta_{t|t-1})(\\beta_t-\\beta_{t|t-1})'\\right]   \\\\\n  &= \\mathbb{E}\\left[ (F\\beta_{t-1}+v_t-F\\beta_{t-1|t-1}) \\left(\\beta_{t-1}'F'+v_t'-\\beta_{t-1|t-1}'F'\\right) \\right] \\\\\n&= \\mathbb{E}\\left[ F\\left(\\beta _{t-1}-\\beta _{t-1|t-1}\\right) \\left( \\beta_{t-1}-\\beta _{t-1|t-1}\\right) ' F' \\right] + \\mathbb{E}[v_t v_t'] \\\\\n&= F\\mathbb{E}\\left[\\left(\\beta _{t-1}-\\beta _{t-1|t-1}\\right) \\left( \\beta_{t-1}-\\beta _{t-1|t-1} \\right)' \\right] F' + \\mathbb{E}[v_t v_t'] \\\\\n&= FP_{t-1|t-1}F' + Q\n\\end{align*}\\] where \\(P_{t-1|t-1} = var(\\beta_{t-1}|\\psi_{t-1})\\). The forecast error variance depends on the previous error variance; that value depends on the information set.\nIf \\(\\beta_{t-1}\\) forms part of the information set \\(\\psi_{t-1}\\) then \\[\\begin{equation}\nP_{t-1|t-1} = var \\left(\\beta_{t-1}|\\psi_{t-1}\\right) = 0\n\\end{equation}\\] and there is no uncertainty other than from the disturbance terms and \\(P_{t|t-1}=P_t=Q\\).\nIf \\(\\beta_{t-1}\\) does not form part of the information set \\(\\psi_{t-1}\\) but \\(\\beta _{t-2}\\) does then \\(P_{t-1|t-1}=Q\\) and \\(P_{t|t-1}=FQF'+Q\\). This can be continued backwards; the unconditional (steady-state) covariance of \\(\\beta_t\\) is the limit \\(P=FPF'+Q\\). We can easily calculate error bands for \\(\\beta_t\\) using the appropriate information set.\n\n\n5.3.4 Prediction error\nWe can turn this around, as it must be the prediction errors are given by \\[\\begin{align}\n\\eta_{t|t-1} &= y_t - \\mathbb{E}[y_t|\\psi_{t-1}]  \\\\\n             &= y_t - \\mathbb{E}[H_t\\beta_t+e_t|\\psi_{t-1}] \\\\\n             &= y_t-H_t\\beta_{t|t-1}\n\\end{align}\\] where \\(\\eta_{t|t-1}\\) is uncorrelated with \\(\\psi_{t-1}\\). So the ‘news’ over that contained in \\(y_t\\) above \\(\\psi_{t-1}\\) is captured by \\(\\eta_{t|t-1}\\). It will be that \\(\\eta_{t|t-1}\\sim N(0,\\Sigma_{\\eta\\eta})\\); we need to find an expression for the covariance.\n\n\n5.3.5 Current-data predictions\nNow we find \\(\\mathbb{E}[\\beta_t | \\psi_t]\\) – the best prediction of the unknown coefficient vector given current information. Using the regression lemma we know that \\[\\begin{align}\n\\mathbb{E}[\\beta_t | \\psi_t ] &= \\mathbb{E}[\\beta_t | \\psi_{t-1}, \\eta_{t|t-1} ]  \\\\\n&= \\mathbb{E}[\\beta_t | \\psi_{t-1}] +\\Sigma_{\\beta\\eta} \\Sigma_{\\eta\\eta}^{-1}\\eta_{t|t-1}  \\\\\n&= \\beta_{t|t-1} + \\Sigma_{\\beta\\eta}\\Sigma_{\\eta\\eta}^{-1} \\eta_{t|t-1}\n\\end{align}\\] because \\(\\psi_{t-1}\\) and \\(\\eta_{t|t-1}\\) are uncorrelated and \\(\\eta_{t|t-1}\\) is mean zero.\nSimilarly, we can find \\(P_{t|t}\\) as the best prediction of the variance of \\(\\beta_t\\) given \\(\\psi_t\\). Using the regression lemma we know that \\[\\begin{align}\nP_{t|t} &= \\mathbb{E}[(\\beta_t-\\beta_{t|t})(\\beta_t - \\beta_{t|t})'|\\psi_{t-1}, \\eta_{t|t-1}] \\\\\n        &= \\mathbb{E}[(\\beta_t - \\beta_{t|t})(\\beta_t-\\beta_{t|t})'|\\psi_{t-1}] - \\Sigma_{\\beta\\eta}\\Sigma_{\\eta\\eta}^{-1}\\Sigma_{\\eta\\beta} \\\\\n        &=P_{t|t-1}-\\Sigma_{\\beta\\eta} \\Sigma_{\\eta\\eta}^{-1} \\Sigma_{\\eta\\beta}\n\\end{align}\\]\n\n\n5.3.6 Estimated model covariances\nAll we need do is plug the relevant expressions into the regression lemma. So, what is \\(\\Sigma_{\\beta\\eta}\\)? \\[\\begin{align}\n\\Sigma_{\\beta\\eta} &= \\mathbb{E}[(\\beta_t-\\beta_{t|t-1}) \\eta_{t|t-1}'] \\\\\n&= \\mathbb{E}\\left[(\\beta_t-\\beta_{t|t-1}) (y_t-H_t\\beta_{t|t-1})'\\right] \\\\\n&= \\mathbb{E}\\left[(\\beta_t-\\beta_{t|t-1}) (H_t\\beta_t+e_t-H_t\\beta_{t|t-1})'\\right] \\\\\n&= \\mathbb{E}[(\\beta_t-\\beta_{t|t-1}) (\\beta_t-\\beta_{t|t-1})'H_t'] + \\mathbb{E}[(\\beta_t - \\beta_{t|t-1}) e_t'] \\\\\n&= P_{t|t-1} H_t' \\tag{$\\Sigma_{\\beta\\eta}$}\n\\end{align}\\] as \\(\\mathbb{E}[(\\beta_t - \\beta_{t|t-1})e_t']=0\\).\nWhat is \\(\\Sigma_{\\eta\\eta}\\)? \\[\\begin{align}\n\\Sigma_{\\eta\\eta} &= \\mathbb{E}[(y_t-H_t \\beta_{t|t-1})(y_t-H_t\\beta_{t|t-1})'] \\\\\n                  &= \\mathbb{E}[(H_t\\beta_t+e_t-H_t\\beta_{t|t-1})(H_t\\beta_t+e_t-H_t\\beta_{t|t-1})'] \\\\\n                  &= \\mathbb{E}[(H_t\\beta_t-H_t \\beta_{t|t-1})(H_t\\beta_t-H_t\\beta_{t|t-1})'] + \\mathbb{E}[e_t e_t'] \\\\\n                  &= \\mathbb{E}[H_t(\\beta_t-\\beta_{t|t-1})(\\beta_t-\\beta_{t|t-1})'H_t'] + R \\\\\n                  &= H_t P_{t|t-1} H_t' + R \\\\\n                  &= f_{t|t-1} \\tag{$\\Sigma_{\\eta\\eta}$} \\label{svv}\n\\end{align}\\] where we define \\(f_{t|t-1}=\\mathbb{E}[\\eta_{t|t-1}\\eta_{t|t-1}']\\). Now we’re ready."
  },
  {
    "objectID": "Kalman.html#the-kalman-filter",
    "href": "Kalman.html#the-kalman-filter",
    "title": "5  Kalman filtering",
    "section": "5.4 The Kalman filter",
    "text": "5.4 The Kalman filter\nThe equations of the filter are\n\nthe conditional expectation depending on \\(\\psi_{t-1}\\);\nan update that uses \\(\\eta_{t|t-1}\\) to obtain the best \\(t\\)-period prediction now based on \\(\\psi_t\\).\n\nThese must be of the form \\[\\begin{align}\n\\mathbb{E}[\\beta_t | \\psi_{t-1}] &= \\mu + F \\mathbb{E} [\\beta_t | \\psi_{t-1}] \\\\\n\\mathbb{E}[P_t|\\psi_{t-1}]       &= F \\mathbb{E}[P_{t-1}|\\psi_{t-1}] F' + Q \\\\\n\\mathbb{E}[\\beta_t | \\psi_t]     &= \\mathbb{E}[\\beta_t|\\psi_{t-1}] + \\Sigma_{\\beta\\eta} \\Sigma_{\\eta\\eta}^{-1}\\eta_{t|t-1} \\\\\n\\mathbb{E}[P_t|\\psi_t]           &= \\mathbb{E}[P_t|\\psi_{t-1}] -\\Sigma_{\\beta\\eta}\\Sigma_{\\eta\\eta}^{-1}\\Sigma_{\\eta \\beta}\n\\end{align}\\] These are specifically \\[\\begin{align}\n\\beta_{t|t-1} &= \\mu +F\\beta_{t-1|t-1}                                  &&\\hbox{Predicted } \\beta \\\\\nP_{t|t-1}     &= F P_{t-1|t-1}F' + Q                                    &&\\hbox{Predicted } P \\\\\n\\eta_{t|t-1}  &= y_t-H_t\\beta_{t|t-1}                                   &&\\hbox{Prediction error} \\\\\nf_{t|t-1}     &= H_t P_{t|t-1}H_t' + R                                  &&\\hbox{Pred. error variance} \\\\\n\\beta_{t|t}   &= \\beta_{t|t-1}+P_{t|t-1}H_t' f_{t|t-1}^{-1}\\eta_{t|t-1} &&\\hbox{Updated } \\beta \\\\\nP_{t|t}       &= P_{t|t-1} - P_{t|t-1}H_t' f_{t|t-1}^{-1}H_tP_{t|t-1}   &&\\hbox{Updated } P\n\\end{align}\\] The filter evaluates these recursively, beginning from \\(\\beta_0\\), \\(P_0\\). Treatment of these initial condition reflects knowledge/model:\n\nStationary models can use the steady-state\nNon-stationary models use something which is often (confusingly) called a diffuse prior (zero mean, large variance)"
  },
  {
    "objectID": "Kalman.html#the-kalman-filter-evaluates-the-likelihood",
    "href": "Kalman.html#the-kalman-filter-evaluates-the-likelihood",
    "title": "5  Kalman filtering",
    "section": "5.5 The Kalman filter evaluates the likelihood",
    "text": "5.5 The Kalman filter evaluates the likelihood\nThere is an extremely useful spin-off to using the Kalman filter. For known initial conditions – say \\(\\beta_0 = b_0\\) – the likelihood of a state-space model with \\(T\\) observations of \\(m\\) variables is \\[\\begin{align}\n\\log L(\\theta | y) &= \\sum_{t=1}^T \\log \\left(p (\\theta | \\psi_{t-1})\\right) \\\\\n  &= - \\Phi - \\frac{1}{2}\\sum_{t=1}^T \\left( \\log(\\det(f_{t|t-1})) + \\eta_{t|t-1}' f_{t|t-1}^{-1} \\eta_{t|t-1} | \\psi_{t-1} \\right)\n\\end{align}\\] where \\(\\Phi = \\frac{Tm}{2}\\log \\left( 2\\pi \\right)\\) with \\(\\theta\\) all the non-state parameters to be estimated. We can use the Kalman filter to obtain \\(\\eta_{t|t-1}\\) and \\(f_{t|t-1}\\) as they are the prediction error and its variance. This is the prediction error decomposition of the log-likelihood.\nA maximum likelihood estimate maximizes \\(\\log L(\\theta | y)\\) by choice of \\(\\theta\\).\n\n\n\n\nDurbin, J., and S. J. Koopman. 2001. Time Series Analysis by State Space Methods. Oxford: Oxford University Press.\n\n\nHamilton, J. D. 1994. Time Series Analysis. Princeton, NJ: Princeton University Press.\n\n\nHarvey, Andrew C. 1989. Forecasting, Structural Time Series Models and the Kalman Filter. Cambridge: Cambridge University Press.\n\n\nHarvey, Andrew C., and Richard G. Pierse. 1984. “Estimating Missing Observations in Economic Time Series.” Journal of the American Statistical Association 79 (385): 125–31.\n\n\nJazwinski, Andrew H. 1970. Stochastic Processes and Filtering Theory. Mineola, N.Y.: Dover Publications Inc.\n\n\nKalman, R. E. 1960. “A New Approach to Linear Filtering and Prediction Problems.” Transactions of the ASME Journal of Basic Engineering 82 (Series D): 35–45.\n\n\nKim, Chang-Jin, and Charles R. Nelson. 1999. State-Space Models with Regime Switching: Classical and Gibbs-Sampling Approaches with Applications. MIT Press.\n\n\nTriantafyllopoulos, Kostas. 2021. Bayesian Inference of State Space Models: Kalman Filtering and Beyond. Springer Texts in Statistics. Cham, Switzerland: Springer."
  },
  {
    "objectID": "intro.html#reading-is-good-for-you",
    "href": "intro.html#reading-is-good-for-you",
    "title": "1  Introduction",
    "section": "1.1 Reading is good for you",
    "text": "1.1 Reading is good for you\nFor me, the best (although slightly dated) text is Hastie, Tibshirani, and Friedman (2009) The Elements of Statistical Learning and the best source for the mathematics, with an easy-reading version by some of the same authors James et al. (2021) Introduction to Statistical Learning.\nI also rather like Boehmke and Greenwell (2019) Hands-On Machine Learning with R which is something of a cookbook rather than a technical manual but with wide scope. Taddy (2019) is more elementary.\nOn text, just read Silge and Robinson (2017) Text Mining with R: A Tidy Approach and then Hvitfeldt and Silge (2021) Supervised Machine Learning for Text Analysis in R. That’s it.\nTwo books I would solidly recommend to make us all into better statisticians and not just econometricians are Gelman, Hill, and Vehtari (2019) Regression and Other Stories, and McElreath (2020) Statistical Rethinking.\n\n\n\n\nBlake, Andrew P, and Haroon Mumtaz. 2017. Applied Bayesian Econometrics for Central Bankers. Revised. Technical Books. Centre for Central Banking Studies, Bank of England. https://www.bankofengland.co.uk/-/media/boe/files/ccbs/resources/applied-bayesian-econometrics-for-central-bankers-updated-2017.pdf.\n\n\nBoehmke, Brad, and Brandon M. Greenwell. 2019. Hands-on Machine Learning with R. The R Series. Boca Raton: Chapman & Hall/CRC. https://bradleyboehmke.github.io/HOML/.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2019. Regression and Other Stories. Cambridge: Cambridge University Press. http://www.stat.columbia.edu/~gelman/regression.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. New York, NY: Springer. https://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. Chapman & Hall: CRC Press. https://smltar.com/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. 2nd ed. Springer Texts in Statistics. New York, NY: Springer. https://www.statlearning.com/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Abingdon, Oxfordshire: CRC Press. https://github.com/rmcelreath/rethinking.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. O’Reilly. https://www.tidytextmining.com/.\n\n\nTaddy, Matt. 2019. Business Data Science: Combining Machine Learning and Economics to Optimize, Automate, and Accelerate Business Decisions. New York, NY: McGraw-Hill Education. https://github.com/TaddyLab/BDS."
  },
  {
    "objectID": "R2021.html#selected-ml-and-dataviz-techniques-in-r",
    "href": "R2021.html#selected-ml-and-dataviz-techniques-in-r",
    "title": "2  Non-econometric methods for econometricians",
    "section": "2.1 Selected ML and Dataviz techniques in R",
    "text": "2.1 Selected ML and Dataviz techniques in R\nEconometricians are used to handling data, performing analysis and reporting results. But somewhere along the line data became big and unstructured, analysis was now machines learning about something and outputs became visualisations.\nThis online seminar takes some big(ish) datasets, sets the machines on them and draws some great graphs. If you ever wondered what use a tree was for forecasting or why everything is a network (including neural ones), or wanted to draw a map with your house in it, or to understand a document without the bother of reading it (or a few other things besides) then you might find something to interest you. All done in R.\nSpecifically, this seminar is designed to introduce some of the key methods used outside of econometrics that econometricians will find very useful in their work in a central bank. This includes some important machine learning techniques as a gateway to others, particularly tree-based methods and neural networks, as well as text processing and map making. All the way through there is an emphasis on the network properties of many of these techniques. We make extensive use of the tidyverse, including ggplot2 and tidytext, and a number of statistics, machine learning, geographical data and other packages.\nThe framework for each day is the following:\n\nEach day is divided into two two-hour sessions starting at 10.30 am and 2.00 pm GMT.\nThe first hour of each will be an online presentation covering a particular topic (or topics) with a look at both techniques and code.\nAfter a quick break the second hour will be largely devoted to the code itself or resources to understand how to code the material.\n\nWe may run polls during the event to prioritize the topics covered in the webinars as it is not expected that everyone will be able to try out everything.\n\n2.1.1 The code\nAll code and some of the data will be made available through the Juno portal. For each presentation the .Rmd (R markdown) file is supplied that creates the presentation, an HTML file of the presentation for you to step through which can be re-created from the .Rmd file, and a further .R file of the code that we use. Some additional code and data is included, including links to a number of videos that cover some additional aspects both in this file and in the presentations.\nSome data will need to be downloaded from original other sites if all the examples are to be followed. All code is additionally available at https://github.com/andrewpeterblake/R2020 or https://github.com/andrewpeterblake/R2021 or through the QR codes below.\n\n\n\n \n\n\n\n\n\n2020\n\n\n\n\n \n\n\n\n\n\n2021\n\n\n\n\n \n\n\n\n\n \n\n\nGitHub repositories for historic CCBS courses"
  },
  {
    "objectID": "R2021.html#how-to-ensure-rstudio-finds-the-code",
    "href": "R2021.html#how-to-ensure-rstudio-finds-the-code",
    "title": "2  Non-econometric methods for econometricians",
    "section": "2.2 HOW TO ENSURE RSTUDIO FINDS THE CODE",
    "text": "2.2 HOW TO ENSURE RSTUDIO FINDS THE CODE\nTo use the code, in particular so that R Studio finds the data files etc, create a directory for each topic, (e.g. Trees, ANN etc) and copy the contents from the zip file or GitHub. Then create a new project in R Studio that uses that directory as its home directory, using “File/New Project” in the drop down menu. Opening files within a project sets the home directory to that directory, so everything (including the sub-directories) can be found."
  },
  {
    "objectID": "R2021.html#typical-program-structure",
    "href": "R2021.html#typical-program-structure",
    "title": "2  Non-econometric methods for econometricians",
    "section": "2.3 Typical program structure",
    "text": "2.3 Typical program structure\n\n2.3.1 Day 1: Trees and maps\n\n2.3.1.1 Trees\n\nClassification and regression trees\nEconometrics strikes back: Bootstrap/bagging and Boosting/Model selection\nRandom forests\nVisualising decision trees\nUse example: House prices\n\nThe presentations for this are Trees.html and LondonHP.html; The two programs TreeCancer.R and TreeNW.R are the use examples.\n\n\n2.3.1.2 Maps\n\nHow to draw a map in R\nA guide to some resources\nChoropleths\nUse examples: Climate change, regional data, postcode wrangling\n\nThe presentation for this is MapAER.html (see also Weatherpretty.html); The program MapAERcode.R is the main map drawing code, I’ve included ZAF.R as as short simple way and source for two countries, and the directory Trendz contains the program (app.R) and data for the weather example.\n\n\n\n\n\n\n\n\nI’ve included an additional video (red QR code) for more about Shiny. This uses unemployment data from the Survey of Professional Forecasters. The code we look at is for climate change data World Bank data.\n\n\n\nA comprehensive treatment of maps is Lovelace, Nowosad, and Muenchow (2019) Geocomputation in R, but it is quite a lot to assimilate all at once.\n\n\n\n2.3.2 Day 2: Networks\n\n2.3.2.1 Neural networks\n\nWhat is an ANN? Deep learning?\nFunction approximation via a network\nData: fit, validate, test\nNetwork architecture\nUse examples: House prices revisited\n\nThe presentation for this is IntroANN.html; The program ANN.R replicates the ANN estimation. The data used is the same as for Day 1.\n\n\n2.3.2.2 Networks (real ones)\n\nDAGs and ANNs as network graphs\nIncidence matrices\nMeasuring connectivity: Degree and betweenness\nPlotting with igraph\nUse examples: Industry inter-relationships\n\n\n\n\n \n\n\n\n\n\nCoding club\n\n\n\n\n \n\n\n\n\n\nR-Bloggers article\n\n\n\n\n \n\n\n\n\n \n\n\nNetwork examples\n\n\n \n\n\n \n\n\n\nThe presentation used for the first part of this is DAG.html and the program Draw_DAG_ANN.R draws the ANN examples from Day 2 Session 1 as well as some of the DAG examples. The example is modified from Cunningham (2021) Causal Inference: The Mixtape, which is a great read with R code. The pdf HandShake3.pdf is the source of the director network graphs, and Graph101a.R is a subset of the analytical work on the corruption data set as described in the post Graph Theory 101 (purple QR code), which is the work of Marina Medina (blue QR code link to presentation site).\n\n\n\n2.3.3 Day 3: Text\nText modelling, a ‘tidytext’ approach.\n\n\n\n2.3.3.1 Session 1\n\nData cleaning\nSentiment\nTopic modelling\n\n\n\n2.3.3.2 Session 2\n\nParts-of-speech tagging\nText regression\nUse examples: Central bank minutes, reports\n\n\n\n\n\n\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. New Haven & London: Yale University Press. https://mixtape.scunning.com/.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with R. 1st ed. The R Series. Boca Raton: Chapman & Hall/CRC. https://geocompr.robinlovelace.net/."
  },
  {
    "objectID": "ssm.html#introduction",
    "href": "ssm.html#introduction",
    "title": "3  State-Space Models: time series models with structure",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nWhat is (a) state space (model) and why do we use it? Here we\n\nexplain the model form (and explain what state space is);\ndiscuss the reasons for using them.\n\nThis is best illustrated by discussing a number of example models and applications. We will also explore what this implies for estimation.\nThe standard textbook for this is Harvey (1989) (which understandably feels a little dated both in terms of content and notation) as well as Hamilton (1994), Durbin and Koopman (2001) and Kim and Nelson (1999). Each of these have numerous classical and Bayesian applications and extensions."
  },
  {
    "objectID": "ssm.html#a-useful-class-of-models",
    "href": "ssm.html#a-useful-class-of-models",
    "title": "3  State-Space Models: time series models with structure",
    "section": "3.2 A useful class of models",
    "text": "3.2 A useful class of models\nState-space models are a useful framework amenable to both classical and Bayesian estimation. Their set-up encompasses a number of common economic models with interesting features such as time variation or unobserved components. In either case this implies we need to estimate unknown values of a time series.\nModel form is quite general but has multiple interpretations and is often not unique. There is also quite a lot of additional apparatus required before we can apply either maximum likelihood estimation or Gibbs sampling.\n\n3.2.1 Unobserved variables and coefficients\nA lot of quantities that we routinely use to build models and analyse policy are unobservable: often correspond to concepts with economic meaning rather than being inherently measurable, e.g.:\n\nBusiness cycle or output gap, natural rate of interest, persistent exogenous shocks such as productivity or preferences\nOther models have latent variables with no clear interpretation: popular procedure to reduce large data sets to dynamic factors\n\nAlternatively we might be interested in models with time-varying parameters:\n\nContrasting examples are models with slowly evolving parameters or with regime switches;\nThe coefficients of these models are both unobserved and (potentially) time-varying;\n\nEconometricians face major problems estimating such models: the set up is complicated and data requirements may be excessive. Fortunately there is a method we can use to estimate where we are (‘the state’ of state space) given available observable data.\nThe estimation method we usually use is the Kalman filter, see Kalman (1960). As an estimation process, this has the useful spin-off that we can also use it to calculate the value of the likelihood function at the same time."
  },
  {
    "objectID": "ssm.html#dynamic-economic-models",
    "href": "ssm.html#dynamic-economic-models",
    "title": "3  State-Space Models: time series models with structure",
    "section": "3.3 Dynamic economic models",
    "text": "3.3 Dynamic economic models\nConsider a first order VAR (we’ll show how this isn’t restrictive) \\[\n     x_t = \\mu + A x_{t-1} + v_t\n\\] with \\(v_t\\sim N(0,Q)\\). Estimation of this model is easy – actually very easy as the unrestricted \\(A\\) matrix means we can use OLS (the proof of this is left as an exercise…). Instead we augment this with a second set of equations to turn it into a system where we map the data to the state \\[\n    \\overbrace{y_t}^{Data} = \\overbrace{x_t}^{State}\n\\] This reflects an important characteristic of state space models: all data needs to be fed into the model through observation equations. This seems an odd way to go about estimating a VAR: but we now generalize this to allow that for variables we can’t see.\n\n3.3.1 General state space model\nState space models consist of two sets of equations: (1) Observation equations and (2) State or Transition equations. The classical Kalman filter set up might be:\n\nObservation equations \\[\n\\overbrace{y_t}^{Data} = \\underbrace{H}_{Coefficients}  \\overbrace{\\beta_t}^{State} +\\ e_t\n\\]\nState/Transition equations \\[\n\\beta_t = \\mu + F\\beta_{t-1} + v_t\n\\]\n\nWe assume \\(v_t\\sim N(0,Q)\\), \\(e_t\\sim N(0,R)\\) and \\(cov(e_t,v_t)=0\\). Set up like this, coefficients of \\(H\\) are akin to linear regression parameters. Even more generally there could be further coefficients i.e. \\(y_t = H \\beta_t + B e_t\\).\nNow consider the following variation \\[\n    \\begin{align*}\n    y_t    &= \\underbrace{H_t}_{Data} \\beta_t + e_t \\\\\n     \\beta_t &= \\mu + F \\beta_{t-1} + v_t\n    \\end{align*}\n\\] This allows \\(H_t\\) to potentially vary through time; written like this we can think of \\(H_t\\) as data and \\(\\beta_t\\) as the regression parameters. For the time being consider \\(F\\) and \\(\\mu\\) to be time-invariant and the disturbances to have constant variances, although we can easily generalize this.\nWritten one way the measurement equation is familiar from our understanding of regression models; written another it looks like a time-varying parameter model. The state equation is familiar from our analysis of dynamic macromodels but here looks more like a potential dynamic process for the regression coefficients to vary through time. One big difference is the combination of the two. In particular, this formalizes the role of ‘signal’ – \\(v_t\\) – and ‘noise’ – \\(e_t\\).\nModel allows for the possibility that we neither directly observe the ‘driving’ variables of the system nor do we measure them accurately. To make sense of state space it is convenient to look at a variety of example models."
  },
  {
    "objectID": "ssm.html#examples",
    "href": "ssm.html#examples",
    "title": "3  State-Space Models: time series models with structure",
    "section": "3.4 Examples",
    "text": "3.4 Examples\n\n3.4.1 Structural time series models (STSMs)\nAlternative time series decomposition associated with Harvey (1989). Suggests instead that economic time series might be split into \\[\n    \\begin{align}\n    y_t &=\\mu_t + e_t     &e_t\\sim N\\left(0,\\sigma_v^2\\right) \\\\\n    \\mu_t &=\\mu_{t-1}+\\lambda_{t-1}+\\xi_t   &\\xi_t\\sim N\\left(0,\\sigma_\\xi^2\\right) \\\\\n    \\lambda_t &=\\lambda_{t-1}+\\zeta_t   &\\zeta_t\\sim N\\left(0,\\sigma_\\zeta^2 \\right)\n    \\end{align}\n\\] This is known as the local linear trend model. Both the trend level and slope can vary over time.\nIn state space this is: \\[\n    \\begin{align}\n    y_t &= \\overset{H}{\\begin{bmatrix} 1 & 0 \\end{bmatrix}}\n    \\overset{\\beta_t}{\\begin{bmatrix}\\mu_t \\\\ \\lambda_t \\end{bmatrix} } + e_{t} \\\\\n    \\overset{\\beta_t}{\\begin{bmatrix}\\mu_t \\\\ \\lambda_t \\end{bmatrix}} &=\n    \\overset{F}{\\begin{bmatrix} 1 & 1 \\\\  0 & 1 \\end{bmatrix}}\n    \\overset{\\beta_{t-1}}{\\begin{bmatrix}\\mu_{t-1} \\\\ \\lambda_{t-1} \\end{bmatrix}} +\n    \\begin{bmatrix} \\xi_t \\\\ \\zeta_t\\end{bmatrix}\n    \\end{align}\n\\] The only parameters of this model that we need to estimate are the variances.\n\n\n3.4.2 Trend-cycle model\nWhat if we wanted to decompose GDP into a trend and a cycle. Let observed GDP be modeled as: \\[\n    y_t = \\chi_t + \\tau_t\n    \\] where the cycle \\(\\chi_t\\) is an \\(AR(2)\\) process and the trend \\(\\tau_t\\) a random walk \\[\n    \\begin{align}\n    \\chi_t &= c + \\rho_1 \\chi_{t-1} + \\rho_2 \\chi_{t-2} + v_{1t} \\\\\n    \\tau_t &= \\tau_{t-1} + v_{2t}\n    \\end{align}\n\\] In state space form this is: \\[\n    \\begin{align}\n    y_{t}& =\\overset{H}{\\left[\\begin{array}{ccc} 1 & 0 & 1 \\end{array} \\right] }\n    \\overset{\\beta_t} {\\left[ \\begin{array}{c} \\chi_t \\\\  \\chi_{t-1} \\\\ \\tau_t  \\end{array} \\right] } \\\\\n    \\overset{\\beta_t} {\\left[\\begin{array}{c} \\chi_t \\\\  \\chi_{t-1} \\\\ \\tau_t \\end{array}\\right] }\n    & =\\overset{\\mu }{\\left[\\begin{array}{c} c \\\\  0 \\\\  0 \\end{array} \\right] } +\n        \\overset{F} {\\left[\\begin{array}{ccc} \\rho_1 & \\rho_2 & 0 \\\\\n    1 & 0 & 0 \\\\\n    0 & 0 & 1\n    \\end{array}\n    \\right] }\\overset{\\beta_{t-1}}{\\left[\n    \\begin{array}{c}\n    \\chi_{t-1} \\\\\n    \\chi_{t-2} \\\\\n    \\tau_{t-1}  \n    \\end{array}\n    \\right] }+\\left[\n    \\begin{array}{c}\n    v_{1t} \\\\\n    0 \\\\\n    v_{2t}\n    \\end{array}\n    \\right]\n    \\end{align}\n\\] where \\(R=0\\) (there is no measurement error) and \\(H\\) is time invariant\n\n\n3.4.3 Time-varying parameters (TVP)\nSometimes we wish to model structural change by incorporating time-varying parameters. From the equations above we could set this up as \\[\\begin{align*}\n    y_t     &= H_t \\beta_t + e_t \\\\\n    \\beta_t &= \\beta_{t-1} + v_t\n    \\end{align*}\\] The time-varying coefficients (\\(\\beta_t\\)) multiply a vector of time-varying regressors (\\(H_t\\)). As the data is fixed the distribution of \\(\\beta_t\\) will be conditionally normal if all the errors are normal.\nWe set \\(F=I\\) so the parameters are assumed to follow a random walk and the steady-state variance of \\(\\beta_t\\) is infinite. The model becomes time-invariant with \\(\\beta=\\beta_0\\) if \\(Q = (\\sigma_{\\nu}^2=)\\ 0\\). We now explicitly write the simplest TVP model \\[\n    y_t = c_t + X_t B_t + e_t\n\\] where \\(c_t\\) and \\(B_t\\) both follow a random walk. In state space form \\[\n    \\begin{align}\n    y_t &= \\overset{H_t} {\\begin{bmatrix} 1 & X_t \\end{bmatrix} }\n     \\overset{\\beta_t} {\\begin{bmatrix} c_t \\\\ B_t \\end{bmatrix} } + e_t & var(e) = R \\\\\n     \\overset{\\beta_t} {\\begin{bmatrix} c_t \\\\ B_t \\end{bmatrix} } &=\n     \\overset{\\beta_{t-1}} {\\begin{bmatrix} c_{t-1} \\\\  B_{t-1} \\end{bmatrix}}\n     + \\begin{bmatrix} v_{1t} \\\\ v_{2t} \\end{bmatrix} & var(v) = Q\n    \\end{align}\n\\] where \\(F=I\\) and \\(\\mu=0\\). As \\(\\sigma_{\\nu}^2 \\neq 0\\) the coefficients aren’t fixed even though \\(F=I\\).\n\n\n3.4.4 Dynamic factor model\nAssume a panel of series \\(y_{it}\\) has a common component \\(f_t\\) \\[\n\\begin{align}\n    y_{it} &= B_i f_t + e_{it}, \\qquad &i=1,\\ldots ,N \\\\\n    f_t    &= c+\\rho_1 f_{t-1} + \\rho_2 f_{t-2} + v_t\n\\end{align}\n\\] In state space form \\[\n    \\begin{align*}\n    \\left[\n    \\begin{array}{c}y_{1t} \\\\ y_{2t} \\\\ \\vdots \\\\ y_{Nt}\\end{array}\\right] &=\n    \\overset{H}{\\left[ \\begin{array}{cc}B_1 & 0 \\\\ B_2 & 0 \\\\ \\vdots & \\vdots \\\\ B_N & 0\\end{array}\\right] }\n    \\overset{\\beta_t}{\\left[ \\begin{array}{c}f_t \\\\ f_{t-1} \\end{array}\\right] } + \\left[ \\begin{array}{c} e_{1t} \\\\ e_{2t} \\\\ \\vdots \\\\ e_{Nt} \\end{array}\\right] \\\\\n    \\overset{\\beta_t}{\\left[ \\begin{array}{c} f_t  \\\\ f_{t-1} \\end{array}\\right] } &=\n    \\overset{\\mu }{\\left[  \\begin{array}{c}c \\\\ 0 \\end{array} \\right] }+\n    \\overset{F}{\\left[ \\begin{array}{cc}\n    \\rho_1 & \\rho_2 \\\\  1 & 0 \\end{array} \\right] }\n    \\overset{\\beta_{t-1}}{\\left[  \\begin{array}{c} f_{t-1} \\\\ f_{t-2} \\end{array} \\right] }+\\left[ \\begin{array}{c} v_t \\\\ 0\\end{array}\n    \\right]\n    \\end{align*}\n\\] - The factors have dynamics but the observed variables do not\n\n\n3.4.5 VAR\nAt a more abstract level, a \\(k\\)-variable, \\(p\\)-lag VAR \\[\n  X_t = c + \\sum_{i=1}^p A_i X_{t-i} + \\varepsilon_t,\\quad\\hbox{where}\\ Y_t = X_t\n\\] This can always be written \\[\n    \\begin{align*}\n    Y_t &=\n    \\overset{H}{\\left[ I \\ 0 \\  ...\\right] }\n    \\overset{\\beta_t}{\\left[ \\begin{array}{c}X_t \\\\ X_{t-1} \\\\ \\vdots\\end{array}\\right] } \\\\\n    \\overset{\\beta_t}{\\left[ \\begin{array}{c} X_t  \\\\ X_{t-1} \\\\ \\vdots \\end{array}\\right] } &=\n    \\overset{\\mu}{\\left[  \\begin{array}{c}c \\\\ 0 \\\\ \\vdots\\end{array} \\right] }+\n    \\overset{F}{\\left[ \\begin{array}{ccc}\n    A_1 & A_2 & \\ldots \\\\  I & 0 & \\ldots \\\\ \\vdots & \\vdots & \\ddots \\end{array} \\right] }\n    \\overset{\\beta_{t-1}}{\\left[ \\begin{array}{c}X_{t-1} \\\\ X_{t-2} \\\\ \\vdots\\end{array}\\right] } +\n    \\left[ \\begin{array}{c} \\varepsilon_t \\\\ 0 \\\\ \\vdots\\end{array}\n    \\right]\n    \\end{align*}\n\\] This is an important example: this model has entirely observed variables. The size of the resulting state is \\(p\\times k\\), potentially quite large: however written this way the model is easy to simulate.\nWe can ask the question: can we find unobserved variables that preserve the properties of \\(Y_t\\)? What if: \\[\n     Y_t = X_t = P \\tilde X_t\n\\] then \\[\n    \\tilde X_t = P^{-1}A P \\tilde X_{t-1} + P^{-1} \\varepsilon_t = \\tilde A \\tilde X_{t-1} + \\tilde \\varepsilon_t\n\\] This shows that the state space representation is not unique.\n\n\n3.4.6 Multiple representations: ARMA(1,1)\nThis reflects the fact that model representations in general are not unique. For example, two representations of \\[\n    y_t = \\phi y_{t-1} + \\varepsilon_t + \\theta \\varepsilon_{t-1}  \n\\] This can be written either as \\[\n    y_t = \\left[ 1 \\ 0\\right]  \\left[ \\begin{array}{c}y_t \\\\ \\varepsilon_t \\end{array}\\right], \\qquad\n    \\left[ \\begin{array}{c} y_t  \\\\ \\varepsilon_t \\end{array}\\right]  =\n    \\left[ \\begin{array}{ccc} \\phi & \\theta \\\\ 0 & 0 \\end{array} \\right]\n    \\left[ \\begin{array}{c}y_{t-1} \\\\ \\varepsilon_{t-1} \\end{array}\\right]   +\n    \\left[ \\begin{array}{c} 1 \\\\ 1 \\end{array} \\right] \\varepsilon_t\n\\] or \\[\n    y_t = \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\begin{bmatrix} y_t \\\\ \\theta\\varepsilon_t \\end{bmatrix}, \\qquad\n    \\begin{bmatrix} y_t  \\\\ \\theta\\varepsilon_t \\end{bmatrix} =\n    \\begin{bmatrix} \\phi & 1 \\\\ 0 & 0 \\end{bmatrix}\n    \\begin{bmatrix} y_{t-1} \\\\ \\theta\\varepsilon_{t-1} \\end{bmatrix} +\n    \\begin{bmatrix} 1 \\\\ \\theta \\end{bmatrix} \\varepsilon_t\n\\]\n\n\n3.4.7 DSGE model\nTake a simple New Keynesian model \\[\\begin{align}\ny_t    &= y_{t+1}^e-\\frac{1}{\\sigma} (i_t - \\pi_{t+1}^e) + e_t^1 \\\\\n\\pi_t  &= \\beta \\pi_{t+1}^e + \\kappa y_t + e_t^2 \\\\\ni_t    &= \\gamma i_{t-1} + (1-\\gamma) \\delta \\pi_t + \\varepsilon_t^3 \\\\\ne_t^1  &= \\rho_1 e_{t-1}^1 + \\varepsilon_t^1 \\\\\ne_t^2  &= \\rho_2 e_{t-1}^2 + \\varepsilon_t^2\n\\end{align}\\] The model comprises a dynamic IS curve, a Phillips Curve and a policy rule with smoothing. There are three shocks, two of which are persistent. This we need to write in the general algebraic linear state-space form: \\[\nE\\begin{bmatrix} z_t \\\\ x_{t+1}^e \\end{bmatrix} = A \\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix} + B \\varepsilon_t  \n\\] We map our variables to their algebraic equivalent as (\\(z_t\\), \\(x_t\\)) \\(=\\) ((\\(e^1_t\\), \\(e^2_t\\), \\(i_t\\)), (\\(y_t\\), \\(\\pi_t\\))). Then the model in state-space form but including the matrix \\(E\\) is \\[\n\\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\\\\n                0 & 1 & 0 & 0 & 0 \\\\\n                0 & 0 & 1 & 0 & 0 \\\\\n                1 & 0 & -\\frac{1}{\\sigma} & 1 & \\frac{1}{\\sigma} \\\\\n                0 & 1 & 0 & 0 & \\beta\n\\end{bmatrix}\n\\begin{bmatrix} e^1_t \\\\ e^2_t \\\\ i_t \\\\ y^e_{t+1} \\\\ \\pi^e_{t+1} \\end{bmatrix}\n   =\n   \\begin{bmatrix} \\rho_1 & 0 & 0 & 0 & 0 \\\\\n                0 & \\rho_2 & 0 & 0 & 0 \\\\\n                0 & 0 & \\gamma & 0 & (1-\\gamma)\\delta \\\\\n                0 & 0 & 0 & 1 & 0 \\\\\n                0 & 0 & 0 & -\\kappa & 1\n   \\end{bmatrix}\n\\begin{bmatrix} e^1_{t-1} \\\\ e^2_{t-1} \\\\ i_{t-1} \\\\ y_t \\\\ \\pi_t \\end{bmatrix}    \n   +\n      \\begin{bmatrix}\n                1 & 0 & 0  \\\\\n                0 & 1 & 0 \\\\\n                0 & 0 & 1 \\\\\n                0 & 0 & 0 \\\\\n                0 & 0 & 0\n   \\end{bmatrix}\n   \\begin{bmatrix} \\varepsilon^1_t \\\\ \\varepsilon^2_t \\\\ \\varepsilon^3_t \\end{bmatrix}    \n\\] As the left matrix is invertible we can write this in the form \\[\n\\begin{bmatrix} z_t \\\\ x_{t+1}^e \\end{bmatrix} = C \\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix} + D \\varepsilon_t  \n\\] where \\(C = E^{-1}A\\), \\(D=E^{-1}B\\). However, even in this form it doesn’t quite conform to the standard state space model. This is because we retain the expectations in the model. Typically we need to solve the model for the expectations using the (BK1980?) method or similar such that \\[\nx_t = -N z_{t-1} - G \\varepsilon_t\n\\] Substituting this in we get a final state-space form \\[\\begin{align}\n\\begin{bmatrix} z_t \\\\ x_t \\end{bmatrix} &= \\begin{bmatrix} C_{11} - C_{12}N & 0 \\\\ -N & 0 \\end{bmatrix} \\begin{bmatrix} z_{t-1} \\\\ x_{t-1} \\end{bmatrix} + \\begin{bmatrix} D_1-C_{12}G \\\\ -G \\end{bmatrix} \\varepsilon_t \\\\\n&= P \\begin{bmatrix} z_{t-1} \\\\ x_{t-1} \\end{bmatrix} + Q \\varepsilon_t\n\\end{align}\\] This needs to be augmented with an observation equation, perhaps \\[\n\\begin{bmatrix} i_t \\\\ y_t \\\\ \\pi_t \\end{bmatrix}\n= \\begin{bmatrix} 0 & 0 & 1 & 0 & 0 \\\\\n                0 & 0 & 0 & 1 & 0 \\\\\n                0 & 0 & 0 & 0 & 1\n\\end{bmatrix}\n\\begin{bmatrix} e^1_t \\\\ e^2_t \\\\ i_t \\\\ y_t \\\\ \\pi_t \\end{bmatrix}\n\\] where we don’t observe the autoregressive shocks."
  },
  {
    "objectID": "ssm.html#estimation-problem",
    "href": "ssm.html#estimation-problem",
    "title": "3  State-Space Models: time series models with structure",
    "section": "3.5 Estimation problem",
    "text": "3.5 Estimation problem\nRecall the observation equation and transition equations \\[\n    \\begin{align}\n    y_t       &= H\\beta_t + e_t,         &var(e_t)=R \\\\\n    \\beta_t   &= \\mu + F\\beta_{t-1}+v_t  &var(v_t)=Q\n    \\end{align}\n\\] where here we treat \\(H\\) as time invariant.\nNotice that at least some parameters of the state space model (\\(H\\), \\(\\mu\\), \\(F\\), \\(R\\) and \\(Q\\)) and the state variables (\\(\\beta_t\\)) are both unknown. Simultaneous estimation of both sets of unknowns is usually required. This is usually approached iteratively, with the unknown state estimated conditional on some initial estimate of the parameters (or in the Bayesian case sampled) using an appropriate method, usually the Kalman Filter and then then the parameters updated conditional on the state which is repeated until convergence. This can be done using Gibb’s Sampling if we can derive the a conditional density for the unobserved components.\n\n\n\n\nDurbin, J., and S. J. Koopman. 2001. Time Series Analysis by State Space Methods. Oxford: Oxford University Press.\n\n\nHamilton, J. D. 1994. Time Series Analysis. Princeton, NJ: Princeton University Press.\n\n\nHarvey, Andrew C. 1989. Forecasting, Structural Time Series Models and the Kalman Filter. Cambridge: Cambridge University Press.\n\n\nKalman, R. E. 1960. “A New Approach to Linear Filtering and Prediction Problems.” Transactions of the ASME Journal of Basic Engineering 82 (Series D): 35–45.\n\n\nKim, Chang-Jin, and Charles R. Nelson. 1999. State-Space Models with Regime Switching: Classical and Gibbs-Sampling Approaches with Applications. MIT Press."
  },
  {
    "objectID": "RegressionLemma.html#a-different-view-of-regression",
    "href": "RegressionLemma.html#a-different-view-of-regression",
    "title": "4  The regression lemma",
    "section": "4.1 A different view of regression",
    "text": "4.1 A different view of regression\nHere we derive a simple characterization of predicting a vector of unknown stochastic variables using observed data that is essentially regression using population statistics rather than using a sample proxy. Anderson (2004), Hamilton (1994) and many multivariate statistics textbooks contain similar treatments.\n\n4.1.1 Unconditional moments\nLet \\(z\\) be vector of stochastic variables such that \\[\n  z\\sim N(\\mu,\\Sigma)\n\\] or \\[\n\\mathbb{E}\\left[ \\left(z-\\mathbb{E}[z]\\right) \\left(z-\\mathbb{E}[z]\\right)' \\right] = \\mathbb{E}\\left[ \\left(z-\\mu\\right) \\left(z-\\mu\\right)'\\right] = \\Sigma\n\\] Divide the vector into two, \\(z_1\\) and \\(z_2\\) so that \\[\n\\begin{bmatrix}\nz_1 \\\\\nz_2\n\\end{bmatrix}\n\\sim N\\left(\n\\begin{bmatrix}\n\\mu_1 \\\\\n\\mu_2\n\\end{bmatrix},\n\\begin{bmatrix}\n\\Sigma_{11} & \\Sigma_{12} \\\\\n\\Sigma_{21} & \\Sigma_{22}\n\\end{bmatrix}\n\\right)\n\\] The unconditional covariance of \\(z_1\\) is unaffected by the known covariance between it and \\(z_2\\), \\(\\Sigma_{12}\\).\n\n\n4.1.2 Conditional mean for \\(z_1\\)\nHow do we best use the information in an observed part of the vector \\(z\\) – say \\(z_2\\) – to better predict the unobserved part?\nAssume there is linear relationship between the two \\(z\\) variables such that\n\\[\nz_1 = Bz_2 + \\varepsilon\\qquad \\text{or}\\qquad \\varepsilon =z_1-Bz_2\n\\] Now choose \\(B\\) such that \\(\\varepsilon\\) and \\(z_2\\) are uncorrelated – a value of \\(B\\) where this holds is a fundamental assumption for a regression model as it implies there is no useful information in \\(z_2\\) that we aren’t using. This implies\n\\[\n\\begin{aligned}\n0 &= \\mathbb{E}\\left[ \\left(\\varepsilon - \\mathbb{E}[\\varepsilon ]\\right) \\left(z_2 - \\mathbb{E}[z_2]\\right)'\\right]\\\\\n   &= E\\left[ \\left(z_1 - Bz_2 - \\mathbb{E}[z_1-Bz_2]\\right) \\left(z_2 - \\mathbb{E}[z_2]\\right)'\\right] \\\\\n   &= E\\left[ \\left(z_1 - \\mu_1 - B\\left(z_2-\\mu_2\\right) \\right) \\left( z_2-\\mu_2\\right)'\\right] \\\\\n   & =\\Sigma_{12}-B\\Sigma_{22}\n\\end{aligned}   \n\\] We recover \\(B=\\Sigma_{12}\\Sigma_{22}^{-1}\\). This is simply regression using a ‘method of moments’ characterization where we use population rather sample moments. For known conditioning values \\(z_2\\) and known \\(\\Sigma_{12}\\), \\(\\Sigma_{22}\\) \\[\n\\begin{aligned}\n  \\mathbb{E}\\left[ z_1|z_2\\right] &= \\mathbb{E}\\left[ Bz_2+\\varepsilon |z_2\\right] \\\\\n                         &= Bz_2 + \\mathbb{E}\\left[\\varepsilon \\right] \\\\\n                         &= Bz_2 + \\mathbb{E}\\left[z_1-Bz_2\\right] \\\\\n                         &= B z_2 + \\mu_1 - B \\mu_2 \\\\\n                         &= \\mu_1 + B(z_2-\\mu_2)\n\\end{aligned}\n\\] Plugging in the formula for \\(B\\) gives the conditional expectation of \\(z_1\\) given \\(z_2\\) as \\[\n  \\mathbb{E}[z_1 | z_2] = \\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1}(z_2-\\mu_2)\n\\] This is a formula that updates the unconditional expectation by a weighted average of the “innovation” (deviation from predicted value) of the observed variables."
  },
  {
    "objectID": "RegressionLemma.html#example",
    "href": "RegressionLemma.html#example",
    "title": "4  The regression lemma",
    "section": "4.2 Example",
    "text": "4.2 Example\nAs an example, consider the \\(4\\times 4\\) covariance matrix \\(\\Sigma\\)\n\nlibrary(kableExtra)\n\nSigma &lt;- matrix(c(rWishart(100,4,diag(4))),4,4)\n\nWarning in matrix(c(rWishart(100, 4, diag(4))), 4, 4): data length differs from\nsize of matrix: [1600 != 4 x 4]\n\nB     &lt;- Sigma[1:2,3:4] %*% solve(Sigma[3:4,3:4])\n\nmat_print &lt;- function(x) {\n  paste(c(\"\\\\left[\\\\begin{array}{r}\", \n          paste(c(t(x)),\n                rep(c(rep(\"&\", nrow(x)-1), '\\\\\\\\'), ncol(x)),\n                collapse=\"\"), \n          \"\\\\end{array}\\\\right]\"), collapse=\"\")\n}\n\n\\[\n\\Sigma = \\left[\\begin{array}{r}1.28 &1.74 &-0.25 &-0.52 \\\\1.74 &10.16 &-4.79 &3.12 \\\\-0.25 &-4.79 &4.44 &-1.86 \\\\-0.52 &3.12 &-1.86 &2.42 \\\\\\end{array}\\right]\n\\] It is a simple matter to generate four sample sequences with this covariance. We set the constants \\(\\mu_i^j = 0\\) for \\(i,j = 1,2\\) as we subtract them out anyway.\n\nlibrary(tidyverse)\nnobs        &lt;- 42\nZ           &lt;- matrix(rnorm(nobs*4), nobs, 4) %*% chol(Sigma)\ncolnames(Z) &lt;- paste0(\"z\", c(\"11\",\"12\",\"21\",\"22\"))\nZZ   &lt;- data.frame(Z, x = 0:(nobs-1)) %&gt;% \n  pivot_longer(cols = -x, names_to = \"Var\",values_to = \"Val\") %&gt;% \n  mutate(F = as_factor(Var), \n         F = recode_factor(F, \n                           z11 = \"z[1]^1\", \n                           z12 = \"z[1]^2\", \n                           z21 = \"z[2]^1\", \n                           z22 = \"z[2]^2\"))\nZZm   &lt;- select(ZZ, F) %&gt;% \n  unique(F) %&gt;% \n  mutate(C = as.factor(c(1,1,2,2)))\n\nggplot(ZZ) +   \n  geom_rect(data=ZZm, aes(fill=C), \n            xmin=-Inf, xmax=Inf, \n            ymin=-Inf, ymax=Inf, alpha=0.15) + \n  geom_line(aes(x=x, y=Val, color=F)) +\n  # scale_fill_manual(values = c(\"red\", \"blue\")) + \n  facet_wrap(~F, ncol=2, labeller=\"label_parsed\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x=\"\",y=\"\")\n\n\n\n\nFour sample series with covariance matrix \\(\\Sigma\\)\n\n\n\n\nDenote the first two series \\(z_1^1\\) and \\(z_1^2\\), and the second two \\(z_2^1\\) and \\(z_2^2\\); thus \\(z_1\\) from earlier is a vector of the first two (\\(z_2^1\\) and \\(z_2^2\\)) and \\(z_2\\) a vector of the second two (\\(z_2^1\\) and \\(z_2^2\\)).\n\n4.2.0.1 Problem\nIf we know \\(\\Sigma\\), and can observe these last two series (on light blue backgrounds), how well can we reconstruct the first (light red backgrounds) two ?\n\n\n4.2.1 Solution\nCalculate \\(B\\) from the above formula. By inspection \\[\n\\Sigma_{22} = \\left[\\begin{array}{r}4.44 &-1.86 \\\\-1.86 &2.42 \\\\\\end{array}\\right]\n\\] and \\[\n\\Sigma_{12} = \\left[\\begin{array}{r}-0.25 &-0.52 \\\\-4.79 &3.12 \\\\\\end{array}\\right]\n\\] so \\(B = \\Sigma_{12}\\Sigma_{22}^{-1}\\) is \\[\nB = \\left[\\begin{array}{r}-0.214 &-0.377 \\\\-0.795 &0.679 \\\\\\end{array}\\right]\n\\] Calculating the predicted variables and plotting them on the same graph we get\n\nZZfit &lt;- data.frame(Z[,3:4] %*% t(B), x = 0:(nobs-1)) %&gt;% \n  rename_all(~ c(\"z11\", \"z12\", \"x\")) %&gt;% \n  gather(Var, Fit, -x) %&gt;% \n  mutate(F = as_factor(Var), \n         F = recode_factor(F, z11=\"z[1]^1\", z12=\"z[1]^2\"))\n\nZZhat &lt;- left_join(ZZ, ZZfit)\n\nJoining with `by = join_by(x, Var, F)`\n\ncors &lt;- ZZhat %&gt;% \n  filter(!is.na(Fit)) %&gt;% \n  group_by(Var) %&gt;% \n  mutate(CC = cor(Val, Fit)) %&gt;% \n  summarise(Cor = mean(CC))\n\nggplot(ZZhat) + \n  geom_rect(data=ZZm, aes(fill=C), \n            xmin=-Inf, xmax=Inf, \n            ymin=-Inf, ymax=Inf, alpha=0.15) + \n  geom_line(aes(x=x, y=Val, color=F)) +\n  geom_line(aes(x=x, y=Fit, group=F), linetype=\"dotted\", color=\"black\", size=0.6) +\n  facet_wrap(~F, ncol=2, labeller=\"label_parsed\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x=\"\",y=\"\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Removed 84 rows containing missing values (`geom_line()`).\n\n\n\n\n\nPlots of actual and predicted first and second series\n\n\n\n\nwhere the correlations between the variables and their fitted values are 0.59, 0.76 respectively.\n\n\n4.2.2 Observing different \\(z\\) variables\nWhat if instead of observing both of the last \\(z\\) variables we only observe one, say the last one? Nothing we have said so far restricts us from having only one observable, or three – and of course ordering is arbitrary.\nLets assume we just see the last variable. Calculate \\(B\\) from the above formula. As before, by inspection \\[\n\\Sigma_{22} = \\left[\\begin{array}{r}2.42 \\\\\\end{array}\\right]\n\\] and \\[\n\\Sigma_{12} = \\left[\\begin{array}{r}-0.52 &3.12 &-1.86 \\\\\\end{array}\\right]'\n\\] and as now \\(\\Sigma_{22}\\) is a scalar we can write \\(B = \\Sigma_{12}/\\Sigma_{22}\\) which is\n\nB &lt;- Sigma[1:3,4, drop=FALSE]/Sigma[4,4]\n\n\\[\nB = \\left[\\begin{array}{r}-0.213 &1.289 &-0.766 \\\\\\end{array}\\right]'\n\\] Applying this we get predictions of the three unknown variables which are\n\ncolnames(Z) &lt;- paste0(\"z\", c(\"11\",\"12\",\"13\",\"21\"))\nZZ   &lt;- data.frame(Z, x = 0:(nobs-1)) %&gt;% \n  pivot_longer(cols = -x, names_to = \"Var\",values_to = \"Val\") %&gt;% \n  mutate(F = as_factor(Var), \n         F = recode_factor(F, \n                           z11 = \"z[1]^1\", \n                           z12 = \"z[1]^2\", \n                           z13 = \"z[1]^3\", \n                           z21 = \"z[2]^1\"))\nZZm   &lt;- select(ZZ, F) %&gt;% \n  unique(F) %&gt;% \n  mutate(C = as.factor(c(1,1,1,2)))\n\nZZfit &lt;- data.frame(Z[,4] %*% t(B), x = 0:(nobs-1)) %&gt;% \n  rename_all(~ c(\"z11\", \"z12\", \"z13\", \"x\")) %&gt;% \n  gather(Var, Fit, -x) %&gt;% \n  mutate(F = as_factor(Var), \n         F = recode_factor(F, z11=\"z[1]^1\", z12=\"z[1]^2\", z13=\"z[1]^3\"))\n\nZZhat &lt;- left_join(ZZ, ZZfit)\n\nJoining with `by = join_by(x, Var, F)`\n\ncors &lt;- ZZhat %&gt;% \n  filter(!is.na(Fit)) %&gt;% \n  group_by(Var) %&gt;% \n  mutate(CC = cor(Val, Fit), r2 = summary(lm(Fit ~ Val))$r.squared) %&gt;% \n  summarise(Cor = mean(CC), R2 = mean(r2))\n\nggplot(ZZhat) + \n  geom_rect(data=ZZm, aes(fill=C), \n            xmin=-Inf, xmax=Inf, \n            ymin=-Inf, ymax=Inf, alpha=0.15) + \n  geom_line(aes(x=x, y=Val, color=F)) +\n  geom_line(aes(x=x, y=Fit, group=F), linetype=\"dotted\", color=\"black\", size=0.6) +\n  facet_wrap(~ F, ncol=2, labeller=\"label_parsed\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(x=\"\",y=\"\")\n\nWarning: Removed 42 rows containing missing values (`geom_line()`).\n\n\n\n\n\nPlots of actual and predicted series, one predictor\n\n\n\n\nwhere we have color coded the backgrounds accordingly. In general, predictions are usually a bit worse, but sometimes almost as good – and may even look a bit better! Comparing them to our previous results the correlations between the variables and their predicted values are now 0.34, 0.51, 0.43 respectively, where of course we are predicting one more series now.\n\n\n4.2.3 Remarks\nDepending on the covariance matrix, correlation can be a lot lower. If the \\(z_1\\) and \\(z_2\\) vectors are uncorrelated then \\(\\Sigma_{12}\\approx 0\\) so \\(B\\approx 0\\).In these circumstances the best prediction is then close to the unconditional mean; if \\(\\Sigma_{11}&gt;0\\) then predictions will be poor although the absolute value of those errors depends on \\(\\Sigma_{11}\\).\nThis also illustrates that there are some variables that are better at predicting the things you are interested in than others. It practice it is frequently the case that there is low correlation between predicted and actual – as ever, you need good regressors to get a good fit between predicted and actual."
  },
  {
    "objectID": "RegressionLemma.html#updating-the-update-and-assessing-uncertainty",
    "href": "RegressionLemma.html#updating-the-update-and-assessing-uncertainty",
    "title": "4  The regression lemma",
    "section": "4.3 Updating the update and assessing uncertainty",
    "text": "4.3 Updating the update and assessing uncertainty\n\n4.3.1 Updating an estimate with new information\nThis result easily generalizes: what if we already have an estimate but we get some new information that we could use to improve our estimate? We can solve this using the formula we have already derived. In effect we can divide \\(z_2\\) and include some further variables denoted \\(v\\) such that \\[\n\\begin{bmatrix}\nz_1 \\\\\nz_2 \\\\\nv\n\\end{bmatrix}\n\\sim N\\left(\n\\begin{bmatrix}\n\\mu_1 \\\\\n\\mu_2 \\\\\n0\n\\end{bmatrix},\n\\begin{bmatrix}\n\\Sigma_{11} &\\Sigma_{12} & \\Sigma_{1v} \\\\\n\\Sigma_{21} &\\Sigma_{22} & 0 \\\\\n\\Sigma_{v1} & 0 & \\Sigma_{vv}\n\\end{bmatrix}\n\\right)\n\\] Notice we have imposed that \\(v\\) is mean zero (\\(\\mu_v = 0\\)) and uncorrelated with \\(z_2\\) (\\(\\Sigma_{v2} = \\Sigma_{2v}'=0\\)). It is important that they are uncorrelated with previous conditioning variables but not the variables to be predicted so \\(\\Sigma_{1v}\\ne 0\\). In this way \\(v\\) represents news. We couldn’t have predicted it from information we already had.\nConditioning \\(z_1\\) on \\((z_2,v)\\) we get \\[\n\\begin{aligned}\n\\mathbb{E}[z_1|z_2,v] &= \\mu_1 +\n\\begin{bmatrix} \\Sigma_{12} & \\Sigma_{1v} \\end{bmatrix}\n\\begin{bmatrix} \\Sigma_{22}^{-1} & 0 \\\\ 0 & \\Sigma_{vv}^{-1} \\end{bmatrix}\n\\begin{bmatrix} z_2-\\mu_2 \\\\  v  \\end{bmatrix} \\\\\n             &= \\mu_1 +\\Sigma_{12}\\Sigma_{22}^{-1}(z_2-\\mu_2)+\\Sigma_{1v}\\Sigma_{vv}^{-1}v\n\\end{aligned}\n\\] We can further write this \\[\n\\mathbb{E}[z_1|z_2, v] = \\mathbb{E}[z_1|z_2] +\\Sigma_{1v} \\Sigma_{vv}^{-1}v\n\\]\n\n\n4.3.2 Covariances\nSo far concentrated on the expected value, but can similarly construct the conditional covariance. The conditional variance for the \\(z_1\\) given \\(z_2\\) is \\[\n\\begin{align}\nvar(z_1|z_2) &= \\mathbb{E}\\left[\\left(z_1 - \\mathbb{E}(z_1|z_2) \\right) \\left(z_1 - \\mathbb{E}(z_1|z_2) \\right)' \\right] \\\\\n&= \\mathbb{E}\\left[ \\left(z_1 -\\mu_1-\\Sigma_{12}\\Sigma_{22}^{-1}(z_2-\\mu_2) \\right) \\left(z_1-\\mu_1 -\\Sigma_{12}\\Sigma_{22}^{-1} (z_2-\\mu_2) \\right)' \\right] \\\\\n&= \\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21} + \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{22}\\Sigma_{22}^{-1}\\Sigma_{21} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21} \\\\\n&= \\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\n\\end{align}\n\\] so \\[\n  var(z_1|z_2) = \\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\n\\] This tells us that the conditional covariance can be no bigger than the unconditional one. We get an efficiency gain.\nWhen we consider updating the covariance of a conditional expectation it follows that \\[\n\\begin{align}\nvar(z_1|z_2,v) &= \\Sigma_{11}-\n   \\begin{bmatrix} \\Sigma_{12} & \\Sigma_{1v} \\end{bmatrix}\n   \\begin{bmatrix} \\Sigma_{22}^{-1} & 0 \\\\  0 & \\Sigma_{vv}^{-1} \\end{bmatrix}\n   \\begin{bmatrix} \\Sigma_{21} \\\\ \\Sigma_{v1} \\end{bmatrix} \\\\\n  &= \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21} - \\Sigma_{1v}\\Sigma_{vv}^{-1}\\Sigma_{v1} \\\\\n  &= var(z_1|z_2) - \\Sigma_{1v}\\Sigma_{vv}^{-1}\\Sigma_{v1}\n\\end{align}\n\\] It follows that the covariance here must be no bigger than the variance conditioned on \\(z_2\\) alone, consistent with our previous result.\n\n\n\n\nAnderson, T. W. 2004. An Introduction to Multivariate Statistical Analysis. 3rd ed. New York: John Wiley; Sons.\n\n\nHamilton, James D. 1994. Time Series Analysis. Princeton, NJ: Princeton University Press."
  },
  {
    "objectID": "QR.html#getting-the-data",
    "href": "QR.html#getting-the-data",
    "title": "7  Quantile regression",
    "section": "7.1 Getting the data",
    "text": "7.1 Getting the data\nWe download the data and save it locally.\n\nh &lt;- \"https://www.philadelphiafed.org/-/media/frbp/assets/surveys-and-data/survey-of-professional-forecasters/historical-data/\"\nf &lt;- \"meanlevel.xlsx\"\n\ndownload.file(paste0(h, f), destfile=f, mode=\"wb\")\n\nRetrieve the unemployment data for the average unemployment forecast.\n\nUNEMP &lt;- f %&gt;%\n  read_excel(na=\"#N/A\", sheet=\"UNEMP\") %&gt;% \n  mutate(Date=as.Date(as.yearqtr(paste(YEAR, QUARTER), format=\"%Y %q\"))) \n\nUsel &lt;- UNEMP %&gt;% \n  select(Date, UNEMP1, UNEMP3, UNEMP4, UNEMP5, UNEMP6) %&gt;%\n  mutate(UNRATE = lead(UNEMP1,1)) %&gt;%\n  select(Date, UNRATE, \n         UNEMP1=UNEMP3, UNEMP2=UNEMP4, UNEMP3=UNEMP5, UNEMP4=UNEMP6) %&gt;%\n  mutate(UNEMP1 = lag(UNEMP1,1), \n         UNEMP2 = lag(UNEMP2,2), \n         UNEMP3 = lag(UNEMP3,3), \n         UNEMP4 = lag(UNEMP4,4)) %&gt;%\n  pivot_longer(cols = -c(Date, UNRATE), names_to=\"Which\", values_to=\"Val\") %&gt;%\n  filter(year(Date) &gt; 2000)"
  },
  {
    "objectID": "QR.html#plots",
    "href": "QR.html#plots",
    "title": "7  Quantile regression",
    "section": "7.2 Plots",
    "text": "7.2 Plots\n\nUsel %&gt;% \n  ggplot(aes(x=Date)) + \n  geom_line(aes(y=UNRATE), colour=\"red\") + \n  geom_point(aes(y=Val, colour=Which, shape=Which)) +\n  theme_light() + \n  labs(title=\"Mean unemployment forecasts\", x=\"\", y=\"\", caption=\"Source: SPF\")"
  },
  {
    "objectID": "Gibbs.html",
    "href": "Gibbs.html",
    "title": "9  Gibbs Sampling",
    "section": "",
    "text": "10 Example: linear regression model\nFor the specific linear model \\[\n   y_t = \\alpha + \\beta_1 X_{1t} + \\beta_2 X_{2t}+v_t\\text{, }v_t\\sim N(0,\\sigma^2)\n\\]"
  },
  {
    "objectID": "Gibbs.html#gibbs-sampling-in-a-table",
    "href": "Gibbs.html#gibbs-sampling-in-a-table",
    "title": "9  Gibbs Sampling",
    "section": "9.1 Gibbs sampling in a table",
    "text": "9.1 Gibbs sampling in a table\nThink about a the joint density of some parameters of interest for a discrete distribution:\n\n\n\n\n\n\n\n\n\nσ₁\n\n\nσ₂\n\n\nσ₃\n\n\nMarginal β\n\n\n\n\n\n\nβ₁\n\n\n0.10\n\n\n0.20\n\n\n0.30\n\n\n0.6\n\n\n\n\nβ₂\n\n\n0.10\n\n\n0.05\n\n\n0.05\n\n\n0.2\n\n\n\n\nβ₃\n\n\n0.05\n\n\n0.10\n\n\n0.05\n\n\n0.2\n\n\n\n\nMarginal σ\n\n\n0.25\n\n\n0.35\n\n\n0.40\n\n\n1.0\n\n\n\n\n\nTable 1 Joint probabilities\n\n\nFor this discrete example, the sums across the columns and rows are the marginal densities for the parameters: they don’t depend on the other parameter. Gibbs sampling estimates these sums by constructing sequences from the conditional densities that have the right joint density."
  },
  {
    "objectID": "Gibbs.html#the-gibbs-sampler",
    "href": "Gibbs.html#the-gibbs-sampler",
    "title": "9  Gibbs Sampling",
    "section": "9.2 The Gibbs’ Sampler",
    "text": "9.2 The Gibbs’ Sampler\nSuppose there are \\(k\\) variables \\(\\phi_i\\) jointly distributed \\[\n   J(\\phi_1,\\phi_2,...,\\phi_k)\n\\] For inferential purposes we are interested in the marginal distributions denoted \\[\n    G(\\phi_i),\\quad i=1,...,k\n\\] Gibbs sampling is a technique that generates a sequence of values that have the same distribution as the underlying marginals. It doesn’t use the joint density but instead a sequence of conditionals densities \\[\n    H(\\phi_i|\\Phi_{j\\ne i}),\\quad i=1,...,k\n\\] where \\(\\Phi_{j\\ne i}\\) are all other parameters. We will look at the procedure first and then at how it works in an example."
  },
  {
    "objectID": "Gibbs.html#gibbs-sampling-is-the-following-steps",
    "href": "Gibbs.html#gibbs-sampling-is-the-following-steps",
    "title": "9  Gibbs Sampling",
    "section": "9.3 Gibbs sampling is the following steps",
    "text": "9.3 Gibbs sampling is the following steps\n\nStep 0 Set starting values for \\(\\phi_1,...,\\phi_k\\) \\[\n  \\phi_1^0,\\ \\phi_2^0,\\ ...,\\ \\phi_k^0\n\\]\nStep 1 Sample \\(\\phi_1^1\\) from \\[\n   H(\\phi_1^1\\ |\\ \\phi_2^0,\\ \\phi_3^0,\\ ...,\\ \\phi_k^0)\n\\]\nStep 2 Sample \\(\\phi_2^1\\) from \\[    \n  \\begin{gather*}\n  H(\\phi_2^1\\ |\\ \\phi_1^1,\\ \\phi_3^0,\\ ...,\\ \\phi_k^0)  \\\\\n  \\vdots\n  \\end{gather*}\n\\]\nStep \\(k\\) Sample \\(\\phi_k^1\\) from \\[\n  H(\\phi_k^1\\ |\\ \\phi_1^1,\\ \\phi_2^1,\\ ...,\\ \\phi_{k-1}^1)\n\\] to complete one iteration.\n\nRepeat for \\(n\\) iterations and save the last \\(n-p\\) values of \\(\\phi_i^j\\) for every \\(i=1,...,k\\). As \\(n \\rightarrow \\infty\\) the joint and marginal distributions of the simulated \\(\\phi_1^j,\\ ...,\\ \\phi_k^j\\) converge at an exponential rate to the joint and marginal distributions of \\(\\phi_1,\\ ...,\\ \\phi_k\\). Then the joint and marginal distributions can be approximated by the empirical distribution.\nFor example, the estimated mean of the marginal distribution of \\(\\phi_i\\) is \\[\n   \\bar \\phi_i = \\frac{\\sum_{j=p+1}^n \\phi_i^j}{n-p}\n\\] where we discard the first \\(p\\) draws."
  },
  {
    "objectID": "Gibbs.html#estimating-a-model-for-us-inflation",
    "href": "Gibbs.html#estimating-a-model-for-us-inflation",
    "title": "9  Gibbs Sampling",
    "section": "10.1 Estimating a model for US inflation",
    "text": "10.1 Estimating a model for US inflation\n\n\n\n\n\n\n\n\n\n\nChoose an \\(AR(4)\\) \\[\n\\pi_t = \\alpha + \\beta_1 \\pi_{t-1} + \\beta_2 \\pi_{t-2} + \\beta_3 \\pi_{t-3} + \\beta_4 \\pi_{t-4} + v_t\\text{, }v_t\\sim N(0,\\sigma^2)\n\\] and write \\(\\beta' = [\\alpha\\ \\beta_1\\ \\beta_2\\ \\beta_3\\ \\beta_4]'\\)\nPriors are \\[\nP(\\beta) \\sim N\\left( \\underset{\\beta_0} {\\left[\n\\begin{array}{c}\n0 \\\\\n1 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{array}\n\\right],}\\ \\underset{\\Sigma} {\\eta\\left[\n\\begin{array}{ccc}\n1 & 0 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 0 & 1\n\\end{array}\n\\right] }\\right), \\qquad\nP(\\sigma^2) \\sim \\Gamma^{-1}\\left( \\frac{1}{2},\\frac{1}{2}\\right)\n\\] where \\(\\eta\\) is a scalar we will use to adjust prior tightness\nPrior model consistent with a random walk for inflation\n\n\n\n\n\n\nRegression results\n\n\n\n\nCoefficient\n\n\nEstimate\n\n\nSE\n\n\nt-stat\n\n\n\n\n\n\nINF_1\n\n\n1.313\n\n\n0.064\n\n\n20.530\n\n\n\n\nINF_2\n\n\n-0.337\n\n\n0.106\n\n\n-3.191\n\n\n\n\nINF_3\n\n\n0.100\n\n\n0.105\n\n\n0.952\n\n\n\n\nINF_4\n\n\n-0.140\n\n\n0.064\n\n\n-2.171\n\n\n\n\nconstant\n\n\n0.247\n\n\n0.076\n\n\n3.265\n\n\n\n\nsigma\n\n\n0.684\n\n\nNA\n\n\nNA\n\n\n\n\n\nLinear regression table (n.b. no priors)\n\n\n\n10.1.1 Gibbs samples\n\n\n\n\n\nSample output for 2000 replications, long burn in\n\n\n\n\n\n\n\n\n\nBayesian estimates\n\n\n\n\nParameter\n\n\nExpected\n\n\nSE\n\n\nlower_5\n\n\nupper_95\n\n\n\n\n\n\nalpha\n\n\n0.265\n\n\n0.043\n\n\n0.202\n\n\n0.337\n\n\n\n\nbeta[1]\n\n\n1.268\n\n\n0.123\n\n\n1.061\n\n\n1.452\n\n\n\n\nbeta[2]\n\n\n-0.331\n\n\n0.141\n\n\n-0.555\n\n\n-0.102\n\n\n\n\nbeta[3]\n\n\n0.093\n\n\n0.107\n\n\n-0.075\n\n\n0.261\n\n\n\n\nbeta[4]\n\n\n-0.164\n\n\n0.063\n\n\n-0.261\n\n\n-0.068\n\n\n\n\nsigma\n\n\n0.771\n\n\n0.603\n\n\n0.430\n\n\n1.813\n\n\n\n\n\nCalculate descriptive statistics from Gibbs Samples\n\n\n\n\n\n\n\nGibbs sampling generates estimates of the full marginal distributions of the parameters\n\n\n\n\nWe can look at the impact of tighter priors and more samples. What should we expect?\n\nMore samples will generate smoother-looking density plots, increase precison (maybe not by as much as you think)\nTighter priors will move estimates towards the prior: we can, say, reduce \\(\\eta\\) or increase degrees of freedom for \\(\\sigma\\)\n\n\n\n\n\n\nHistograms/density estimates for 7000 replications, long burn in\n\n\n\n\n\n\n\n\n\nHistograms/density estimates for 7000 samples, \\(\\eta=.025\\), long burn in"
  },
  {
    "objectID": "CK.html#using-the-kalman-filter",
    "href": "CK.html#using-the-kalman-filter",
    "title": "10  Carter-Kohn",
    "section": "10.1 Using the Kalman Filter",
    "text": "10.1 Using the Kalman Filter\nEstablish the usefulness of the Kalman Filter (and not just for state estimation). Refresh idea of maximum likelihood estimation in the context of state space models.\n\nIntroduce smoothing\nDevelop Gibbs sampling by the Carter-Kohn method\nAll of these use the Kalman Filter to develop conceptually different tools\n\nFollow Kim and Nelson (1999); also see Harvey (1989), Hamilton (1994), Durbin and Koopman (2001)"
  },
  {
    "objectID": "CK.html#maximum-likelihood",
    "href": "CK.html#maximum-likelihood",
    "title": "10  Carter-Kohn",
    "section": "10.2 Maximum likelihood",
    "text": "10.2 Maximum likelihood\n\n10.2.1 Classical Maximum Likelihood Estimation\nThe principle of maximum likelihood is that the parameters should be chosen so that the probability of observing a given sample is maximized.\nFor time series models the joint density of \\(\\psi_T = \\{y_T, y_{T-1},\\ldots ,y_1 \\}\\) and parameters \\(\\theta\\) in conditional form is \\[\\begin{equation}\np(\\theta|\\psi_T) = \\prod\\nolimits_{t=1}^T p (y_t|\\psi_{t-1},\\theta)\n\\end{equation}\\] emphasizing the serial dependence of observations.\nInterpret this as the likelihood for a particular sample. Assuming (conditional) normality, the likelihood of any particular \\(n\\)-vector of observations is \\[\\begin{equation}\np(y_t) = (2\\pi)^{-\\frac{n}{2}}|var(y_t)|^{-\\frac{1}{2}}e^{\\left\\{ -\\frac{1}{2}(y_t-\\mu )' var(y_t)^{-1}(y_t-\\mu)\\right\\} }\n\\end{equation}\\] Notice this depends on the observed data and the values of the parameters. It can be multivariate and for any underlying density. A maximum likelihood (ML) estimate of \\(\\theta\\) maximizes the likelihood of the parameter given an observed sample.\n\n\n10.2.2 Poisson example\nTh Poisson distribution is a nice one to consider as the maximum likelihood estimate can be calculated easily – essentially in your head.1\nThe Poisson distribution is \\[\\begin{equation}\np(y_i,\\ \\theta )=\\frac{e^{-\\theta }\\theta ^{y_i}}{y_i!}\n\\end{equation}\\] for \\(y&gt;0\\), zero otherwise with the property \\(E[Y]=var(Y)=\\theta\\).\nCount variables often modelled as a random Poisson process: numbers of road traffic accidents, sales, telephone calls, electron emissions. Greene’s example is to find the most likely value of \\(\\theta\\) given observations. \\[\\begin{equation}\n5,\\ 0,\\ 1,\\ 1,\\ 0,\\ 3,\\ 2,\\ 3,\\ 4,\\ 1\n\\end{equation}\\] For independent observations the joint density is \\[\\begin{equation}\np(y,\\ \\theta) =\\prod_{i=1}^{10}p(y_i,\\ \\theta )\n  = \\frac{e^{-10\\theta}\\theta^{\\sum_i y_i}}{\\prod_i (y_i!)}\n  = \\frac{e^{-10\\theta}\\theta^{20}}{207,360}\n\\end{equation}\\] We can plot this function to see if it has a maximum\n\n\n\n\n\n\n\n\n\nWe can also find this by calculus. As ever, because the log function is monotonic it is convenient to take logs \\[\\begin{equation}\n   \\ln L(\\theta) = -10\\theta + 20\\ln\\theta -\\ln(207,360)\n\\end{equation}\\] First order conditions are \\[\\begin{equation}\n  \\frac{\\partial \\ln L(\\theta )}{\\partial\\theta} = -10+\\frac{20}{\\theta}\n\\Rightarrow \\theta =\\frac{20}{10}=2\n\\end{equation}\\] Check for maximum \\[\\begin{equation}\n   \\frac{\\partial^2\\ln L(\\theta )}{\\partial\\theta^2} = -\\frac{20}{\\theta^2} &lt; 0\n\\end{equation}\\]"
  },
  {
    "objectID": "CK.html#poisson-example-1",
    "href": "CK.html#poisson-example-1",
    "title": "10  Carter-Kohn",
    "section": "10.3 Poisson example",
    "text": "10.3 Poisson example\nThe Poisson density for each observation is \\[\\begin{equation}\n   p(y_i, \\theta) = \\frac{e^{-\\theta} \\theta^{y_i}}{y_i!}\n\\end{equation}\\] for \\(y&gt;0\\), zero otherwise, with \\(E[Y]=var(Y)=\\theta\\). For \\(n\\) independent observations, joint density is \\[\\begin{equation}\nP(y, \\theta) = \\prod_{i=1}^n p(y_i, \\theta)\n  = \\frac{e^{-n\\theta}\\theta^{\\sum_i y_i}}{\\prod_i (y_i!)}\n\\end{equation}\\] Interpret this as a likelihood function, i.e. a probability measure for \\(\\theta\\) given some observed \\(y\\) \\[\\begin{equation}\nL(\\theta | y) = P(y,\\theta)\n\\end{equation}\\] As log function is monotonic \\[\\begin{equation}\n\\ln L(\\theta | y) = -n\\theta + {\\textstyle{\\sum_i} y_i} \\ln\\theta  - \\ln\\left(\\textstyle{\\prod_i} y_i! \\right)\n\\end{equation}\\] First order conditions are \\[\\begin{equation}\n  \\frac{\\partial \\ln L(\\theta|y)}{\\partial\\theta}\n     = -n + \\frac{\\sum_i y_i}{\\theta}\n     \\Rightarrow\n        \\theta =\\frac{\\sum_i y_i}{n}\n\\end{equation}\\] Finally, check for maximum \\[\\begin{equation}\n\\frac{\\partial^2\\ln L(\\theta)}{(\\partial\\theta)^2} = -\\frac{\\sum_i y_i}{\\theta^2} &lt; 0\n\\end{equation}\\]\n\n\nCode\nlibrary(tidyverse)\nlibrary(scales)\n\nreps   &lt;- 20\nlambda &lt;- 2\nn      &lt;- 10\n\na &lt;- tibble(x=seq(0, 3*lambda),\n            p=(exp(-lambda*x)*(lambda^x))/prod(factorial(lambda)))\n\nd &lt;- matrix(rpois(n*reps,lambda), n, reps)\n\nt &lt;- as_tibble(factorial(d), .name_repair = ~paste0(\"Rep\",1:reps)) %&gt;%\n  pivot_longer(cols = everything()) %&gt;%\n  group_by(name) %&gt;%\n  summarise(dd = prod(value)) %&gt;%\n  mutate(sum  = colSums(d),\n         ests = sum/n)\n\n\n\n\nCode\nggplot(t) + \n  geom_histogram(aes(x=ests), fill=\"blue\", alpha=.33, color=NA, bins=20) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\ntheta &lt;- seq(0, 8, 1)\ny     &lt;- (exp(-lambda)*lambda^(theta))/(factorial(theta))\ndf    &lt;- tibble(theta = theta) %&gt;%\n  mutate(`lambda == 1` = (exp(-1)*1^(theta))/(factorial(theta)),\n         `lambda == 2` = (exp(-2)*2^(theta))/(factorial(theta)),\n         `lambda == 3` = (exp(-3)*3^(theta))/(factorial(theta)))\n\ndf %&gt;%\n  pivot_longer(cols = -theta, names_to = \"lambda\") %&gt;%\n  ggplot() +\n  geom_line(aes(x=theta, y=value, color=lambda), size=1) +\n  geom_point(aes(x=theta, y=value, color=lambda)) +\n  theme_minimal() +\n  labs(title=\"Poisson density example\", x=\"\", y=\"\",\n       color=expression(lambda)) +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5),\n        legend.position = c(.8,.8)) +\n  scale_colour_discrete(labels = parse_format())\n\n\n\n\n\n\n\n\n\n\n\nCode\ne &lt;- c(5,0,1,1,0,3,2,3,4,1)\nl &lt;- seq(0,5,0.05)\ny &lt;- (exp(-n*l)*l^(sum(e)))/(prod(factorial(e)))\n\ntibble(l=l, y=y) %&gt;%\n  ggplot() +\n  geom_line(aes(x=l, y=y), color=\"red\", linewidth=1) +\n  theme_minimal() +\n  labs(title=\"Poisson density example\",\n       x=expression(paste(\"Value of \", lambda)),\n       y=\"Joint density\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\nMaintain the value of \\(\\lambda\\) of 2. Now generate \\(reps=20\\) replications of \\(n=10\\) observations and plot the empirical density of each and the maximum likelihood estimate for each replication.\n\n\nCode\nt2 &lt;- t %&gt;%\n  group_by(name) %&gt;%\n  mutate(r = list((exp(-n*l)*l^sum)/dd))\n\nnms &lt;- paste0(\"Rep_\", str_pad(1:reps, 2, pad=\"0\"))\n\nt2$r %&gt;% \n  bind_cols(.name_repair = ~ nms) %&gt;%\n  mutate(l = l) %&gt;%\n  pivot_longer(cols = -l) %&gt;%\n  ggplot() +\n  geom_line(aes(x=l, y=value, color=name), show.legend = FALSE) +\n  geom_vline(aes(xintercept=lambda), color=\"red\", linetype=3, linewidth=0.75) +\n  facet_wrap(~ name, scales = \"free_y\") +\n  theme_minimal() +\n  labs(title    = paste(\"Poisson density, sample size\", n),\n       subtitle = expression(paste(\"Dotted line: true value of \", lambda)),\n       x        = expression(paste(\"Estimated value of \", lambda)),\n       y        = \"Joint density\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\n\n10.3.1 ML and regression\nLinear regression problem is \\[\\begin{equation}\nL(\\beta|y,X) = \\frac{1}{(2\\pi \\sigma^2)^{n/2}}\\exp \\left[ - \\frac{1}{2\\sigma^2} (y-X\\beta)'(y-X\\beta) \\right]\n\\end{equation}\\] The log-likelihood is \\[\\begin{equation}\n   \\ln L = -\\frac{n}{2}\\ln (2\\pi)-\\frac{n}{2}\\ln (\\sigma^2) - \\frac{1}{2\\sigma^2}(y-X\\beta)'(y-X\\beta)\n\\end{equation}\\] As before, find the extremum by calculus; yields likelihood equations \\[\\begin{align}\n\\frac{\\partial \\ln L}{\\partial \\beta} & = - \\frac{2}{2\\sigma^2} (X'y-X'X\\beta) = 0 \\\\\n& \\Rightarrow \\hat{\\beta}_{ml} = (X'X)^{-1}X'y\n\\end{align}\\] and \\[\\begin{align}\n\\frac{\\partial \\ln L}{\\partial \\sigma^2} &=\n   - \\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4}   \n     (y - X\\beta)'(y - X\\beta) = 0 \\\\\n& \\Rightarrow -n + \\sigma^{-2} (\\epsilon'\\epsilon) = 0 \\\\\n& \\Rightarrow \\hat{\\sigma}_{ml}^2 = \\frac{\\hat{\\epsilon}'\\hat{\\epsilon}}{n}\n\\end{align}\\] ML estimate of \\(\\sigma^2\\) divided by \\(n\\) (not \\(n-k\\)) so biased in small samples but not asymptotically"
  },
  {
    "objectID": "CK.html#kalman-filter-tricks",
    "href": "CK.html#kalman-filter-tricks",
    "title": "10  Carter-Kohn",
    "section": "10.4 Kalman filter tricks",
    "text": "10.4 Kalman filter tricks\nFor some initial condition – say \\(\\beta_0 \\sim N(\\mu_0,P_0))\\) – the conditional log-likelihood for sample \\(1\\) to \\(T\\) \\[\\begin{align}\n\\log L(\\psi_t|\\theta) &= \\sum\\nolimits_{t=1}^T\\log p(y_t|\\psi_{t-1},\\theta) \\\\\n&\\propto -\\sum\\nolimits_{t=1}^T\\left( \\log \\left\\vert f_{t|t-1}\\right\\vert +\\eta_{t|t-1}' f_{t|t-1}^{-1}\\eta_{t|t-1} |\\ \\theta \\right)\n\\end{align}\\] Note we could obtain \\(\\eta_{t|t-1}\\) and \\(f_{t|t-1}\\) from the Kalman filter, i.e.  \\[\\begin{equation}\n  f_{t|t-1} = (H_tP_{t|t-1}H_t' + Q) = \\Sigma_{\\eta\\eta}\n\\end{equation}\\] This is the prediction error decomposition of the log-likelihood. For a classical approach we estimate \\(\\theta\\) by numerically maximizing \\(\\log L(\\psi_T|\\theta)\\).This gives a point estimate for the value of \\(\\theta\\) and we typically apply classical inference using the estimated standard errors. Note to do this we need to evaluate the best estimate of the state as well as maximize the likelihood: the Kalman Filter is a key ingredient in both.\n\n\n\n\n\n\nMaximisation\n\n\n\nA suitable numerical maximization routine will (in principle) maximize the likelihood straightforwardly. Often use Chris Sims’ csminwel in Matlab or R as well-suited to this type of problem.\n\n\nCan show (via Cramer-Rao) that \\[\\begin{equation}\n\\widehat{\\theta}\\sim N\\left(\\theta, -\\frac{\\partial^2 \\log L(\\psi_T|\\theta)} {\\partial\\theta\\partial\\theta'} \\right)\n\\end{equation}\\]"
  },
  {
    "objectID": "CK.html#full-sample-estimates-of-beta_t",
    "href": "CK.html#full-sample-estimates-of-beta_t",
    "title": "10  Carter-Kohn",
    "section": "10.5 Full sample estimates of \\(\\beta_t\\)",
    "text": "10.5 Full sample estimates of \\(\\beta_t\\)\n\n10.5.1 The regression lemma again\nYou may recall that for any \\[\\begin{equation}\n\\begin{bmatrix} z \\\\ y \\\\ \\varepsilon \\end{bmatrix}\n\\sim N\\left(\\begin{bmatrix} \\mu_z \\\\ \\mu_y \\\\ 0 \\end{bmatrix},\n\\begin{bmatrix}\n\\Sigma_{zz} & \\Sigma_{zy} & \\Sigma_{z\\varepsilon } \\\\\n\\Sigma_{yz} & \\Sigma_{yy} & 0 \\\\\n\\Sigma_{\\varepsilon z} & 0 & \\Sigma_{\\varepsilon\\varepsilon}\n\\end{bmatrix}\\right)\n\\end{equation}\\] then it must be that \\[\\begin{align}\nE[z|y,\\varepsilon] &= \\mu_z + \\Sigma_{zy}\\Sigma_{yy}^{-1}(y-\\mu_y) + \\Sigma_{z\\varepsilon}\\Sigma_{\\varepsilon\\varepsilon}^{-1}\\varepsilon \\\\\n&= E[z|y] + \\Sigma_{z\\varepsilon}\\Sigma_{\\varepsilon\\varepsilon }^{-1}\\varepsilon\n\\end{align}\\] We use this to derive a recursive update to smooth our estimates. We will also derive an appropriate conditional expectation which we can use in Gibbs sampling."
  },
  {
    "objectID": "CK.html#smoothing",
    "href": "CK.html#smoothing",
    "title": "10  Carter-Kohn",
    "section": "10.6 Smoothing",
    "text": "10.6 Smoothing\nThe Kalman filter estimates \\(\\beta_t\\) recursively: it only uses information available up until time \\(t\\). This means that the estimate of \\(\\beta_{T|T}\\) uses all available information, but any previous estimate doesn’t. Indeed there must be some values of \\(\\eta_{i|t-1}\\) \\[\\begin{equation}\n   \\beta_{t|T} = E(\\beta_t | \\psi_{t-1}, \\eta_{t|t-1}, \\eta_{t+1|t-1},...,\\eta_{T|t-1})\n\\end{equation}\\] where the ‘news’ is relative to period \\(t\\).\nWe could update \\(\\beta_{t|t-1}\\) using the (uncorrelated) future innovations \\[\\begin{equation}\n  \\beta_{t|T}=\\beta_{t|t}+\\sum_{j=t}^T\\Sigma_{\\beta_t\\eta_j}\\Sigma_{\\eta_j\\eta_j}^{-1}\\eta_{j|t-1}\n\\end{equation}\\] and recalling \\(\\beta_{t|t}=E(\\beta_t|\\psi_{t-1},\\eta_{t|t-1})\\).\nThis is a fixed interval smoother; often used for full sample estimates of \\(\\beta_t\\). Remember we already have an estimate of \\(\\beta_{T|T}\\) from the Kalman filter so smoothers work backwards; we sketch a derivation here.\nIn the last but one period we have a different prediction error \\[\\begin{equation}\n   \\varsigma_{T|T-1} = \\beta_{T|T} - F\\beta_{T-1|T-1} - \\mu\n\\end{equation}\\] which is the error in predicting \\(\\beta_T\\) using \\(\\psi_{T-1}\\).\nAn ‘update’ has to be of the form \\[\\begin{equation}\n\\beta_{T-1|T}=\\beta_{T-1|T-1} + \\Sigma_{\\beta\\varsigma}\\Sigma_{\\varsigma\\varsigma}^{-1}\\varsigma_{T|T-1}\n\\end{equation}\\] where \\(\\Sigma_{\\varsigma\\varsigma} = var\\left[ \\varsigma_T|\\psi_{T-1}\\right]\\) and \\(\\Sigma_{\\beta \\varsigma}=cov\\left[\\beta_{T-1},\\varsigma_T|\\psi_{T-1}\\right]\\).\nThese are \\[\\begin{align}\n\\Sigma_{\\varsigma\\varsigma} &= var(\\beta_T - F\\beta_{T-1|T-1}-\\mu) \\\\\n  &= var\\left(F(\\beta_{T-1}-\\beta_{T-1|T-1}) + e_t \\right) \\\\\n  &= F P_{T-1|T-1}F' + Q\n\\end{align}\\] and \\[\\begin{align}\n\\Sigma_{\\beta\\varsigma} &= E\\left[ (\\beta_{T-1}-\\beta_{T-1|T-1}) \\left( \\beta_T - F\\beta_{T-1|T-1}-\\mu \\right)'\n\\right] \\\\\n&= E\\left[ (\\beta_{T-1}-\\beta_{T-1|T-1}) (\\beta_T-\\beta_{T-1|T-1})'\\right] F' \\\\\n&= P_{T-1|T-1}F'\n\\end{align}\\] Plugging these definitions in gives us \\[\\begin{equation}\n\\beta_{T-1|T} = \\beta_{T-1|T-1} + P_{T-1|T-1}F' P_{T|T-1}^{-1} (\\beta_{T|T}-F\\beta_{T-1|T-1}-\\mu)\n\\end{equation}\\] Applying the argument backward in time gives the recursion \\[\\begin{align}\n\\beta_{t|T} &= \\beta_{t|t}+P_{t|t} F'P_{t+1|t}^{-1} (\\beta_{t+1|T}-F\\beta_{t|t}-\\mu) \\\\\n&= \\beta_{t|t} - K_{t|T} (\\beta_{t+1|T}-F\\beta_{t|t}-\\mu) \\tag{smooth}\n\\end{align}\\] All these quantities are outputs of the Kalman filter so smoothing is easy to implement.\nThe smoothed variance of \\(\\beta_{t|T}\\) found by multiplying out (smooth). To do this use \\(\\beta_{t+1|t} = \\mu + F\\beta_{t|t}\\) so rearranging gives \\[\\begin{equation}\n  \\widetilde{\\beta}_{t|T} + K_{t|T}\\beta_{t+1|T} = \\widetilde{\\beta}_{t|t}+K_{t|T}\\beta_{t+1|t}\n\\end{equation}\\] where \\(\\widetilde{\\beta}_{t|t}=\\beta_t-\\beta_{t|t}\\). Now square both sides and take expectations \\[\\begin{equation}\n  P_{t|T} + K_{t|T} E\\left[\\beta_{t+1|T}\\beta_{t+1|T}' \\right] K_{t|T}' = P_{t|t} + K_{t|T} E\\left[ \\beta_{t+1|t} \\beta_{t+1|t}' \\right] K_{t|T}'\n\\end{equation}\\] Adding and subtracting \\(E[\\beta_{t+1}\\beta_{t+1}']\\) we can show that \\[\\begin{equation}\n-E\\left[\\beta_{t+1|T}\\beta_{t+1|T}'\\right] + E\\left[\\beta_{t+1|t}\\beta_{t+1|t}'\\right] = P_{t+1|T}-P_{t+1|t}\n\\end{equation}\\] to obtain \\[\\begin{equation}\n  P_{t|T} = P_{t|t} + K_{t|T} (P_{t+1|T}-P_{t+1|t}) K_{t|T}'.\n\\end{equation}\\]"
  },
  {
    "objectID": "CK.html#kalman-filter-in-econometrics",
    "href": "CK.html#kalman-filter-in-econometrics",
    "title": "10  Carter-Kohn",
    "section": "10.7 Kalman filter in econometrics",
    "text": "10.7 Kalman filter in econometrics\n\n10.7.1 Classical approach\nThe typical procedure is some variation on:\n\nFormulate state-space model\nEstimate the model by maximum likelihood\nCondition on the parameters to retrieve the (usually smoothed) state estimates and standard errors\nUse Cramer-Rao to calculate the standard errors of any other parameter estimates\n\nFor this the Kalman filter is a useful tool, as it allows a great deal of flexibility in the estimation of a variety of models, as is is an appropriate tool for models with unobserved components. However, it must be used with care: it is easy to try to estimate models that are essentially unidentified.\nFurther useful tools\n\nThe Extended Kalman filter linearises the filter at every step and can be used for nonlinear models (such as ones where you need to estimate \\(B_T\\) and \\(\\theta\\) simultaneously)\nIncreasingly non-Gaussian non-linear models are estimated using the particle filter\n\n\n\n10.7.2 Bayesian approach\nBayesian approach is to generate the entire distribution of the model parameters.\n\nNow no longer just look for the point estimate obtained by maximum likelihood\nUse Gibbs sampling or some other appropriate method applied to the state space model\nIn particular we treat the states and the parameters as jointly determined by the data\nAs the state is estimated we need a way to draw the states conditional on the other estimates to do Gibbs sampling\nSeek a conditional updating algorithm that replicates the Gibbs sampling approach we have used before\n\nWe require a procedure such that:\n\nStep 1 Conditional on \\(\\theta\\) and the data, generate the sequence \\(B_T = (\\beta_1, \\beta_2, \\ldots, \\beta_T)\\)\nStep 2 Conditional on \\(B_T\\) and the data, generate values of \\(\\theta\\)\nStep 3 Iterate previous two steps until convergence\n\nIn this way the joint distribution of the two can be obtained from the resulting simulation."
  },
  {
    "objectID": "CK.html#carter-kohn-algorithm",
    "href": "CK.html#carter-kohn-algorithm",
    "title": "10  Carter-Kohn",
    "section": "10.8 Carter-Kohn algorithm",
    "text": "10.8 Carter-Kohn algorithm\n\nStep 2 above relatively easy but how do we generate a sequence of states?\n\nThe state estimates depend on the parameter value through the Kalman filter\n\nAppropriate algorithm designed by Carter and Kohn (1994)\nTakes the form of a modified Kalman smoother\nKnown as multimove Gibbs sampling\nSimilar to above, define\n\n\\[\\begin{equation}\n  B_t = \\begin{bmatrix} \\beta_1 & \\beta_2 & \\ldots & \\beta_t \\end{bmatrix}\n\\end{equation}\\] so in particular \\[\\begin{equation}\nB_{T-1} = \\begin{bmatrix} \\beta_1 & \\beta_2 & \\ldots & \\beta_{T-1} \\end{bmatrix}\n\\end{equation}\\] consistent with out earlier definition of \\(\\psi_t\\).\n\nMultimove Gibbs sampling generates the whole vector of states (\\(B_T\\)) at once\nWe therefore need to generate a realization of \\(B_T\\) given the probability distribution \\(p( B_T|\\psi_T)\\)\nWe want to generate an appropriate conditional probability distribution \\(p(\\beta_t|B_{j\\neq t}, \\psi_T)\\) to sample from for our Gibbs sampler\nJust as for the Kalman smoother we use the outputs of the Kalman filter and a separate backward recursion to obtain the conditional distribution"
  },
  {
    "objectID": "CK.html#joint-distribution",
    "href": "CK.html#joint-distribution",
    "title": "10  Carter-Kohn",
    "section": "10.9 Joint distribution",
    "text": "10.9 Joint distribution\nDeriving the appropriate distributions is easy if we know what to condition on. The joint probability density function can be split into a sequence of conditional distributions: \\(p(B_T|\\psi_T)\\) can be written recursively \\[\\begin{align}\np(B_T|\\psi_T) &= p(\\beta_T|\\psi_T) \\times p(B_{T-1}|\\beta_T,\\psi_T) \\\\\n&= p(\\beta_T|\\psi_T) \\times p(\\beta_{T-1}|\\beta_T,\\psi_T) \\times p(B_{T-2}|\\beta_{T-1},\\beta_T,\\psi_T) \\\\\n&= p(\\beta_T|\\psi_T) \\times p(\\beta_{T-1}|\\beta_T,\\psi_T) \\times p(B_{T-2}|\\beta_{T-1},\\psi_T)\n\\end{align}\\] Final simplification follows as the state vector is a Markov chain so there is no information in \\(\\beta_T\\) not contained in \\(\\beta_{T-1}\\) and \\(\\psi_T\\). Further as soon as we know \\(\\beta_{T-1}\\) there is no information contained in \\(\\psi_T\\) so we can drop that, so \\[\\begin{align}\np(B_T|\\psi_T) &= p(\\beta_T|\\psi_T)\\times p(\\beta_{T-1}|\\beta_T, \\psi_T) \\times  p(B_{T-2}|\\beta_{T-1},\\psi_T) \\\\\n&= p(\\beta_T|\\psi_T)\\times p(\\beta_{T-1}|\\beta_T, \\psi_{T-1}) \\times p(B_{T-2}|\\beta_{T-1},\\psi_{T-2}) \\\\\n&= p(\\beta_T|\\psi_T) \\times \\prod_{t=1}^{T-1} p(\\beta_t|\\beta_{t+1}, \\psi_t)\n\\end{align}\\]"
  },
  {
    "objectID": "CK.html#the-carter-kohn-equations",
    "href": "CK.html#the-carter-kohn-equations",
    "title": "10  Carter-Kohn",
    "section": "10.10 The Carter-Kohn equations",
    "text": "10.10 The Carter-Kohn equations\nThe estimated \\(\\beta\\) variables are distributed \\[\\begin{align}\n\\beta_{T|\\psi_T} &\\sim N(\\beta_{T|T}, P_{T|T}) \\\\\n\\beta_{t|\\psi_t, \\beta_{t+1}} &\\sim N(\\beta_{t|t,\\beta_{t+1}},P_{t|t,\\beta_{t+1}})\n\\end{align}\\] where \\[\\begin{align}\n\\beta_{t|t,\\beta_{t+1}} &= E\\left[\\beta_t|\\psi_t,\\beta_{t+1}\\right]\n= E\\left[\\beta_t|\\beta_{t|t},\\beta_{t+1}\\right] \\\\\nP_{t|t,\\beta_{t+1}} &= cov\\left[\\beta_t|\\psi_t,\\beta_{t+1}\\right] = cov\\left[\\beta_t |{\\beta_{t|t},\\beta_{t+1}}\\right]\n\\end{align}\\] Carter-Kohn derive appropriate recursions so that, for example, we update the state estimate conditioning on some known value of \\(\\beta_{t+1}\\) \\[\\begin{equation}\n  \\beta_{t|t,\\beta_{t+1}} = \\beta_{t|t}-K_{t|t+1} (\\beta_{t+1}-F\\beta_{t|t}-\\mu)\n\\end{equation}\\] Define \\[\\begin{equation}\n   \\varsigma_{t+1|t} = \\beta_{t+1}-F\\beta_{t|t}-\\mu\n\\end{equation}\\] as the ‘innovation’ in predicted \\(\\beta_{t+1|t}\\) where we have some realized \\(\\beta_{t+1}\\) drawn from its probability distribution. The Carter-Kohn smoother comprises updates to the conditional expectations that use this news. \\[\\begin{align}\nE[\\beta_t|\\psi_t,\\beta_{t+1}] &= E[\\beta_t|\\psi_t] + \\Sigma_{\\beta\\varsigma} \\Sigma_{\\varsigma\\varsigma}^{-1}\\varsigma_{t+1|t} \\\\\n&= \\beta_{t|t} + \\Sigma_{\\beta\\varsigma}\\Sigma_{\\varsigma\\varsigma}^{-1}\\varsigma_{t+1|t} \\\\\nvar[\\beta_t|\\psi_t,\\beta_{t+1}] &= var[\\beta_t|\\psi_t] -\\Sigma_{\\beta\\varsigma} \\Sigma_{\\varsigma\\varsigma}^{-1} \\Sigma_{\\varsigma\\beta} \\\\\n&= P_{t|t} - \\Sigma_{\\beta\\varsigma} \\Sigma_{\\varsigma\\varsigma}^{-1} \\Sigma_{\\varsigma\\beta} \\\\\n&= P_{t|t,\\beta_{t+1}}\n\\end{align}\\] Both \\(\\beta_{t|t}\\) and \\(P_{t|t}\\) are outputs of the Kalman filter.\n\n10.10.1 Deriving \\(\\Sigma_{\\beta\\varsigma}\\) and \\(\\Sigma_{\\varsigma\\varsigma}\\)\nAs before we just plug in the definitions so \\[\\begin{align}\n\\Sigma_{\\varsigma\\varsigma} &= var[\\beta_{t+1}-F\\beta_{t|t}-\\mu] \\\\\n&= var\\left[ F\\beta_t+\\mu +v_{t+1}-F\\beta_{t|t}-\\mu \\right] \\\\\n&= var\\left[ F(\\beta_t-\\beta_{t|t})+v_{t+1}\\right] \\\\\n&= F P_{t|t}F' + Q\n\\end{align}\\] and \\[\\begin{align}\n\\Sigma_{\\beta\\varsigma} &= E\\left[ (\\beta_t - \\beta_{t|t}) (\\beta_{t+1}-F\\beta_{t|t}-\\mu)'\\right] \\\\\n&= E\\left[ (\\beta_t-\\beta_{t|t})\\left( F(\\beta_t-\\beta_{t|t})+v_{t+1}\\right)'\\right] \\\\\n&= P_{t|t}F'\n\\end{align}\\]"
  },
  {
    "objectID": "CK.html#kalman-gain-again",
    "href": "CK.html#kalman-gain-again",
    "title": "10  Carter-Kohn",
    "section": "10.11 ‘Kalman gain’ again",
    "text": "10.11 ‘Kalman gain’ again\nSo using the definitions of the covariances and the regression lemma we get \\[\\begin{align}\n\\beta_{t|t,\\beta_{t+1}} &= \\beta_{t|t} + \\Sigma_{s\\eta} \\Sigma_{\\eta\\eta}^{-1}\\varsigma_t \\\\\n&= \\beta_{t|t} + P_{t|t}F' \\left( FP_{t|t}F' + Q\\right)^{-1} (\\beta_{t+1}-F\\beta_{t|t}-\\mu) \\\\\n&= \\beta_{t|t} - K_{t|t}(\\beta_{t+1}-F\\beta_{t|t}-\\mu)\n\\end{align}\\] where \\[\\begin{equation}\n   K_{t|t+1} = - P_{t|t}F' (FP_{t|t}F' + Q)^{-1}\n\\end{equation}\\] Like the Kalman smoother, this uses the filter’s estimate of \\(P_{t|t}\\) and updates \\(\\beta_t\\) using the error in predicting \\(\\beta_{t+1}\\) not \\(y_t\\).\n\n10.11.1 Conditional mean and variance of the state\nUpdating equations for the state and variance obtained directly from the regression lemma \\[\\begin{align}\n\\beta_{t|t,\\beta_{t+1}} &= \\beta_{t|t} - K_{t|t+1}\\varsigma_{t+1|t} \\\\\n    P_{t|t,\\beta_{t+1}} &= P_{t|t}-P_{t|t}F'(FP_{t|t}F' + Q)^{-1}F P_{t|t}\n\\end{align}\\] CK equations recursively evaluate these quantities backwards beginning from \\(s_T\\), \\(P_T\\) obtained from the Kalman filter.\nGenerate appropriate conditional samples using \\[\\begin{equation}\n  \\beta_{t|t,\\beta_{t+1}} \\sim N\\left(\\beta_{t|t,\\beta_{t+1}}, P_{t|t,\\beta_{t+1}} \\right)\n\\end{equation}\\] to give \\(B_T|\\psi_T\\). This is a conditional sample that depends on a given parameter vector to use in a Gibbs sampling scheme that draws those parameters in turn from distributions conditioned on the states."
  },
  {
    "objectID": "CK.html#state-space-gibbs-sampling-in-practice",
    "href": "CK.html#state-space-gibbs-sampling-in-practice",
    "title": "10  Carter-Kohn",
    "section": "10.12 State-space Gibbs sampling in practice",
    "text": "10.12 State-space Gibbs sampling in practice\nAbove approach cannot be used explicitly if \\(\\Sigma_{\\varsigma\\varsigma}\\) is singular, for example if we have more states than shocks (which is not uncommon). A simple modification given in KN can deal with this; we treat only those states that are shocked as observed.\nIn general we need conditional distributions for all the other parameters to be estimated. Need to store the complete sequence of states and covariances to implement the Gibbs sampler. We will investigate the exact implementation of Gibbs sampling for state-space models in the exercises."
  },
  {
    "objectID": "CK.html#comparing-the-filters-and-smoothers",
    "href": "CK.html#comparing-the-filters-and-smoothers",
    "title": "10  Carter-Kohn",
    "section": "10.13 Comparing the filters and smoothers",
    "text": "10.13 Comparing the filters and smoothers\n\\[\\begin{alignat*}{3}\n&\\text{Filter} & &\\text{Innovation} & & \\text{Gain and state covariance}  \\\\\n& && &&\\\\\n&KF &\\qquad\\ &\\eta_t = y_t-H_t\\beta_{t|t-1} && K_{t|t} =-P_{t|t-1}H_t' (H_t P_{t|t-1} H_t' + R)^{-1} \\\\\n& &&  && P_{t|t} = P_{t|t-1}-P_{t|t-1} H_t'(H_t P_{t|t-1} H_t' +R)^{-1} H_t P_{t|t-1} \\\\\n&KS && \\varsigma_t = \\beta_{t+1|T}-F\\beta_{t|t}-\\mu & \\qquad\\ & K_{t|T}=-P_{t|t}F' P_{t+1|t}^{-1} \\\\\n&   &&      && P_{t|T} = P_{t|t}+K_{t|T}(P_{t+1|T}-P_{t+1|t}) K_{t|T}' \\\\\n&CK && \\varsigma_t = \\beta_{t+1}-F\\beta_{t|t}-\\mu && K_{t|t,\\beta_{t+1}} = -P_{t|t}F' P_{t+1|t}^{-1} \\\\\n&   &&   && P_{t|t,\\beta_{t+1}} = P_{t|t}-P_{t|t}F'(FP_{t|t}F' + Q)^{-1}FP_{t|t}\n\\end{alignat*}\\]\n\n\n\n\nCarter, C. K., and R. Kohn. 1994. “On Gibbs sampling for state space models.” Biometrika 81 (3): 541–53. https://doi.org/10.1093/biomet/81.3.541.\n\n\nDurbin, J., and S. J. Koopman. 2001. Time Series Analysis by State Space Methods. Oxford: Oxford University Press.\n\n\nGreene, William H. 1997. Econometric Analysis. Third. McGraw Hill.\n\n\nHamilton, J. D. 1994. Time Series Analysis. Princeton, NJ: Princeton University Press.\n\n\nHarvey, Andrew C. 1989. Forecasting, Structural Time Series Models and the Kalman Filter. Cambridge: Cambridge University Press.\n\n\nKim, Chang-Jin, and Charles R. Nelson. 1999. State-Space Models with Regime Switching: Classical and Gibbs-Sampling Approaches with Applications. MIT Press."
  },
  {
    "objectID": "CK.html#footnotes",
    "href": "CK.html#footnotes",
    "title": "10  Carter-Kohn",
    "section": "",
    "text": "For example Greene (1997) constructs a Poisson distribution example where the chosen observations yield an exact estimate of the underlying parameter of the distribution. The example is to find the most likely value of \\(\\theta\\) given observations \\[\\begin{equation}\n5,\\ 0,\\ 1,\\ 1,\\ 0,\\ 3,\\ 2,\\ 3,\\ 4,\\ 1\n\\end{equation}\\] ten observations which sum to 20.↩︎"
  },
  {
    "objectID": "SMH.html#complicated-densities",
    "href": "SMH.html#complicated-densities",
    "title": "11  Metropolis-Hastings",
    "section": "11.1 Complicated densities",
    "text": "11.1 Complicated densities\nWhat if we have a posterior density that is too complicated to factor into a full set of conditional densities? Recall that Gibbs Sampling is a procedure that generates marginal densities from conditional densities. What if we don’t have conditional densities?\n\n\n\n\n\n\nWhy can’t we use Gibbs for DSGEs?\n\n\n\nSay a model only has two unknown parameters, \\(\\alpha\\) and \\(\\gamma\\). Any predictions of that model conditional on them is: \\[\\begin{equation}\n  y = f(X,\\ \\alpha,\\ \\gamma)\n\\end{equation}\\] The likelihood of that model \\[\\begin{equation}\n  \\mathcal{L}(\\alpha,\\ \\gamma\\ | \\ y,\\ X)\n\\end{equation}\\] is readily available but the conditional density of say, \\(\\alpha\\) \\[\\begin{equation}\n  \\mathcal{P}(\\alpha\\ | \\ y,\\ X,\\ \\gamma)\n\\end{equation}\\] typically isn’t. This is because the predictive model \\(f(\\cdot)\\) depends on all the parameters through the reduced form solution used in DSGEs even if the underlying model is linear, i.e. the solution in (BK1980?). As \\(f(X,\\ \\alpha,\\ \\gamma)\\) always depends on both \\(\\alpha\\) and \\(\\gamma\\) it is difficult to find an appropriate conditional likelihood which isolates one of them.\n\n\nCan we derive a technique that generates marginal densities from a joint density? Turns out we (perhaps surprisingly) can, using the Metropolis-Hastings algorithm.\n\n11.1.1 Target density\nWe’ll derive the simplest posterior density that we can use in an exercise, but we begin with a more general case. Consider a posterior likelihood that is the product of a likelihood and \\(k\\) prior densities, say \\[\\begin{equation}\n\\mathcal{H}(\\theta\\ | \\ y) = \\mathcal{L}(\\theta\\ | \\ y)\\times \\mathcal{P}_1(\\theta_1)\\times \\mathcal{P}_2(\\theta_2)\\times \\mathcal{P}_3(\\theta_3)\\times ...\\times \\mathcal{P}_k(\\theta_k)\n\\end{equation}\\] This is the target density. How can we estimate the densities of the underlying \\(\\theta_i\\) from this posterior alone? This (somewhat amazingly) turns out to be rather simple.\nThe trick is to simulate draws for all the elements of \\(\\theta\\) from the target density – despite not having a generating function to draw from the density – and then estimating the marginal densities from the resulting series."
  },
  {
    "objectID": "SMH.html#simplified-problem-estimating-known-prior",
    "href": "SMH.html#simplified-problem-estimating-known-prior",
    "title": "11  Metropolis-Hastings",
    "section": "11.2 Simplified problem: estimating (known) prior",
    "text": "11.2 Simplified problem: estimating (known) prior\nAs the aim is to introduce MH in as simple a context as possible, we will sample from a posterior distribution for which we don’t have an appropriate random number generator but for marginal distributions that we do. This means we can compare analytical and estimated results.\nWe will try an estimate the marginal distributions for the priors alone; essentially \\(\\mathcal{H}(\\theta\\ | \\ y)\\) where \\(\\mathcal{L}(y\\ |\\ \\theta)\\) is flat for all values of the prior so \\[\\begin{equation}\n\\mathcal{H}(\\theta\\ | \\ y) = \\mathcal{P}_1(\\theta_1)\\times \\mathcal{P}_2(\\theta_2)\\times \\mathcal{P}_3(\\theta_3)\\times ...\\times \\mathcal{P}_k(\\theta_k)\n\\end{equation}\\] This means we know what the marginals should look like – they are just the priors!\nAssume there are \\(k\\) unknown parameters with a prior density set by the investigator, such as \\[\\begin{equation}\n  \\mathcal{P}(\\rho_1) \\sim \\text{Beta}(1.2,1.8)\n\\end{equation}\\] subject to the arbitrary bounds that \\(0.001 &lt; \\rho_1 &lt;0.999\\).\nWe have \\(k\\) of these, so in any code we could specify this in a matrix where each prior is specified in a row containing a name, a PDF type, the parameters of the PDF, as well as a lower and upper bound."
  },
  {
    "objectID": "SMH.html#example-1",
    "href": "SMH.html#example-1",
    "title": "11  Metropolis-Hastings",
    "section": "11.3 Example 1",
    "text": "11.3 Example 1\nFirst example: target density is the product of six parameters densities of four types: Normal, Gamma, Inverse Gamma and Beta. Any suitable density could be used as a prior, so the Uniform, say, or the Inverse Weibull or Log Gamma could be slotted in – and we will later on. All that is required is that the some function exists to evaluate the density. Obviously we could generalize this to one or three or more parameter distributions with appropriate code.\nFor example we could fit a Skew-\\(t\\) say, as long as we can evaluate the (log) density for this 4 parameter distribution. See Klugman, Panjer, and Willmot (2008) for a very comprehensive list of densities we could use – I cannot recommend this highly enough.\n\n\n\n\n\n\n\nname\n\n\nlb\n\n\nub\n\n\nPDF\n\n\np1\n\n\np2\n\n\n\n\n\n\nbeta\n\n\n0.001\n\n\n0.999\n\n\nbeta\n\n\n2.3\n\n\n1.200\n\n\n\n\nrho[1]\n\n\n0.001\n\n\n0.999\n\n\nbeta\n\n\n1.2\n\n\n1.800\n\n\n\n\nkappa\n\n\n0.001\n\n\n3.000\n\n\ngamma\n\n\n2.0\n\n\n4.000\n\n\n\n\nmu\n\n\n-5.000\n\n\n1.000\n\n\nnorm\n\n\n-2.0\n\n\n0.550\n\n\n\n\nsigma[1]\n\n\n0.001\n\n\n5.000\n\n\ninvgamma\n\n\n12.0\n\n\n0.050\n\n\n\n\nsigma[2]\n\n\n0.001\n\n\n5.000\n\n\ninvgamma\n\n\n9.0\n\n\n0.075\n\n\n\n\n\nParameters of six densities used in Example 1\n\n\n\n11.3.1 Analytic densities\n\n\n\n\n\nPlots of the theoretical densities given parameters in Table\n\n\n\n\n\n\n11.3.2 Random draws\n\n\n\n\n\nTrue density known so we can draw from an appropriate random number generator"
  },
  {
    "objectID": "SMH.html#estimating-the-marginals-from-the-joint-density",
    "href": "SMH.html#estimating-the-marginals-from-the-joint-density",
    "title": "11  Metropolis-Hastings",
    "section": "11.4 Estimating the marginals from the joint density",
    "text": "11.4 Estimating the marginals from the joint density\nFirst we need to understand the estimation procedure. The MH algorithm uses only information from the posterior to estimate the marginal processes that generated it, in stark contrast with Gibbs Sampling that uses conditional densities to approximate the unconditional one and then back out the marginals.\n\n11.4.1 Metropolis-Hastings\nA thorough explanation can be found in (Chib?) or (BDA?), and here we describe the procedure without proof. Our aim is to draw samples from some distribution \\[\n\\mathcal{H}(\\theta)\n\\] where a direct approach is not feasible, because we don’t have a random number generator. The Metropolis-Hastings algorithm requires that we can evaluate this posterior density at some arbitrary points. As the form of the marginals is (potentially) unknown, we draw values from some arbitrary density and decide whether it looks like it came from the marginals that generated the posterior. \\(\\mathcal{H}(\\theta)\\) is typically the posterior density where this distribution is far too complex to directly sample.\nThis indirect approach is to specify a candidate density \\[\n  q(\\theta^{k+1}|\\theta^k)\n\\] from which we can make candidate draws. Given some value for the parameters \\(\\theta^k\\), we can randomly generate new values, which may or may not be independent of this draws.\nThe MH algorithm requires that we are able to evaluate \\(\\frac{H(\\theta^{k+1})}{H(\\theta^k)}\\), and then draw a candidate value \\(\\theta^{k+1}\\) from \\(q(\\theta^{k+1}|\\theta^k)\\). We then accept this candidate value with the probability \\[\n  \\alpha = \\min \\left(\\frac{H(\\theta^{k+1})/q(\\theta^{k+1}|\\theta^k)}{H(\\theta^k)/q(\\theta^k|\\theta^{k+1})}, 1\\right)\n\\] Practically, this requires we compute \\(\\alpha\\) and draw a number \\(u\\) from \\(U(0,1)\\), and if \\(u&lt;\\alpha\\) accept \\(\\theta^{k+1}\\) otherwise keep \\(\\theta^k\\).\n\n\n11.4.2 Simplification\nThe random walk version of the algorithm takes the specific candidate density \\(q(\\theta^{k+1}|\\theta^k)\\) as\n\\[\n\\theta^{k+1} = \\theta^k + \\epsilon_t\n\\] where \\(\\epsilon_t\\sim N(0,\\Sigma)\\) for some \\(\\Sigma\\) which we need to choose. This is a simple vector-random walk. Let \\(\\theta^k\\) be some existing draw and \\(\\theta^{k+1}\\) be a new draw. We can write \\[\n  \\epsilon_t = \\theta^{k+1}-\\theta^k\n\\] then \\[\n  P(\\epsilon_t) = P(\\theta^{k+1}-\\theta^k)\n\\]\nBecause this is a normal density (which is symmetric) then\n\\[\n  P(\\epsilon_t) = P(-\\epsilon_t)\n\\] Symmetry implies an acceptance probability of\n\\[\n  \\frac{H(\\theta^{k+1})}{H(\\theta^k)}\n\\] as \\(q(\\theta^{k+1}|\\theta^k) = q(\\theta^k|\\theta^{k+1})\\) so these terms cancel.\n\n11.4.2.1 Algorithm\n\nStep 1 Draw a candidate value \\(\\theta^{G+1}\\) from \\(q(\\theta^{k+1}|\\theta^k)\\), specifically \\(\\theta^{k+1} = \\theta^k + \\epsilon_t\\) where \\(\\epsilon_t\\sim N(0,\\Sigma)\\)\nStep 2 Compute the acceptance probability \\[\n  \\alpha = \\min \\left(\\frac{H(\\theta^{k+1})}{H(\\theta^k)}, 1\\right)\n\\]\nStep 3 If \\(u\\sim U(0,1)\\) is less than \\(\\alpha\\), keep \\(\\theta^{k+1}\\), else repeat \\(\\theta^k\\) and discard the new draw\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe density \\(\\mathcal{H}(\\theta)\\) will usually be a posterior, combining the priors and likelihood information;\nAt present we have no likelihood information, so all we need is a function to evaluate the (log) joint prior."
  },
  {
    "objectID": "SMH.html#a-simpler-problem",
    "href": "SMH.html#a-simpler-problem",
    "title": "11  Metropolis-Hastings",
    "section": "11.5 A simpler problem",
    "text": "11.5 A simpler problem\nWe have no data (or indeed model) to pass to the posterior function (as there is no likelihood). Generalizing this to incorporate likelihood information is straightforward.\nWe need to specify the scale of the random walk: assume \\[\n  \\Sigma = sI\n\\] We should choose a value of \\(s\\) to ensure that the whole parameter space is explored as\n\nif \\(s\\) too small we don’t walk far enough, and stay too close to potentially only local maxima;\nif \\(s\\) too large may jump over highest density points at every step and take a long time to converge.\n\nWe check if it is a suitable value by monitoring the acceptance rate: between about 1/5 and 2/5 fine.\n\n11.5.1 Estimating example 1\nChoose some arbitrary initial values at which we can evaluate our posterior likelihood (remembering for this example this only the joint prior). These are 0.9, 0.2, 0.4, -2, 1.5, 1.5. As we have chosen values close to the highest density this evaluates as 0.425433. We choose \\(s=0.25\\) and do 100000 iterations and discard the first half. For the run that generates the graphs below we get the message:\n\n\n[1] \"Acceptance ratio: 0.33282\"\n\n\nThe acceptance ratio is good, so we plot the draws:\n\n\n\n\n\nPlots of the 50,000 draws\n\n\n\n\nor just the first few to better see the algorithm in action:\n\n\n\n\n\nPlots of first 333 draws\n\n\n\n\nClearly there are some repeat values. What do the estimated densiites from each of these sequences look like?\n\n\n\n\n\nHistograms of draws and theoretical priors\n\n\n\n\nPretty good! Compare this with the sequence of draws we got from the correct random number generators for each density.\n\n\n11.5.2 Example 2\n\n\n\n\n\n\n\nname\n\n\nlb\n\n\nub\n\n\nPDF\n\n\np1\n\n\np2\n\n\n\n\n\n\neta\n\n\n0.250\n\n\n5.000\n\n\ninvweibull\n\n\n2.3\n\n\n1.2\n\n\n\n\nzeta[1]\n\n\n0.001\n\n\n6.000\n\n\nparalogis\n\n\n1.0\n\n\n4.0\n\n\n\n\nzeta[2]\n\n\n0.001\n\n\n3.000\n\n\nparalogis\n\n\n2.0\n\n\n3.0\n\n\n\n\ndelta\n\n\n0.001\n\n\n5.000\n\n\ninvpareto\n\n\n2.0\n\n\n0.3\n\n\n\n\nalpha[1]\n\n\n1.001\n\n\n5.000\n\n\nlgamma\n\n\n2.0\n\n\n5.0\n\n\n\n\nalpha[2]\n\n\n1.001\n\n\n5.000\n\n\nlgamma\n\n\n3.0\n\n\n4.0\n\n\n\n\nupsilon[1]\n\n\n0.001\n\n\n0.999\n\n\nunif\n\n\n0.3\n\n\n0.7\n\n\n\n\nupsilon[2]\n\n\n0.001\n\n\n0.999\n\n\nunif\n\n\n0.5\n\n\n1.0\n\n\n\n\n\nAll-different distributions – Inverse Weibull, Paralogistic, Inverse Pareto, Log Gamma and Uniform\n\n\n\n\n\n\n\nPlots of the theoretical densities given parameters in Table 2\n\n\n\n\n\n\n\n\n\nWalk a little less \\(s=0.125\\) (could iterate a little more?)\n\n\n\n\n\n\n\n\nKlugman, Stuart A., Harry H. Panjer, and Gordon E. Willmot. 2008. Loss Models: From Data to Decisions. Third. Hoboken, N.J.: John Wiley & Sons, Inc."
  },
  {
    "objectID": "Stemp.html#study-question-1.3.2",
    "href": "Stemp.html#study-question-1.3.2",
    "title": "12  Causal Inference",
    "section": "12.1 Study question 1.3.2",
    "text": "12.1 Study question 1.3.2\nData:\n\nlibrary(tidyverse)\ned &lt;- tibble(Gender = c(\"M\",\"M\",\"M\",\"M\",\"F\",\"F\",\"F\",\"F\"),\n             eLevel = c(\"U\",\"H\",\"C\",\"G\",\"U\",\"H\",\"C\",\"G\"),\n             num    = c(112,231,595,242,136,189,763,172)) %&gt;%\n  mutate(total = sum(num))\n\nwhich we tabulate as\n\ned %&gt;%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\n\n\n\n\nM\nU\n112\n2440\n\n\nM\nH\n231\n2440\n\n\nM\nC\n595\n2440\n\n\nM\nG\n242\n2440\n\n\nF\nU\n136\n2440\n\n\nF\nH\n189\n2440\n\n\nF\nC\n763\n2440\n\n\nF\nG\n172\n2440"
  },
  {
    "objectID": "Stemp.html#exercises-and-answers",
    "href": "Stemp.html#exercises-and-answers",
    "title": "12  Causal Inference",
    "section": "12.2 Exercises and answers",
    "text": "12.2 Exercises and answers\n\n12.2.1 Find \\(P(eLevel = H)\\)\n\ned %&gt;%\n  filter(eLevel == \"H\") %&gt;%\n  mutate(p_H = sum(num)/total) %&gt;%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\np_H\n\n\n\n\nM\nH\n231\n2440\n0.1721311\n\n\nF\nH\n189\n2440\n0.1721311\n\n\n\n\n\n\n\n12.2.2 Find \\(P(eLevel = H\\ \\vee \\ Gender = F)\\)\n\ned %&gt;%\n  filter(Gender == \"F\" | eLevel == \"H\") %&gt;%\n  mutate(p_HorF = sum(num)/total) %&gt;%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\np_HorF\n\n\n\n\nM\nH\n231\n2440\n0.6110656\n\n\nF\nU\n136\n2440\n0.6110656\n\n\nF\nH\n189\n2440\n0.6110656\n\n\nF\nC\n763\n2440\n0.6110656\n\n\nF\nG\n172\n2440\n0.6110656\n\n\n\n\n\n\n\n12.2.3 Find \\(P(eLevel = H\\ |\\ Gender = F)\\)\n\ned %&gt;%\n  filter(Gender == \"F\") %&gt;%\n  mutate(tcond = sum(num)) %&gt;% \n  filter(eLevel == \"H\") %&gt;%\n  mutate(p_HgivenF = sum(num)/tcond) %&gt;%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\ntcond\np_HgivenF\n\n\n\n\nF\nH\n189\n2440\n1260\n0.15\n\n\n\n\n\n\n\n12.2.4 Find \\(P(Gender = F\\ | \\ eLevel = H)\\)\n\ned %&gt;%\n  filter(eLevel == \"H\") %&gt;%\n  mutate(tcond = sum(num)) %&gt;% \n  filter(Gender == \"F\") %&gt;%\n  mutate(p_FgivenH = sum(num)/tcond) %&gt;%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\ntcond\np_FgivenH\n\n\n\n\nF\nH\n189\n2440\n420\n0.45\n\n\n\n\n\n\n\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal Inference in Statistics: A Primer. Chichester: John Wiley & Sons. http://bayes.cs.ucla.edu/PRIMER/."
  },
  {
    "objectID": "Maps.html#how-heterogenous-is-uk-house-price-inflation",
    "href": "Maps.html#how-heterogenous-is-uk-house-price-inflation",
    "title": "13  Mapping regional house price inflation",
    "section": "13.1 How heterogenous is UK house price inflation?",
    "text": "13.1 How heterogenous is UK house price inflation?\nA simple enough question, and one that Bahaj, Foulis, and Pinter (2020) thought was best answered with a map – actually a referee asked for one. As I know how to draw a map in R they asked me if I could do it. Well yes, but there are some particular difficulties.\n\nThe UK (actually Great Britain) is an awkward (but not too awkward) shape.\nPopulation in the UK is heavily concentrated in a small number of centres, such as London or Manchester.\nThere are three different periods to compare.\nIt has to be in grayscale.\n\nBefore all of this we need some data, with boundaries that correspond to areas that we have data for. The regional inflation data is available at the level of the Land Registry, which almost by local authority but amalgamates a number of the areas. So a map at Local Authority level would be fine as long as we can amalgamate some of the regions.\nThe map data used here is available from the UK’s ONS geoportal, with a lot of administrative data available including local authority boundaries. The Local Authority data is specifically available from here, where I use the clipped full extent version. There are a number of possibilities, but in general high water mark, and enough but not too much detail is needed.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(sf)\n\nThe information in the map file is comprehensive, and by Local Authority as of December 2015.\n\nfle &lt;- \"LAD_Dec_2015_GCB_GB\"\nshape &lt;- read_sf(dsn=\".\", layer=fle)\n\nWe can look at the attributes using summary.\n\nsummary(shape)\n\n   lad15cd            lad15nm            lad15nmw           GlobalID        \n Length:380         Length:380         Length:380         Length:380        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n          geometry  \n MULTIPOLYGON :380  \n epsg:27700   :  0  \n +proj=tmer...:  0  \n\n\nThis can be plotted straightforwardly using ggplot.\n\nshape %&gt;%\n  ggplot() +\n  geom_sf(aes(geometry=geometry, fill=lad15nm), \n          color=NA, alpha=.66, show.legend=FALSE) +\n  theme_void()\n\n\n\n\n\n\n\n\nLooking at the read-out above, each of the 380 regions have some metadata associated, which are contained in each of the listed attributes. It should be obvious that objectid is just a sequence from 1 to 380. lad15nm turns out to be a list of names of the regions – I suspect lad for Local Authority District, 15 for 2015 and nm for name – and it is easy to specify this as the name to use for the region when using tidy.\nNow this can be plotted using ggplot, using geometry for the \\(x\\) and \\(y\\) coordinates. The choice of fill colour is determined by fill and we can set the colour of the lines by colour (or color). The two extra arguments are for a suitable blank style and to impose an appropriate ratio of height to width.\nImmediately, the awkward shape of the British Isles is apparent. (Note this is a plot of Great Britain, and there is no Northern Ireland.) The islands to the far north are somewhat unnecessary, although quite rightly the inhabitants get a bit tired of being left off maps! Nonetheless I’ll do exactly the same by filtering out the polygons associated with Orkney Islands and Shetland Islands.\nFewer Scottish Islands makes the graphs a lot clearer with little loss of information, paticularly given the tiny number of transactions in the Orkneys and the Shetlands, very far to the north.\nIn what follows we filter out the islands using\n\nshape &lt;- read_sf(dsn=\".\", layer=fle) %&gt;%\n  filter(!lad15nm %in% c(\"Shetland Islands\",\"Orkney Islands\")) %&gt;%\n  mutate(Country=str_sub(lad15cd, 1, 1), .after=1)\n\nwhere we also create an indicator of country using the first letter of the code string.\nSo the final country map is\n\nshape %&gt;%\n  group_by(Country) %&gt;%\n  summarise() %&gt;%\n  ggplot() +\n  geom_sf(aes(fill=Country), color=\"grey77\", linewidth=.25, alpha=.66) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroup and summarise can join geographical areas\n\n\n\nNote the really nice feature – if we group by something, in this case country, we can summarise to amalgamate the geometries!\n\n\nYou may have noticed, one thing that that’s missing on the LA graphs is the boundaries. They aren’t, they’re just invisible. That’s because I set colour = NA, so I can fix that by choosing a colour and making the lines very thin so they don’t swamp the map, as in the country one.\nOne further amendment, the fill is moved inside the aes() specification and made conditional. R now chooses unique colours for each of the regions.\nTwo things now need to be done to get the map colours right to illustrate regional inflation rates. First we need to amalgamate some of the Local Authority boundaries to the Land Registry definitions, and second we need to assign the inflation rate to each area."
  },
  {
    "objectID": "Maps.html#inflation-data-and-regions",
    "href": "Maps.html#inflation-data-and-regions",
    "title": "13  Mapping regional house price inflation",
    "section": "13.2 Inflation data and regions",
    "text": "13.2 Inflation data and regions\nWe have a map, and we have that data in a form that is easy to understand. If we can suitably attach an inflation rate to each area then we can fill the individual areas with a colour unique to each individual inflation rates.\nRecall that the Land Registry areas aren’t quite what we have, and will need amalgamating. Bahaj, Foulis, and Pinter (2020) supplied me the areas that needed amalgamating (and the inflation rates) using the ONS codes. This is contained in the metadata lad15cd above.\nThe data is structured in ‘wide’ format with one row for each Land Registry region. The details aren’t very important for us now, but what it means is I can manipulate it to get\n\n# Price data by Land Registry region, converted to long format\nhp_data &lt;- read_excel(\"house_price_data_figure_1.xls\")  %&gt;% \n  select(\"land_reg_region\", starts_with(\"e_\"), starts_with(\"av_\")) %&gt;% \n  pivot_longer(names_to  = \"name\", \n               values_to = \"lad15cd\", \n               cols      = c(-land_reg_region, -starts_with(\"av_\"))) %&gt;% \n  drop_na() %&gt;%\n  select(land_reg_region, lad15cd, starts_with(\"av_\")) \n\ncodes &lt;- hp_data %&gt;% \n  select(lad15cd, land_reg_region) \n\nThe important thing that the pivot_longer achieves is that for every land_reg_region I get a list of all the ONS codes that makes up the Local Authority level. So if I look at buckinghamshire as an example there are four ONS codes now associated with it.\n\nfilter(codes, land_reg_region == \"buckinghamshire\")\n\n# A tibble: 4 × 2\n  lad15cd   land_reg_region\n  &lt;chr&gt;     &lt;chr&gt;          \n1 E07000004 buckinghamshire\n2 E07000005 buckinghamshire\n3 E07000006 buckinghamshire\n4 E07000007 buckinghamshire\n\n\nJoin these together\n\n# Join polygons defined by Land Registry regions\ngg &lt;- shape %&gt;%\n  select(starts_with(c(\"lad\",\"C\"))) %&gt;% \n  left_join(codes, by=\"lad15cd\") %&gt;%\n  group_by(land_reg_region) %&gt;%\n  summarise() \n\nwhich produces a match between the Land Registry and the Local Authority areas, plus the inflation rates.\n\n13.2.1 Inflation in grayscale\nAll the information required to plot the Land Registry-based regional inflation rates is now available. As you can see from the buckinghamshire data above, there are three average rates in three different periods, so I’ll focus on one, 2002-2007 to begin with.\nFirst, augment the geographic data with the inflation data, and call them something better.\n\ngg %&gt;%\n  ggplot() +\n  geom_sf(aes(geometry=geometry, fill=land_reg_region), \n          color=NA, alpha=.66, show.legend = FALSE) +\n  theme_void()\n\n\n\n\n\n\n\n\nThen specify gray and put the legend at the bottom.\n\nnms &lt;- gsub(\"av_hp_growth\", \"HPI\", colnames(hp_data))\n\nhp_data %&gt;%\n  rename_all( ~ nms) %&gt;%\n  select(land_reg_region, starts_with(\"HPI\")) %&gt;%\n  distinct() %&gt;%\n  left_join(gg) %&gt;%\n  ggplot() +\n  geom_sf(aes(geometry=geometry, fill=HPI_02_07), \n          color=NA, alpha=.66, show.legend = TRUE) +\n  theme_void() +\n  scale_fill_gradient(low=grey(0.9), high=grey(0.05)) +\n  theme(legend.direction = \"horizontal\", \n        legend.position  = c(0.75,0.05),\n        legend.title     = element_blank())\n\nJoining with `by = join_by(land_reg_region)`\n\n\n\n\n\n\n\n\n\n\n\n\n\nBahaj, Saleem, Angus Foulis, and Gabor Pinter. 2020. “Home Values and Firm Behavior.” American Economic Review 110 (7): 2225–70. https://doi.org/10.1257/aer.20180649."
  },
  {
    "objectID": "BK.html#introduction",
    "href": "BK.html#introduction",
    "title": "14  Linear rational expectations models",
    "section": "14.1 Introduction",
    "text": "14.1 Introduction\nHow do we solve rational expectations models? What does that even mean? Here I show how to implement versions of the Blanchard and Kahn (1980) and Klein (2000) solutions to linear rational expectations models in R. The implementation is fairly general, and copes with singular models. It is a very transparent implementation, with all the necessary code, and also shows how to calculate and plot impulse responses."
  },
  {
    "objectID": "BK.html#model",
    "href": "BK.html#model",
    "title": "14  Linear rational expectations models",
    "section": "14.2 Model",
    "text": "14.2 Model\nWe take a simple New Keynesian model \\[\\begin{align}\ny_t    &= y_{t+1}^e-\\frac{1}{\\sigma} (i_t - \\pi_{t+1}^e) + e_t^1 \\\\\n\\pi_t  &= \\beta \\pi_{t+1}^e + \\kappa y_t + e_t^2 \\\\\ni_t    &= \\gamma i_{t-1} + (1-\\gamma) \\delta \\pi_t + \\varepsilon_t^3 \\\\\ne_t^1  &= \\rho_1 e_{t-1}^1 + \\varepsilon_t^1 \\\\\ne_t^2  &= \\rho_2 e_{t-1}^2 + \\varepsilon_t^2\n\\end{align}\\] The model comprises a dynamic IS curve, a Phillips Curve and a policy rule with smoothing. There are three shocks, two of which are persistent. This we need to write in the general algebraic linear state-space form: \\[\nE\\begin{bmatrix} z_t \\\\ x_{t+1}^e \\end{bmatrix} = A \\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix} + B \\varepsilon_t  \n\\] We map our variables to their algebraic equivalent as (\\(z_t\\), \\(x_t\\)) \\(=\\) ((\\(e^1_t\\), \\(e^2_t\\), \\(i_t\\)), (\\(y_t\\), \\(\\pi_t\\))). Then the model in state-space form but including the matrix \\(E\\) is \\[\n\\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\\\\n                0 & 1 & 0 & 0 & 0 \\\\\n                0 & 0 & 1 & 0 & 0 \\\\\n                1 & 0 & -\\frac{1}{\\sigma} & 1 & \\frac{1}{\\sigma} \\\\\n                0 & 1 & 0 & 0 & \\beta\n\\end{bmatrix}\n\\begin{bmatrix} e^1_t \\\\ e^2_t \\\\ i_t \\\\ y^e_{t+1} \\\\ \\pi^e_{t+1} \\end{bmatrix}\n   =\n   \\begin{bmatrix} \\rho_1 & 0 & 0 & 0 & 0 \\\\\n                0 & \\rho_2 & 0 & 0 & 0 \\\\\n                0 & 0 & \\gamma & 0 & (1-\\gamma)\\delta \\\\\n                0 & 0 & 0 & 1 & 0 \\\\\n                0 & 0 & 0 & -\\kappa & 1\n   \\end{bmatrix}\n\\begin{bmatrix} e^1_{t-1} \\\\ e^2_{t-1} \\\\ i_{t-1} \\\\ y_t \\\\ \\pi_t \\end{bmatrix}    \n   +\n      \\begin{bmatrix}\n                1 & 0 & 0  \\\\\n                0 & 1 & 0 \\\\\n                0 & 0 & 1 \\\\\n                0 & 0 & 0 \\\\\n                0 & 0 & 0\n   \\end{bmatrix}\n   \\begin{bmatrix} \\varepsilon^1_t \\\\ \\varepsilon^2_t \\\\ \\varepsilon^3_t \\end{bmatrix}    \n\\] Anyone wanting to code up solutions should familiarize themselves with this before continuing.\n\n14.2.1 Coding the model\nBefore we begin coding this in R, load the tidyverse libraries so we can do impulse responses with our usual tool kit and then we can forget about it.\n\nlibrary(tidyverse)\n\nSet the model parameters\n\nnf    &lt;- 2\nns    &lt;- 5\nne    &lt;- 3\nnp    &lt;- ns-nf\n\nbeta  &lt;- 0.99   # Discount factor \nsigma &lt;- 2.0    # Elas. substitution\nkappa &lt;- 0.075  # Slope PC\ndelta &lt;- 1.5    # Inflation feedback\ngamma &lt;- 0.75   # Smoothing\nrho_1 &lt;- 0.9    # AR1\nrho_2 &lt;- 0.8    # AR1\nOmega &lt;- diag(c(0.33,0.33,0.33)) # SE of 3 shocks\n\nNow define the model matrices ‘long hand’ and some variable names, which we put in labels.\n\nlabels &lt;- c(\"e^1\",\"e^2\",\"i\",\"y\",\"pi\")\n\nE &lt;- matrix(0,ns,ns)\nA &lt;- matrix(0,ns,ns)\nB &lt;- diag(1,ns,ne)\n\n# Now put the equations in matrix form\ndiag(E[1:2,1:2]) &lt;- 1\ndiag(A[1:2,1:2]) &lt;- c(rho_1, rho_2)\n\nE[3,3]             &lt;- 1 \nE[4,c(1, 3, 4, 5)] &lt;- c(1, -1/sigma, 1, 1/sigma)\nE[5,c(2, 5)]       &lt;- c(1, beta)\n\nA[3,c(3, 5)]       &lt;- c(gamma, (1-gamma)*delta)\nA[4,4]             &lt;- 1\nA[5,c(4,5)]        &lt;- c(-kappa, 1)\n\nwhere for example, \\(E\\) and \\(A\\) are \\[\\begin{equation}\n  E = \\left[\\begin{matrix}1 &0 &0 &0 &0 \\\\0 &1 &0 &0 &0 \\\\0 &0 &1 &0 &0 \\\\1 &0 &-0.5 &1 &0.5 \\\\0 &1 &0 &0 &0.99 \\\\\\end{matrix}\\right]\n\\end{equation}\\] \\[\\begin{equation}  \n  A = \\left[\\begin{matrix}0.9 &0 &0 &0 &0 \\\\0 &0.8 &0 &0 &0 \\\\0 &0 &0.75 &0 &0.375 \\\\0 &0 &0 &1 &0 \\\\0 &0 &0 &-0.075 &1 \\\\\\end{matrix}\\right]\n\\end{equation}\\] Calculate the reduced form state-space model \\[\\begin{equation}\n\\begin{bmatrix} z_t \\\\ x_{t+1}^e \\end{bmatrix} = C \\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix} + D \\varepsilon_t  \n\\end{equation}\\] which is done in R very simply as\n\nC &lt;- solve(E,A)\nD &lt;- solve(E,B)\n\nWhy can’t we solve this for impulse responses?\nThe following function simulates the impulse responses of a model in a loop within a loop1 and returns the time series in a suitably organised data frame.\n\nimpulse_responses &lt;- function(P, Q, Omega, labels, T) {\n  s   &lt;- matrix(0, ncol(Q), 1)\n  z   &lt;- matrix(0, nrow(Q), T)\n  rownames(z) &lt;- labels\n  dza &lt;- NULL\n  for (j in 1:ncol(Q)) {\n    s[j]  &lt;- Omega[j,j]\n    z[,1] &lt;- Q %*% s\n    for (i in 1:(T-1)) {\n      z[,i+1] &lt;- P %*% z[,i]\n    }\n    s[j] &lt;- 0\n    dz &lt;- as_tibble(t(z)) %&gt;% \n      mutate(Period = 1:T, Shock = paste0(\"epsilon^\",j))\n    dza &lt;- bind_rows(dza,dz)\n  }\n  return(dza)\n}\n\nA function to plot the impulses will be useful, so we create one.\n\nresponse_plot &lt;- function(series, title) {\n  return(pivot_longer(series, cols = -c(Period,Shock), names_to=\"Var\", values_to = \"Val\") %&gt;%\n           ggplot() +\n           geom_line(aes(x=Period, y=Val, group=Shock, colour=Var), show.legend=FALSE) +\n           facet_grid(Shock~Var, scales=\"free\", labeller=label_parsed) +\n           scale_x_continuous(expand=c(0,0)) +\n           theme_minimal() +\n           labs(title=title, x=\"\",y=\"\"))\n}\n\nCall the impulse response function using the model \\(C\\) and \\(D\\).\n\nT &lt;- 25\nz &lt;- impulse_responses(C, D, Omega, labels, T)\n\nand plot\n\nresponse_plot(z, \"Impulse responses: Taylor rule\")\n\n\n\n\n\n\n\n\nOh! That’s not looking good. Let’s try a few more periods.\n\nT &lt;- 150\nz &lt;- impulse_responses(C, D, Omega, labels, T)\nresponse_plot(z, \"Impulse responses: Taylor rule\")\n\n\n\n\n\n\n\n\nThis is clearly exploding. But it’s rational – we’re solving forward so expectations are always fulfilled. This is a key the insight of the early rational expectations modellers – rational isn’t enough, non-explosive is necessary too. Fortunately we know how to find this."
  },
  {
    "objectID": "BK.html#bk80",
    "href": "BK.html#bk80",
    "title": "14  Linear rational expectations models",
    "section": "14.3 Blanchard and Kahn (1980)",
    "text": "14.3 Blanchard and Kahn (1980)\nTo solve this model to give a unique stable rational expectations equilibrium, we appeal to the following. Consider the eigenvalue decomposition \\[\n  MC=\\Lambda M\n\\] where \\(\\Lambda\\) is a diagonal matrix of eigenvalues in increasing absolute value and \\(M\\) is a non-singular matrix of left eigenvectors. Note that computer routines (including the one in R) usually calculate right eigenvectors such that \\(CV=V\\Lambda\\) and that \\(M=V^{-1}\\), so be aware of this in what follows.\nWe can diagonalise \\(C\\) and write it as \\(C=M^{-1}\\Lambda M\\). So pre-multiplying the reduced form model by \\(M\\) gives \\[\nM \\begin{bmatrix} z_t \\\\ x_{t+1}^e \\end{bmatrix} = \\Lambda M \\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix} + M D \\varepsilon_t\n\\] Blanchard and Kahn (1980) (following Vaughan (1970)) show uniqueness requires as many unstable eigenvalues as jump variables. To see this, define \\[\n\\begin{bmatrix} \\xi_{t-1}^{s} \\\\  \\xi_t^{u} \\end{bmatrix}\n  =  \\begin{bmatrix} M_{11} & M_{12} \\\\  M_{21} & M_{22} \\end{bmatrix}\n      \\begin{bmatrix} z_{t-1} \\\\  x_t \\end{bmatrix}\n\\] Write the normalized model as \\[\n\\begin{bmatrix} \\xi_t^s \\\\ \\xi_{t+1}^u \\end{bmatrix}\n= \\begin{bmatrix} \\Lambda_s & 0 \\\\ 0 & \\Lambda_u \\end{bmatrix}\n\\begin{bmatrix} \\xi_{t-1}^s \\\\ \\xi_t^u \\end{bmatrix} +\n\\begin{bmatrix} M_1 \\\\ M_2 \\end{bmatrix} D\\varepsilon_t\n\\] where the eigenvalues are split into stable (\\(\\Lambda_s\\)) and unstable (\\(\\Lambda_u\\)). If we ignore the stochastic bit for a moment \\[\n\\begin{bmatrix} \\xi_t^s \\\\ \\xi_{t+1}^u \\end{bmatrix}\n= \\begin{bmatrix} \\Lambda_s & 0 \\\\ 0 & \\Lambda_u \\end{bmatrix}\n\\begin{bmatrix} \\xi_{t-1}^s \\\\ \\xi_t^u \\end{bmatrix}\n\\]\nWe seek a non-explosive solution, and this turns out to be easy to find using the following\n\nThe dynamics of \\(\\xi_t^u\\) are determined by \\(\\Lambda_u\\) and nothing else;\nIf they don’t start at \\(0\\) they must explode;\nThis implies they must start at \\(0\\) and are always \\(0\\).\n\nThus the definition of the canonical variables necessarily implies \\[\n\\begin{bmatrix} \\xi_{t-1}^s \\\\  0 \\end{bmatrix}\n  =  \\begin{bmatrix} M_{11} & M_{12} \\\\  M_{21} & M_{22} \\end{bmatrix}\n      \\begin{bmatrix} z_{t-1} \\\\  x_t \\end{bmatrix}\n\\]\nFrom this it is clear that the jump variables themselves are only on the saddle path if \\[\n   M_{21} z_{t-1} + M_{22} x_t = 0\n\\]\nThe rational solution implies that the jump variables are linearly related to the predetermined ones through \\[\\begin{align}\nx_t &= -M_{22}^{-1} M_{21}z_{t-1} \\\\\n    &= N z_{t-1}\n\\end{align}\\] We’ll deal with the shocks in a moment.\nHow do we do this in R? First, find the eigenvalue decomposition of \\(C\\) using\n\nm &lt;- eigen(C, symmetric=FALSE)\n\nwhich yields\n\n\neigen() decomposition\n$values\n[1] 1.0715518+0.092734i 1.0715518-0.092734i 0.9000000+0.000000i\n[4] 0.8000000+0.000000i 0.6548762+0.000000i\n\n$vectors\n                      [,1]                  [,2]         [,3]           [,4]\n[1,]  0.0000000+0.0000000i  0.0000000+0.0000000i 0.2854942+0i  0.00000000+0i\n[2,]  0.0000000+0.0000000i  0.0000000+0.0000000i 0.0000000+0i  0.09783896+0i\n[3,]  0.1599159-0.5089425i  0.1599159+0.5089425i 0.7830500+0i  0.49622464+0i\n[4,] -0.6991064+0.0000000i -0.6991064+0.0000000i 0.4552131+0i -0.86012270+0i\n[5,]  0.2629802-0.3968579i  0.2629802+0.3968579i 0.3132200+0i  0.06616328+0i\n              [,5]\n[1,]  0.0000000+0i\n[2,]  0.0000000+0i\n[3,]  0.6351203+0i\n[4,] -0.7554249+0i\n[5,] -0.1611069+0i\n\n\nHowever this calculates right eigenvectors. We will need to invert it for left ones. Given the number of jump variables in the model satisfies the Blanchard-Kahn conditions of as many unstable roots (1.072+0.093i, 1.072-0.093i) as jump variables (2) we can calculate the reaction function from the eigenvectors\n\niz &lt;- 1:np\nix &lt;- (np+1):ns\nM  &lt;- solve(m$vectors[,ns:1])        # Invert & reverse order for increasing abs value\nN  &lt;- -Re(solve(M[ix,ix], M[ix,iz])) # Drop tiny complex bits (if any)\n\nwhere iz are the indices of the first np variables and ix those of the remaining nf ones.\n\n14.3.1 Stochastic part\nWhat about the shocks? Assume the stochastic reaction function is \\[\n  x_t = N z_{t-1} + G \\varepsilon_t\n\\] Following Blake (2004), note that \\(x_{t+1}^e = N z_t\\) as the expected value of \\(\\varepsilon_{t+1}=0\\), meaning we can write \\[\nNz_t = C_{21}z_{t-1} + C_{22} x_t + D_2 \\varepsilon_t\n\\] or \\[\nN\\left( C_{11}z_{t-1} + C_{12}x_t + D_1 \\varepsilon_t\\right) = C_{21}z_{t-1} + C_{22} x_t + D_2 \\varepsilon_t\n\\] Gathering terms we obtain \\[\n  (C_{22} - N C_{12}) x_t = (NC_{11} - C_{21}) z_{t-1} + (N D_1 - D_2) \\varepsilon_t\n\\] which implies \\[\nG=(C_{22} - N C_{12})^{-1}(N D_1 - D_2)\n\\] Notice it also implies \\(N = (C_{22} - N C_{12})^{-1}(NC_{11} - C_{21})\\). It is this fixed point nature of the solution for \\(N\\) – which in turn implies the quadratic matrix equation \\(C_{21} = NC_{11} - C_{22}N + N C_{12}N\\) – that means we need to use the Blanchard and Kahn (1980) method in the first place.\nAll of this means that\n\nG &lt;- solve((C[ix,ix] - N %*% C[iz,ix]), (N %*% D[iz,]- D[ix,]))\n\nso for our model and parameters \\(N\\) and \\(G\\) are\n\\[\\begin{equation}  \n  N = \\left[\\begin{matrix}4.8568 &-2.7586 \\\\-1.1894 &1.7929 \\\\1.9628 &-0.2537 \\\\\\end{matrix}\\right]\n\\end{equation}\\] \\[\\begin{equation}\n  G = \\left[\\begin{matrix}5.3964 &-3.4483 \\\\-1.5859 &1.9921 \\\\2.4535 &-0.3382 \\\\\\end{matrix}\\right]\n\\end{equation}\\]\nThe ‘fixed point’ check is that the following should be the same as \\(N\\)\n\nsolve((C[ix,ix] - N %*% C[iz,ix]), (N %*% C[iz,iz]- C[ix,iz]))\n\n        [,1]      [,2]       [,3]\n[1,] 4.85680 -2.758647 -1.1894200\n[2,] 1.79286  1.962790 -0.2536635\n\n\nwhich it is.\nThe solved model is finally \\[\\begin{align}\n\\begin{bmatrix} z_t \\\\ x_t \\end{bmatrix} &= \\begin{bmatrix} C_{11}+C_{12}N & 0 \\\\ N & 0 \\end{bmatrix} \\begin{bmatrix} z_{t-1} \\\\ x_{t-1} \\end{bmatrix} + \\begin{bmatrix} D_1+C_{12}G \\\\ G \\end{bmatrix} \\varepsilon_t \\\\\n&= P \\begin{bmatrix} z_{t-1} \\\\ x_{t-1} \\end{bmatrix} + Q \\varepsilon_t\n\\end{align}\\] which can be coded as\n\nP  &lt;- cbind(rbind((C[iz,iz] + C[iz,ix] %*% N), N), matrix(0,ns,nf))\nQ  &lt;- rbind(D[iz,] + C[iz,ix] %*% G, G)\n\n\n\n14.3.2 Digression – right eigenvector version\nIt turns out that we could use the output from the standard eigenvalue/vector routine directly by exploiting the following. This time, let \\(M\\) be the matrix of right eigenvectors so \\[\n  C M = M \\Lambda \\text{  or  } C = M\\Lambda M^{-1}\n\\]\nand \\[\n\\begin{bmatrix} M_{11} & M_{12} \\\\ M_{21} & M_{22} \\end{bmatrix}\n\\begin{bmatrix} \\xi_{t-1}^s \\\\ \\xi_t^u \\end{bmatrix}\n=\n\\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix}\n\\]\nWritten this way around, if \\(\\xi_t^{u}=0\\) \\(\\forall\\ t\\) then (again ignoring stochastics)\n\\[\n  M_{11} \\xi_{t-1}^s = z_{t-1}, \\ M_{21}\\xi_t^s = x_t\n\\]\n\\[\n   \\Rightarrow x_t = M_{21} M_{11}^{-1} z_t\n\\] so\n\nM &lt;- m$vectors[,ns:1]            # Don't invert as already right vectors, but reorder\nRe(M[ix,iz] %*% solve(M[iz,iz])) # Again, drop tiny complex bits\n\n        [,1]      [,2]       [,3]\n[1,] 4.85680 -2.758647 -1.1894200\n[2,] 1.79286  1.962790 -0.2536635\n\n\nThe result is identical. This method is particularly useful if there are fewer predetermined variables than jumps as the matrix we need to invert is of the same dimension as the predetermined variables this way round.\n\n\n14.3.3 Impulse responses\nWe now call the impulse response function using the model solved for rational expectations.\n\nT &lt;- 25\nz &lt;- impulse_responses(P, Q, Omega, labels, T)\n\nNow plot these responses\n\nresponse_plot(z, \"Impulse responses: Taylor rule\")\n\n\n\n\n\n\n\n\nNow, that looks better! It is no longer explosive. It also makes complete economic sense, which you can verify by going through the dynamics of the different demand, supply and monetary shocks."
  },
  {
    "objectID": "BK.html#generalized-solution",
    "href": "BK.html#generalized-solution",
    "title": "14  Linear rational expectations models",
    "section": "14.4 Generalized solution",
    "text": "14.4 Generalized solution\nSometimes for a model \\(E\\) is singular. A more general solution was proposed by Klein (2000), that doesn’t require \\(E\\) to be non-singular. This uses a generalized Schur decomposition instead of an eigenvalue one and is applied to the structural model represented by the matrix pencil \\((A,E)\\), and is considered much more numerically stable (see Pappas, Laub, and Sandell (1980)). The generalized Schur form of \\((A,E)\\) is \\((QTZ', QSZ')\\), so we can write the model as \\[\nE \\begin{bmatrix} z_t \\\\ x_{t+1}^e \\end{bmatrix} \\equiv QTZ' \\begin{bmatrix} z_t \\\\ x_{t+1}^e \\end{bmatrix} \\equiv QT \\begin{bmatrix} \\xi_t^s \\\\ \\xi_{t+1}^u \\end{bmatrix}\n\\] and \\[\nA \\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix} \\equiv QSZ' \\begin{bmatrix} z_{t-1} \\\\ x_t \\end{bmatrix} \\equiv QS\\begin{bmatrix} \\xi_{t-1}^s \\\\ \\xi_t^u \\end{bmatrix}\n\\] so the model pre-multiplied by \\(Q'\\) is \\[\nT \\begin{bmatrix} \\xi_{t+1}^s \\\\ \\xi_{t+1}^u \\end{bmatrix} = S \\begin{bmatrix} \\xi_t^s \\\\ \\xi_t^u \\end{bmatrix} + Q'B\\varepsilon_t\n\\]\nWe use the function gqz from the library geigen for this\n\nd &lt;- geigen::gqz(A, E, sort=\"S\") # Option \"S\" puts the stable roots first\n\nWe can check that this is actually saddle path using gevalues() to get all the eigenvalues from the generalized Schur decomposition, and the unstable ones are\n\ne &lt;- geigen::gevalues(d)\ne[abs(e) &gt; 1]\n\n[1] 1.071552+0.092734i 1.071552-0.092734i\n\n\nThe number of stable roots is returned in d$sdim which is 3.\nWe then modify our solution function to calculate Ns and Gs using the matrix Z and a generalized version of the formula for \\(G\\) and calculate the reduced form model Ps and `Q which are \\[\\begin{align}\n    N_s &= Z_{21} Z_{11}^{-1} \\\\\n    H   &= (E_{11} + E_{12} N_s)^{-1} \\\\\n    W   &= (E_{21} + E_{22} N_s) H\\\\\n    G_s &= (A_{22} - W A_{12})^{-1} (W B_1 - B_2) \\\\\n    P_s &= H (A_{11} + A_{12} N_s) \\\\\n    Q_s &= H (B_1 + A_{12} G_s)\n\\end{align}\\] Verify this yourself with a bit of matrix algebra!\nThe R code for this is\n\nsolveGenBK &lt;- function(E,A,B,n) {\n  d  &lt;- geigen::gqz(A, E, sort=\"S\") \n  np &lt;- d$sdim\n  ns &lt;- nrow(E)\n  print(paste(\"Number of unstable roots is\", ns-np))\n  if (n == np) {\n    iz &lt;- 1:n\n    ix &lt;- (n+1):ns\n    Ns &lt;- d$Z[ix,iz] %*% solve(d$Z[iz,iz])\n    H  &lt;- solve(E[iz,iz] + E[iz,ix] %*% Ns)\n    W  &lt;- (E[ix,iz] + E[ix,ix] %*% Ns) %*% H\n    Gs &lt;- solve((A[ix,ix] - W %*% A[iz,ix]), (W %*% B[iz,] - B[ix,]))\n    As &lt;- H %*% (A[iz,iz] + A[iz,ix] %*% Ns)\n    Bs &lt;- H %*% (B[iz,] + A[iz,ix] %*% Gs)\n    return(list(P=cbind(rbind(As,Ns),matrix(0,ns,ns-n)), Q=rbind(Bs, Gs)))\n    } \n  else { \n    return(-1) \n    }\n}\n\nUsing this on our original model gives\n\nS  &lt;- solveGenBK(E,A,B,np)\n\n[1] \"Number of unstable roots is 2\"\n\nPs &lt;- S$P\nQs &lt;- S$Q\n\nand comparing Ps and Qs with P and Q obtained using Blanchard-Kahn we find\n\nround(max(abs(P-Ps), abs(Q-Qs)), 12)\n\n[1] 0\n\n\nThey are, as expected, the same – at least up to 12 decimal places, which should be enough."
  },
  {
    "objectID": "BK.html#singular-models-optimal-policy",
    "href": "BK.html#singular-models-optimal-policy",
    "title": "14  Linear rational expectations models",
    "section": "14.5 Singular models: optimal policy",
    "text": "14.5 Singular models: optimal policy\nHowever, this is an easy test. What we need is to use a model that can’t be solved using the BK method. Under optimal policy, the interest rate instrument rule is replaced with a targeting rule, so that \\[\n  \\pi_t = -\\mu \\Delta y_t - \\varepsilon^3_t\n\\] for some value of \\(\\mu\\) that reflects the optimal trade-off between output (gap) growth and inflation, and we’ve included a disturbance which we can loosely describe as a monetary policy shock. We modify the model above by dropping the Taylor rule in favour of the targeting rule. This requires a lagged value of \\(y\\) to be created. The following does the trick\n\nnf &lt;- 2\nne &lt;- 3\nns &lt;- 6      # One extra state\nnp &lt;- ns-nf\nmu &lt;- 0.75   # Representative trade-off\n\nlabels &lt;- c(\"e^1\",\"e^2\",\"ylag\",\"i\",\"y\",\"pi\") # New variable order\n\nE &lt;- matrix(0,ns,ns)\nA &lt;- E\nB &lt;- matrix(0,ns,ne)\nB[1,1] &lt;- 1\nB[2,2] &lt;- 1\nB[4,3] &lt;- -1\n\ndiag(E[1:3,1:3]) &lt;- 1\ndiag(A[1:2,1:2]) &lt;- c(rho_1, rho_2)\nA[3,5]           &lt;- 1\n\nE[4,3]           &lt;- 1\nA[4,c(3, 6)]     &lt;- c(1, -1/mu)\n\nE[5,c(1, 4, 5, 6)] &lt;- c(1, -1/sigma, 1, 1/sigma)\nA[5,5]           &lt;- 1\n\nE[6,c(2, 6)]     &lt;- c(1, beta)\nA[6,c(5, 6)]     &lt;- c(-kappa, 1)\n\nThe new \\(E\\) and \\(A\\) system matrices are then \\[  \nE = \\left[\\begin{matrix}1 &0 &0 &0 &0 &0 \\\\0 &1 &0 &0 &0 &0 \\\\0 &0 &1 &0 &0 &0 \\\\0 &0 &1 &0 &0 &0 \\\\1 &0 &0 &-0.5 &1 &0.5 \\\\0 &1 &0 &0 &0 &0.99 \\\\\\end{matrix}\\right]\n\\] \\[  \nA = \\left[\\begin{matrix}0.9 &0 &0 &0 &0 &0 \\\\0 &0.8 &0 &0 &0 &0 \\\\0 &0 &0 &0 &1 &0 \\\\0 &0 &1 &0 &0 &-1.333 \\\\0 &0 &0 &0 &1 &0 \\\\0 &0 &0 &0 &-0.075 &1 \\\\\\end{matrix}\\right]\n\\] Now we have a singular model. The matrix \\(E\\) is clearly singular as rows 3 and 4 are identical. But we have a problem using the code above. To use it we need the matrices \\(H\\) and \\((A_{22} - W A_{21})\\) to be non-singular. What to do?\nThere are two ways out. Klein (2000) gives a solution that depends on the decomposed matrix pencil, which is what is typically implemented, but you don’t actually need it although it is easiest. Instead, all you need to do is reorder the equations.\nThe real problem is that with a targeting rule that doesn’t include the interest rate, and the interest rate is now only determined by the IS curve. But we can swap the location of any two rows of the model arbitrarily. If we swap the positions of the equations for the IS curve and the targeting rule (rows 4 and 5) using the following\n\nE[4:5,] &lt;- E[5:4,]\nA[4:5,] &lt;- A[5:4,]\nB[4:5,] &lt;- B[5:4,]\n\nthen the model is unchanged but now we have \\[  \nE_{11} = \\left[\\begin{matrix}1 &0 &0 &0 \\\\0 &1 &0 &0 \\\\0 &0 &1 &0 \\\\1 &0 &0 &-0.5 \\\\\\end{matrix}\\right]\n\\] so \\(E_{11} + E_{12}N\\) is likely non-singular (it is). Also, note after the re-ordering \\(A_{22}\\) is \\[  \nA_{22} = \\left[\\begin{matrix}0 &-1.33 \\\\-0.07 &1 \\\\\\end{matrix}\\right]\n\\] which is guaranteed non-singular for zero \\(W\\). We can now proceed as before. First, check for saddle path stability\n\ne &lt;- geigen::gevalues(geigen::gqz(A, E, sort=\"S\"))\ne[abs(e) &gt; 1]\n\n[1] 1.378195      Inf\n\n\nwhich confirms that it has a unique saddle path stable solution. This is\n\nSo &lt;- solveGenBK(E,A,B,np)\n\n[1] \"Number of unstable roots is 2\"\n\nPo &lt;- So$P\nQo &lt;- So$Q\n\nThe solved model is then\n\nPo\n\n     [,1]      [,2]          [,3] [,4] [,5] [,6]\n[1,]  0.9  0.000000  0.000000e+00    0    0    0\n[2,]  0.0  0.800000  6.986592e-17    0    0    0\n[3,]  0.0 -1.863455  7.329156e-01    0    0    0\n[4,]  1.8 -1.241330 -2.446879e-01    0    0    0\n[5,]  0.0 -1.863455  7.329156e-01    0    0    0\n[6,]  0.0  1.397591  2.003133e-01    0    0    0\n\nQo\n\n     [,1]      [,2]          [,3]\n[1,]    1  0.000000  0.000000e+00\n[2,]    0  1.000000 -6.986592e-17\n[3,]    0 -2.329318 -7.329156e-01\n[4,]    2 -1.551663  2.446879e-01\n[5,]    0 -2.329318 -7.329156e-01\n[6,]    0  1.746989 -2.003133e-01\n\n\n\n14.5.1 Optimal impulse responses\nWe can now simulate the model under optimal policy and plot using\n\nzo &lt;- impulse_responses(Po, Qo, Omega, labels, T) %&gt;%\n  select(-ylag) # Drop duplicate series\nresponse_plot(zo, \"Impulse responses: Optimal policy\")"
  },
  {
    "objectID": "BK.html#dummy-jumps",
    "href": "BK.html#dummy-jumps",
    "title": "14  Linear rational expectations models",
    "section": "14.6 Dummy jumps",
    "text": "14.6 Dummy jumps\nBut this isn’t the only way to get this to work. Effectively what we just did was create an extra predetermined variable and reorder the system to give us non-singularity. What if instead of including an unused \\(i_{t-1}\\) on the right hand side, we instead include an unused \\(i^e_{t+1}\\) on the left hand side? So we swap to having one more jump variable, one less predetermined one?\nCompare the following to the previous model. When we pick out the interest rate we do so on the right hand side of the matrix equation, not the left as before.\n\nns &lt;- 6      # One extra state\nnf &lt;- 3      # And one extra jump\nnp &lt;- ns-nf\nlabels &lt;- c(\"e^1\",\"e^2\",\"ylag\",\"i\",\"y\",\"pi\") # New variable order\n\nE &lt;- matrix(0,ns,ns)\nA &lt;- E\nB &lt;- matrix(0,ns,ne)\nB[1,1] &lt;- 1\nB[2,2] &lt;- 1\nB[4,3] &lt;- -1\n\ndiag(E[1:3,1:3]) &lt;- 1\ndiag(A[1:2,1:2]) &lt;- c(rho_1, rho_2)\nA[3,5]           &lt;- 1\n\nE[4,3]           &lt;- 1\nA[4,c(3, 6)]     &lt;- c(1, -1/mu)\n\nE[5,c(1, 5, 6)]  &lt;- c(1, 1, 1/sigma) # One less coefficient\nA[5,c(4, 5)]     &lt;- c(1/sigma, 1)    # One more - nothing else changes\n\nE[6,c(2, 6)]     &lt;- c(1, beta)\nA[6,c(5, 6)]     &lt;- c(-kappa, 1)\n\nThis is still a singular model, as we can see from \\[  \nE = \\left[\\begin{matrix}1 &0 &0 &0 &0 &0 \\\\0 &1 &0 &0 &0 &0 \\\\0 &0 &1 &0 &0 &0 \\\\0 &0 &1 &0 &0 &0 \\\\1 &0 &0 &0 &1 &0.5 \\\\0 &1 &0 &0 &0 &0.99 \\\\\\end{matrix}\\right]\n\\] with column 4 all zeros. Is this model saddle path stable?\n\ne &lt;- geigen::gevalues(geigen::gqz(A, E, sort=\"S\") )\ne[abs(e) &gt; 1]\n\n[1]     -Inf 1.378195      Inf\n\n\nAgain, it is with an extra unstable root for the extra jump variable. We could simplify the solution. As that top left 3 by 3 block, \\(E_{11}\\), is the identity matrix and \\(E_{12}\\) is all zeros this Ei is always an identity matrix. However, here we simply re-use solveGenBG\n\nSo2 &lt;- solveGenBK(E,A,B,np)\n\n[1] \"Number of unstable roots is 3\"\n\nPo2 &lt;- So2$P\nQo2 &lt;- So2$Q\n\nNow the solved model is\n\nPo2\n\n     [,1]      [,2]       [,3] [,4] [,5] [,6]\n[1,]  0.9  0.000000  0.0000000    0    0    0\n[2,]  0.0  0.800000  0.0000000    0    0    0\n[3,]  0.0 -1.863455  0.7329156    0    0    0\n[4,]  1.8 -1.241330 -0.2446879    0    0    0\n[5,]  0.0 -1.863455  0.7329156    0    0    0\n[6,]  0.0  1.397591  0.2003133    0    0    0\n\nQo2\n\n     [,1]      [,2]       [,3]\n[1,]    1  0.000000  0.0000000\n[2,]    0  1.000000  0.0000000\n[3,]    0 -2.329318 -0.7329156\n[4,]    2 -1.551663  0.2446879\n[5,]    0 -2.329318 -0.7329156\n[6,]    0  1.746989 -0.2003133\n\n\nwhich is actually identical to our previous solution. This is because I have preserved the order of the solved-out variables, and shows that the swap from a predetermined to a jump variable is completely arbitrary."
  },
  {
    "objectID": "BK.html#substituting-out",
    "href": "BK.html#substituting-out",
    "title": "14  Linear rational expectations models",
    "section": "14.7 Substituting out",
    "text": "14.7 Substituting out\nBut even this doesn’t exhaust the possible re-parametrisations of the model. We can reduce the number of jump variables to 1 and find the same solution. There exist formal methods for reducing models (see King and Watson (2002)) but there is an obvious way to proceed here. From the targeting rule, it must be that \\[\n  y^e_{t+1} = y_t - \\frac{1}{\\mu}\\pi^e_{t+1}\n\\] as the expected shock is zero. This means the IS curve can be rewritten \\[\ny_t = y_t - \\frac{1}{\\mu}\\pi^e_{t+1} - \\frac{1}{\\sigma} \\left (i_t - \\pi_{t+1}^e \\right ) + e_t^1\n\\] implying \\[\ni_t =  \\left (1 - \\frac{\\sigma}{\\mu} \\right )\\pi_{t+1}^e + \\sigma e_t^1\n\\] This is the required interest rate consistent with the targeting rule holding. Now the only jump variable is the inflation rate as we have eliminated the expected output gap. \\[\\begin{align}\ny_t    &= y_{t-1} -\\frac{1}{\\mu} \\pi_t  + \\frac{1}{\\mu} \\varepsilon^3_t \\\\\n\\pi_t  &= \\beta \\pi_{t+1}^e + \\kappa y_t + e_t^2 \\\\\ni_t    &= \\left (1 - \\frac{\\sigma}{\\mu} \\right ) \\pi_{t+1}^e + \\sigma e_t^1 \\\\\ne_t^1  &= \\rho_1 e_{t-1}^1 + \\varepsilon_t^1 \\\\\ne_t^2  &= \\rho_2 e_{t-1}^2 + \\varepsilon_t^2\n\\end{align}\\]\nWe can code this\n\nns &lt;- 5      # Back to 5 states\nnf &lt;- 1      # Now only one jump\nnp &lt;- ns-nf\n\nlabels &lt;- c(\"e^1\",\"e^2\",\"i\",\"y\",\"pi\") # Lose a y\n\nE &lt;- matrix(0,ns,ns)\nA &lt;- E\nB &lt;- matrix(0,ns,ne)\nB[1,1] &lt;- 1\nB[2,2] &lt;- 1\nB[4,3] &lt;- -1\n\ndiag(E[1:4,1:4]) &lt;- 1\ndiag(A[1:2,1:2]) &lt;- c(rho_1, rho_2)\n\nE[3,c(1, 3, 5)]  &lt;- c(-sigma, 1, sigma/mu-1)\n\nA[4,c(4,5)]      &lt;- c(1, -1/mu)\n\nE[5,c(2, 4, 5)]  &lt;- c(1, kappa, beta)\nA[5,5]           &lt;- 1\n\nand solve it using\n\nSs &lt;- solveGenBK(E,A,B,np)\n\n[1] \"Number of unstable roots is 1\"\n\nPs &lt;- Ss$P\nQs &lt;- Ss$Q\n\nCompare the realized of Ps\n\nPs\n\n     [,1]      [,2] [,3]       [,4] [,5]\n[1,]  0.9  0.000000    0  0.0000000    0\n[2,]  0.0  0.800000    0  0.0000000    0\n[3,]  1.8 -1.241330    0 -0.2446879    0\n[4,]  0.0 -1.863455    0  0.7329156    0\n[5,]  0.0  1.397591    0  0.2003133    0\n\n\nwith Po above, say. This is the most ‘efficient’ way of programming the model, in that we have only five states, and indeed the repeated behavioural equations we had before have disappeared in the reduced form solution. Just to confirm this, simulating and plotting this version gives\n\nresponse_plot(impulse_responses(Ps,Qs,Omega,labels,T), \"Optimal, substituted out\")\n\n\n\n\n\n\n\n\nwhich are identical results to those above. But of course \\(E\\) is now invertible so we could solve this using the simplest Blanchard-Kahn variant. Try it!\n\n\n\n\nBlake, Andrew P. 2004. “Analytic Derivatives in Linear Rational Expectations Models.” Computational Economics 24 (1): 77–96.\n\n\nBlanchard, O., and C. Kahn. 1980. “The Solution of Linear Difference Models Under Rational Expectations.” Econometrica 48 (5): 1305–11.\n\n\nKing, Robert G., and Mark W. Watson. 2002. “System Reduction and Solution Algorithms for Singular Linear Difference Systems Under Rational Expectations.” Computational Economics 20 (1–2): 57–86.\n\n\nKlein, Paul. 2000. “Using the Generalized Schur Form to Solve a Multivariate Linear Rational Expectations Model.” Journal of Economic Dynamics and Control 24 (10): 1405–23.\n\n\nPappas, T., A. J. Laub, and N. R. Sandell. 1980. “On the Numerical Solution of the Discrete-Time Algebraic Riccati Equation.” IEEE Transaction on Automatic Control AC-25.4: 631–41.\n\n\nVaughan, D. R. 1970. “A Non Recursive Algorithm Solution for the Discrete Riccati Equation.” IEEE Transactions on Automatic Control AC-15.5: 597–99."
  },
  {
    "objectID": "BK.html#footnotes",
    "href": "BK.html#footnotes",
    "title": "14  Linear rational expectations models",
    "section": "",
    "text": "Sometimes a loop is the right way to do something.↩︎"
  },
  {
    "objectID": "BVAR.html#estimating-bvars-using-us-data",
    "href": "BVAR.html#estimating-bvars-using-us-data",
    "title": "15  BVAR with dummies",
    "section": "15.1 Estimating BVARs using US data",
    "text": "15.1 Estimating BVARs using US data\nWe will the Fed Funds rate, annual GDP growth and annual CPI inflation data from FRED, retrieved 2023-07-13. These are:\n\n\n\n\n\n\n\n\n\nWe will build a variety and two and three variable BVARs. More details on the data are given below."
  },
  {
    "objectID": "BVAR.html#bvars-with-dummy-variable-priors",
    "href": "BVAR.html#bvars-with-dummy-variable-priors",
    "title": "15  BVAR with dummies",
    "section": "15.2 BVARs with dummy variable priors",
    "text": "15.2 BVARs with dummy variable priors\nRather than combine a prior distribution with a likelihood and draw from the resulting joint posterior distribution there is another convenient way of parameterizing the problem. We can instead add some ‘dummy variables’ that have the same properties of the prior so we have a single modified likelihood that incorporates the prior information. This approach was most obviously adopted by Banbura, Giannone, and Reichlin (2010). Further discussion of this can be found in Giannone, Lenza, and Primiceri (2015). In general this is a version of the Theil and Goldberger (1961) mixed estimator given a Bayesian interpretation.\n\n15.2.1 VAR model\nSimple bi-variate two-lag VAR model:\n\\[\\begin{align}\n  \\left[\\begin{matrix}g_t \\cr \\pi_t\\end{matrix}\\right] &=\n  \\left[\\begin{matrix}c_1 \\cr c_2\\end{matrix}\\right] +\n  \\left[\\begin{matrix}b_{11} & b_{12} \\cr\n    b_{21} & b_{22}\\end{matrix}\\right]\n\\left[\\begin{matrix}g_{t-1} \\cr \\pi_{t-1} \\end{matrix}\\right] +\n   \\left[\\begin{matrix}d_{11} & d_{12} \\cr\n    d_{21} & d_{22}\\end{matrix}\\right]\n\\left[\\begin{matrix}g_{t-2} \\cr \\pi_{t-2} \\end{matrix}\\right] +\n  \\left[\\begin{matrix}\\nu_{g,t} \\cr \\nu_{\\pi,t}\\end{matrix}\\right] \\\\\n  \\left[\\begin{matrix}\\nu_{g,t} \\cr \\nu_{\\pi,t}\\end{matrix}\\right] &\\sim N(0, \\Sigma)\n\\end{align}\\]"
  },
  {
    "objectID": "BVAR.html#bvar-hyperparameters",
    "href": "BVAR.html#bvar-hyperparameters",
    "title": "15  BVAR with dummies",
    "section": "15.3 BVAR hyperparameters",
    "text": "15.3 BVAR hyperparameters\nWe will (similarly to the straightforward Minnesota prior) need some control parameters:\n\n\\(\\tau\\) controls the overall tightness of the prior for the AR coefficients\n\\(d\\) controls the prior on higher lags;\n\\(\\lambda\\) controls the prior on constants;\n\\(\\gamma\\) controls the prior on the sum of coefficients;\n\\(\\delta\\) controls the cointegration prior;\n\nwhere\n\n\\(\\sigma_i\\) standard deviation of error terms from individual OLS regressions;\n\\(\\mu_i\\) sample means of the data.\n\n\n15.3.0.1 First lag\nNow consider the following artificial data for the first lag. We construct some dummy observations of the dependent and explanatory variables that look like: \\[\\begin{equation}\n  Y_{D,1} = \\left[\\begin{matrix}\\frac{1}{\\tau}\\sigma_1 & 0 \\cr\n    0 & \\frac{1}{\\tau}\\sigma_2\\end{matrix}\\right]\n\\end{equation}\\]\nand\n\\[\\begin{equation}\n  X_{D,1} = \\left [ \\begin{matrix}0 & \\frac{1}{\\tau}\\sigma_1 & 0 & 0 & 0\\cr\n    0 & 0 & \\frac{1}{\\tau}\\sigma_2 & 0 & 0\\end{matrix}\\right]\n\\end{equation}\\]\nIntuition:\n\\[\\begin{equation}\n  \\left[\\begin{matrix} \\frac{\\sigma_1}{\\tau} & 0 \\cr\n    0 & \\frac{\\sigma_2}{\\tau}\\end{matrix}\\right] =\n  \\left[\\begin{matrix} 0 & \\frac{\\sigma_1}{\\tau} & 0 & 0 & 0\\cr\n   0 & 0 & \\frac{\\sigma_2}{\\tau} & 0 & 0\\end{matrix} \\right]\n\\left[\\begin{matrix}c_1 & c_2 \\cr\n  b_{11} & b_{21} \\cr\n  b_{12} & b_{22} \\cr\n  d_{11} & d_{21} \\cr\n  d_{12} & d_{22}\\end{matrix}\\right]\n+\n  \\left[\\begin{matrix}\\xi_{11} & \\xi_{12} \\cr \\xi_{21} & \\xi_{22} \\end{matrix}\\right]\n\\end{equation}\\]\nMultiplying out we get:\n\\[\\begin{equation}\n  \\left[\\begin{matrix}\\frac{\\sigma_1}{\\tau} & 0 \\cr\n    0 & \\frac{\\sigma_2}{\\tau}\\end{matrix}\\right]\n=\n  \\left[\\begin{matrix}\\frac{\\sigma_1}{\\tau}b_{11} & \\frac{\\sigma_1}{\\tau}b_{21}\\cr\n  \\frac{\\sigma_2}{\\tau}b_{12} &  \\frac{\\sigma_2}{\\tau}b_{22}\\end{matrix} \\right]\n+\n  \\left[\\begin{matrix}\\xi_{11} & \\xi_{12} \\cr \\xi_{21} & \\xi_{22} \\end{matrix}\\right]\n\\end{equation}\\]\nConcentrating on the first row, notice:\n\\[\\begin{equation}\n  \\frac{\\sigma_1}{\\tau} = \\frac{\\sigma_1}{\\tau}b_{11} + \\xi_{11}\n\\end{equation}\\]\nimplying:\n\\[\\begin{equation}\n  b_{11} = 1 - \\frac{\\tau}{\\sigma_1}\\xi_{11}\n\\end{equation}\\]\nso we can write:\n\\[\\begin{equation}\n  b_{11} \\sim N\\left(1, \\frac{\\tau^2var(\\xi_{11})}{\\sigma^2_1}\\right)\n\\end{equation}\\]\nas \\(E[b_{11}] = 1 - \\frac{\\tau}{\\sigma_1}E[\\xi_{11}] = 1\\) and the variance is easily derived. Similarly: \\[\\begin{equation}\n  b_{12} = - \\frac{\\tau}{\\sigma_1}\\xi_{12}\n\\end{equation}\\] which is clearly zero in expectation.\n\n\n15.3.1 Further priors\n\n15.3.1.1 Higher lags\nRather than derive the implications we state the rest of the dummy priors. Consider the following artificial data for the second lag:\n\\[\\begin{align*}\n  Y_{D,2} &= \\left[\\begin{matrix}0 & 0 \\cr 0 & 0\\end{matrix}\\right] \\\\\n  X_{D,2} &= \\left[\\begin{matrix}0 & 0 & 0 & \\frac{\\sigma_1 2^d}{\\tau} & 0 \\cr\n    0 & 0 & 0 & 0 & \\frac{\\sigma_2 2^d}{\\tau} \\end{matrix}\\right]\n\\end{align*}\\]\nWe can multiply these out and check the properties, in particular we can verify in the same way as for the first lag that:\n\\[\\begin{equation}\n  b_{ji} \\sim N\\left(0, \\frac{1}{4}\\frac{\\tau^2var(\\xi_{ji})} {2^d\\sigma^2_j}\\right)\n\\end{equation}\\]\nfor \\(j=1,...N\\), \\(i=1,...l\\).\n\n\n15.3.1.2 Constant\nConsider the following artificial data for the constant:\n\\[\\begin{align*}\n  Y_{D,3} &= \\left[\\begin{matrix}0 & 0 \\end{matrix}\\right] \\\\\n  X_{D,3} &= \\left[\\begin{matrix}\\lambda & 0 & 0 & 0 & 0 \\end{matrix}\\right]\n\\end{align*}\\]\nso \\(\\lambda c_1 = \\varepsilon_1\\) and \\(\\lambda c_2 = \\varepsilon_2\\). As \\(\\lambda \\rightarrow \\infty\\) the prior is implemented more tightly.\n\n\n15.3.1.3 Covariances\nDummy observations to implement the prior on the error covariance matrix are: \\[\\begin{align*}\n  Y_{D,4} &= \\left[\\begin{matrix}\\sigma_1 & 0 \\cr 0 & \\sigma_2\\end{matrix}\\right] \\\\\n  X_{D,4} &= \\left[\\begin{matrix}0 & 0 & 0 & 0 & 0 \\cr\n    0 & 0 & 0 & 0 & 0 \\end{matrix}\\right]\n\\end{align*}\\]\nwith the magnitude of the diagonal elements of \\(\\Sigma\\) controlled by the scale of the diagonal elements of \\(Y_{D,4}\\), as larger diagonal elements implement the prior belief that the variance of \\(\\nu_1\\) and \\(\\nu_2\\) is larger.\nBanbura, Giannone, and Reichlin (2010) stop here, but there are additional priors that could be added.\n\n\n15.3.1.4 Sum of coefficients\nWe could add a prior that reflects the belief that the sum of coefficients on ‘own’ lags add up to 1. This is an additional ‘unit root’-style prior. Consider:\n\\[\\begin{align*}\n  Y_{D,5} &= \\left[\\begin{matrix} \\gamma\\mu_1 & 0\\cr 0 & \\gamma\\mu_2\\end{matrix}\\right] \\\\\n  X_{D,5} &= \\left [ \\begin{matrix} 0 & \\gamma\\mu_1 & 0 & \\gamma\\mu_1 & 0\\cr\n    0 & 0 & \\gamma\\mu_2 & 0 & \\gamma\\mu_2\\end{matrix}\\right]\n\\end{align*}\\]\nwhere \\(\\mu_1\\) is the sample mean of \\(y_t\\) and \\(\\mu_2\\) is the sample mean of \\(x_t\\). Note that these dummy observations imply prior means of the form \\(b_{ii} + d_{ii} = 1\\) where \\(i = 1, 2\\) and \\(\\gamma\\) controls the tightness of the prior. As \\(\\gamma \\rightarrow \\infty\\) the prior is implemented more tightly. Forecast growth rates eventually converge to their sample averages.\n\n\n15.3.1.5 Trends\nWe can also specify common stochastic trend dummies:\n\\[\\begin{align*}\n  Y_{D,6} &= \\left[\\begin{matrix}\\delta\\mu_1 & \\delta\\mu_2 \\end{matrix}\\right] \\\\\n  X_{D,6} &= \\left [\\begin{matrix}\\delta & \\delta\\mu_1 & \\delta\\mu_2 & \\delta\\mu_1 & \\delta\\mu_2 \\end{matrix}\\right]\n\\end{align*}\\]\nwhere this imposes that the coefficients are consistent with limiting the amount of drift between the predictions at their average values.\n\n\n\n15.3.2 Implementation\nThe data and the artificial data are now stacked:\n\\[\\begin{equation}\n  Y^* = \\left[\\begin{matrix} g_3 & \\pi_3 \\cr\n    \\vdots & \\vdots \\cr\n    g_T & \\pi_T \\cr\n    \\frac{1}{\\tau}\\sigma_1 & 0 \\cr\n    0 & \\frac{1}{\\tau}\\sigma_2\\cr\n    0 & 0 \\cr\n    0 & 0 \\cr\n    0 & 0 \\cr\n    \\sigma_1 & 0 \\cr\n    0 & \\sigma_2 \\cr\n    \\gamma\\mu_1 & 0 \\cr\n    0 & \\gamma\\mu_2 \\cr\n    \\delta\\mu_1 & \\delta\\mu_2 \\end{matrix}\\right], \\quad\nX^* = \\left [ \\begin{matrix}1 & g_2 & \\pi_2 & g_1 & \\pi_1 \\cr\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\cr\n  1 & g_{T-1} & \\pi_{T-1} & g_{T-2} & \\pi_{T-2} \\cr\n  0 & \\frac{1}{\\tau}\\sigma_1 & 0 & 0 & 0\\cr\n  0 & 0 & \\frac{1}{\\tau}\\sigma_2 & 0 & 0\\cr\n  0 & 0 & 0 & \\frac{\\sigma_1 2^d}{\\tau} & 0 \\cr\n  0 & 0 & 0 & 0 & \\frac{\\sigma_2 2^d}{\\tau} \\cr\n  \\lambda & 0 & 0 & 0 & 0 \\cr\n  0 & 0 & 0 & 0 & 0 \\cr\n  0 & 0 & 0 & 0 & 0 \\cr\n  0 & \\gamma\\mu_1 & 0 & \\gamma\\mu_1 & 0 \\cr\n  0 & 0 & \\gamma\\mu_2 & 0 & \\gamma\\mu_2 \\cr\n  \\delta & \\delta\\mu_1 & \\delta\\mu_2 & \\delta\\mu_1 & \\delta\\mu_2 \\end{matrix}\\right]\n\\end{equation}\\]\nEstimation via Gibbs sampling now proceeds in a very straightforward way. There is no need to draw for the prior separately."
  },
  {
    "objectID": "BVAR.html#examples",
    "href": "BVAR.html#examples",
    "title": "15  BVAR with dummies",
    "section": "15.4 Examples",
    "text": "15.4 Examples\nFirst we use quarterly US Growth (FRED series A191RO1Q156NBEA) and CPI (FRED series CPALTT01USQ661S) expressed as the annual inflation rate from 1961-01-01 to 2023-01-01 in a bi-variate BVAR. The last ten observations are:\n\n\n\n\n\nDate\nGrowth\nInflation\n\n\n\n\n2020-10-01\n-1.5\n1.224176\n\n\n2021-01-01\n1.2\n1.905310\n\n\n2021-04-01\n12.5\n4.776278\n\n\n2021-07-01\n5.0\n5.264633\n\n\n2021-10-01\n5.7\n6.765892\n\n\n2022-01-01\n3.7\n8.023109\n\n\n2022-04-01\n1.8\n8.556077\n\n\n2022-07-01\n1.9\n8.284860\n\n\n2022-10-01\n0.9\n7.110821\n\n\n2023-01-01\n1.8\n5.769521\n\n\n\n\n\nWe specify a VAR with two lags, and use it to forecast 12 periods ahead. The BVAR are specified using the names above, with only tau particularly binding in this case. We set the total number of iterations in each case to 20000 and discard the first half. The parameter nb is used to set how much back data should appear in a fan chart.\n\n#########\n# Options\n#########\nnf &lt;- 12 # Max forecast horizon\nnb &lt;- 21 # No. back periods plotted in graphs\nl  &lt;- 2  # Number of lags in VAR\n\n# specify parameters of the Minnesota-type prior\ntau    &lt;- .1   # controls prior on own 1st lags (1 makes wibbly)\nd      &lt;- 1    # decay for higher lags\nlambda &lt;- 1    # prior for the constant\ngamma  &lt;- 1    # sum of coefficients unit roots\ndelta  &lt;- 1    # cointegration prior\n\n# Gibbs control\nreps &lt;- 20000 # total numbers of Gibbs iterations\nburn &lt;- 10000 # number of burn-in iterations\n\nIn what follows we vary tau and the lag length to illustrate their effects. To do this we create the augmented data and then run the Gibbs sampler, using:\n\n# Create augmented data\nYplus &lt;- augmentData(Y, l, tau, d, lambda, gamma, delta)\n\n# Run Gibbs sampler\nout   &lt;- Gibbs_estimate(Yplus[[1]], Yplus[[2]], reps, burn, 1, nf)\n\nwhere Y contains the data in a dataframe/tibble with the date in the first column as in the data example above. The code strips out the date and then uses the remaining \\(N\\) columns in the BVAR. See the Code Appendix for the details of the functions.\nThe output contains any forecast draws from the Gibbs sampler in the third list element from the Gibbs_estimate() function. The first two elements are coefficient draws. Two further functions plots the fan charts using the Gibbs draws:\n\n# String to put in subtitle\ncontrols &lt;- paste0(\"Lag length \", l, \": tau=\", tau, \", d=\", d,\n                   \", lambda=\", lambda, \", gamma=\", gamma, \", delta=\", delta)\n\n# Plots\nfan_chart(Y, out[[3]], controls, nb)\np           &lt;- coeff_plot(Y, l, out[[1]], out[[2]], 333, controls)\npnum        &lt;- pnum+1\npce[[pnum]] &lt;- p[[1]]\n\nwhere the string controls is put in the chart subtitle and the coefficient densities. It can be anything but is a good place to remind yourself of how you specified the model. Notice we save the coefficient plots for later use.\n\n15.4.1 Example 1: BVAR(2) with \\(\\tau=0.1\\)\n\n\n\n\n\n\n\n\n\n\n\n15.4.2 Example 2: BVAR(2) with \\(\\tau=1\\)\n\n\n\n\n\n\n\n\n\n\n\n15.4.3 Example 3: BVAR(6) with \\(\\tau=0.1\\)\n\n\n\n\n\n\n\n\n\n\n\n15.4.4 Example 4: BVAR(6) with \\(\\tau=1\\)\n\n\n\n\n\n\n\n\n\n\n\n15.4.5 Coefficient estimates\nAll of these have underlying parameters. Their estimated posterior densities are:"
  },
  {
    "objectID": "BVAR.html#tri-variate-bvar",
    "href": "BVAR.html#tri-variate-bvar",
    "title": "15  BVAR with dummies",
    "section": "15.5 Tri-variate BVAR",
    "text": "15.5 Tri-variate BVAR\nNow we add the FedFunds rate (FRED series FEDFUNDS), so the last ten periods of the data set is now:\n\n\n\n\n\nDate\nGrowth\nInflation\nFedFunds\n\n\n\n\n2020-10-01\n-1.5\n1.224176\n0.09\n\n\n2021-01-01\n1.2\n1.905310\n0.09\n\n\n2021-04-01\n12.5\n4.776278\n0.07\n\n\n2021-07-01\n5.0\n5.264633\n0.10\n\n\n2021-10-01\n5.7\n6.765892\n0.08\n\n\n2022-01-01\n3.7\n8.023109\n0.08\n\n\n2022-04-01\n1.8\n8.556077\n0.33\n\n\n2022-07-01\n1.9\n8.284860\n1.68\n\n\n2022-10-01\n0.9\n7.110821\n3.08\n\n\n2023-01-01\n1.8\n5.769521\n4.33\n\n\n\n\n\nTwo more examples follow.\n\n15.5.1 Example 5: BVAR(4) with \\(\\tau=.1\\), 3 variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.5.2 Example 6: BVAR(6) with \\(\\tau=1\\), 3 variables"
  },
  {
    "objectID": "BVAR.html#code-appendix",
    "href": "BVAR.html#code-appendix",
    "title": "15  BVAR with dummies",
    "section": "15.6 Code appendix",
    "text": "15.6 Code appendix\nYou can download the program and functions used for the estimates above from the links below. Put them in the same directory and they should recreate exactly (within sampling error) the same graphs as above. Ensure you have all the libraries available that are loaded at the top of BVARdum.R.\nMain program:\n\n\nDownload BVARdum.R\n\n\nFunctions:\n\n\nDownload BVARdumFUNCs.R\n\n\n\n\n\n\nBanbura, M., D. Giannone, and L. Reichlin. 2010. “Large Bayesian Vector Auto Regressions.” Journal of Applied Econometrics 25 (1): 71–92.\n\n\nGiannone, Domenico, Michele Lenza, and Giorgio E. Primiceri. 2015. “Prior Selection for Vector Autoregressions.” The Review of Economics and Statistics 97 (2): 436–51.\n\n\nTheil, Henri, and Arthur S. Goldberger. 1961. “On Pure and Mixed Statistical Estimation in Economics.” International Economic Review 2: 317–32."
  },
  {
    "objectID": "Dynare.html#what-is-dynare",
    "href": "Dynare.html#what-is-dynare",
    "title": "14  Dynare basics",
    "section": "14.1 What is Dynare?",
    "text": "14.1 What is Dynare?\nBest answered by the authors themselves, from their website:\n\nDynare … handl[es] a wide class of economic models, in particular dynamic stochastic general equilibrium (DSGE) and overlapping generations (OLG) models. The models solved by Dynare include those relying on the rational expectations hypothesis … [but also] models where expectations are formed differently [including] models where agents have limited rationality or imperfect knowledge of the state of the economy and, hence, form their expectations through a learning process.\n\n\nDynare offers a user-friendly and intuitive way of describing […] models. It is able to perform simulations of the model given a calibration of the model parameters and is also able to estimate these parameters given a dataset. In practice, the user will write a text file containing the list of model variables, the dynamic equations linking these variables together, the computing tasks to be performed and the desired graphical or numerical outputs."
  },
  {
    "objectID": "Dynare.html#software-for-solving-models",
    "href": "Dynare.html#software-for-solving-models",
    "title": "14  Dynare basics",
    "section": "14.2 Software for solving models",
    "text": "14.2 Software for solving models\nWhy do we need it? Any serious model will usually require a computer package to, say, analyse policy options. Lots of Matlab code (and other languages) is available to do this but it is convenient to use one of a few packages that makes the tedious things easier to do. It is also nice to be able to code up a model very quickly; we may need to marry that up with data too. Dynare is one such ‘user friendly’ program to we can use to solve, simulate and estimate DSGE models, and other stuff besides. Dynare is a Matlab program, so you need a licenced product1. It runs on a variety of OS/hardware platforms too, including Debian Linux. It is available free from http://www.dynare.org/ and there is extensive documentation.\nAlthough Dynare is frequently the tool of choice (particularly by researchers) there are alternatives: IRIS, YADA etc are also available for Matlab, with gEcon for R, plus a number of commercial alternatives.\n\n\n\n\n\n\nNotable characteristics of Dynare include:\n\n\n\n\nTheir definition of user friendly isn’t the same as mine.\nThe terminology that they use is at times a little strange; this partly reflects the history of the program rather than the methods being used.\nData handling is somewhat ad hoc.\nThere are a lot of embedded methods (Kalman filter, MCMC, etc) and it is not always clear to the casual user how (or why) they are being implemented.\nI wouldn’t recommend Dynare as a forecast platform although you could use it. In my experience IRIS (for one) is better but the learning curve is steeper."
  },
  {
    "objectID": "Dynare.html#dynare-model-files",
    "href": "Dynare.html#dynare-model-files",
    "title": "14  Dynare basics",
    "section": "14.3 Dynare model files",
    "text": "14.3 Dynare model files\nWe begin with getting a representative DSGE model into Dynare, then solve and simulate it. Each model in Dynare file requires a fairly precisely structured text file to control both the specification of the model (and any data) and what we wish to do with it.\n\n14.3.1 File structure\nThe Dynare files have the extension .mod so we will often call them mod files. The structure of a mod file has to be as follows :\n\nPreamble\nModel\nSteady-state/initial values\nShocks\nComputation\n\nMany of these are specified as blocks; they have a begin/end form:\n &lt; Begin block &gt;;\n\n    &lt; commands &gt;;\n\n end;"
  },
  {
    "objectID": "Dynare.html#example-model",
    "href": "Dynare.html#example-model",
    "title": "14  Dynare basics",
    "section": "14.4 Example model",
    "text": "14.4 Example model\nWe will code the canonical closed economy New Keynesian model with three variables: Output gap, \\(y_t\\), inflation deviations from target, \\(\\pi_t\\), and the nominal interest rate, \\(i_t\\). Model equations are: \\[\\begin{eqnarray}\ny_t   &=& y_{t+1}-\\frac{1}{\\sigma} (i_t - \\pi^e_{t+1}) \\\\\n\\pi_t &=& \\beta \\pi^e_{t+1}+\\kappa y_t + \\varepsilon_t \\\\\ni_t   &=& \\theta_y y_t + \\theta_\\pi \\pi_t\n\\end{eqnarray}\\] comprising\n\na dynamic IS curve;\na New Keynesian Phillips curve w/cost-push shock;\na standard Taylor-type rule\n\nand \\(x^e_{t+1}\\) is shorthand for \\(E_t x_{t+1}\\). To meaningfully simulate this we will need to specify some values for the parameters (\\(\\beta\\), \\(\\kappa\\), \\(\\sigma\\), \\(\\theta_\\pi\\), \\(\\theta_y\\), \\(\\hbox{var}(\\varepsilon_t)\\)), some sort of data and probably some shocks.\n\n14.4.1 Dynare code\n\n14.4.1.1 Preamble\nDefine the variables and parameters of the model, and set numerical values for any parameters. All endogenous variables are declared as of type var and shocks as varexo. All the parameters are declared with the command parameters.\nFor the example model:\nvar y, i, pi;\nvarexo eps;\n\nparameters beta, kappa, sigma, theta_y, theta_pi;\n\nbeta = 0.99;\nkappa = 0.05;\nsigma = 2;\ntheta_y = 0.5;\ntheta_pi = 1.5;\n\n\n14.4.1.2 Model\nWhatever the model, linearize each equation (although Dynare is able to solve the model from its original form) and enter them into a mod file. We need to follow some simple rules so \\(x^e_{t+1}\\) is entered as x(+1) or \\(x_{t-1}\\) as x(-1). We don’t specify the conditioning date as they are always the same.\nFor a linear model the block usually starts with model(linear); and conclude with end;. In between we enter all the (linearized) model equations. This is the big advantage: Dynare doesn’t need you to input the model matrices as a pre-processor does this for you.\nOne way of entering the model above is:\nmodel(linear);\n\n    y = y(+1) - (1/sigma)*(i-pi(+1));\n    pi = beta*pi(+1) + kappa*y + eps;\n    i = theta_y*y + theta_pi*pi;\n\nend;\nEquation order doesn’t matter, nor does normalization.\n\n\n14.4.1.3 Data/steady state\nAll models need data, but some can get away with zero (but not missing). In particular linear models specified as deviations from steady state are, but few others. Most models won’t fit any arbitrary data set – there will always be some implicit residual. How we deal with these is important for forecasting, so we will want to supply some historical data and calculate the implicit residual which can be done.\nFor now, assume a linear model. Of course linearity doesn’t mean zero is the baseline around which we should simulate a model by applying arbitrary shocks, but ours is. We can either enter the exact steady-state values in the .mod file or leave Dynare to find the steady-state values through numerical methods. In either case, some initial values are entered with the command initval; followed by the numerical values for each variables and then end;. By using the command steady; Dynare uses the steady state of the model rather than the values you just entered which become the approximate steady state.\nFor our example we could use:\ninitval;\n\n    y  = 0;\n    pi = 0;\n    i  = 0;\n\nend;\nand then\nsteady;\nThis is a belt-and-braces approach; we’ve given it both the exact steady state and made sure it solve for it too. Beware: Dynare isn’t very good at finding arbitrary steady states without helpful starting values.\n\n\n14.4.1.4 Shocks\nShocks are entered using the command shocks; followed by the variance of each shocks which is declared as var.2 Once all the shocks are declared we conclude with the command end;.\nFor our example, the single shock is declared as:\nshocks;\n\n    var eps = 0.001;\n\nend;\nOften in Dynare things can be done in several different ways. An alternative formulation is:\nshocks;\n\n    var eps;\n    stderr 0.001;\n\nend;\n\n\n14.4.1.5 Computation\nOnce we have defined preamble, the model, the steady state and the shocks we can then simulate the model. To simulate a model we usually enter the command stoch_simul perhaps with some options that will tell Dynare what to do with the model. (Why stoch_simul you may ask: what’s wrong with calling it something like simul?) Dynare produces outputs such as a model summary, eigenvalues, the covariance of exogenous shocks, the solved transition function, model moments, correlations and autocorrelations of the simulated variables as well as graphs.\nA common option is to set the number of periods for the impulse responses, for example IRF=30, which computes impulse response functions for 30 periods.\nSo we would specify\nstoch_simul(IRF=30);\nOther additional options will be necessary if we have a lot of shocks or endogenous variables and so on."
  },
  {
    "objectID": "Dynare.html#exercises",
    "href": "Dynare.html#exercises",
    "title": "14  Dynare basics",
    "section": "14.5 Exercises",
    "text": "14.5 Exercises\nLet’s code this up and look at the simulations. (Hint: copy/paste.)\nThey’re going to look pretty boring. To be more interesting we can do a couple of things:\n\nMore shock persistence, so \\(\\varepsilon_t = \\rho\\varepsilon_{t-1} + \\epsilon_t\\).\nInterest rate persistence, so \\(i_t=\\gamma i_{t-1}+(1-\\gamma ) (\\theta_y y_t + \\theta_{\\pi}\\pi_t)\\)\n\nCode these up following the rules above (how do we define \\(\\varepsilon\\) now?) and check you get the same answers if \\(\\rho =\\gamma =0\\). Set \\(\\rho =\\gamma =0.8\\) and see what happens."
  },
  {
    "objectID": "Dynare.html#aside-on-estimation",
    "href": "Dynare.html#aside-on-estimation",
    "title": "14  Dynare basics",
    "section": "14.6 Aside on estimation",
    "text": "14.6 Aside on estimation\nFor estimation we need additional information. Sometimes we need to introduce measurement equations specifically, unless straight state variables. Smets/Wouters model, for example, uses\n// measurement equations\n\npinfobs = pinf + constepinf;\nso observed inflation is related to the state variable. We also need to declare the estimated parameters, which we do together with their priors. Example again from Smets/Wouters:\nestimated_params;\n\n    // Param Name, init, lb, ub, Prior, P1, P2\n    // Priors: &lt;P&gt;_PDF, &lt;P&gt; = BETA, GAMMA, etc}\n\n    stderr ea, 0.4, 0.01, 3, INV_GAMMA_PDF, 0.1, 0.2;\n    cthetaa, .9, .01, .9999, BETA_PDF, 0.5, 0.20;\n    csadjcost, 6.3325, 2, 15, NORMAL_PDF, 4, 1.5;\n    ...\n\nend;\nObserved variables are attached to states using:\nvarobs dy dc dinve labobs pinfobs dw robs;\nEssentially just another declaration, can be put anywhere before estimation."
  },
  {
    "objectID": "Dynare.html#footnotes",
    "href": "Dynare.html#footnotes",
    "title": "14  Dynare basics",
    "section": "",
    "text": "Actually Octave will do, but is much slower.↩︎\nNot to be confused with var above. I did say it was strange…↩︎"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "15  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever…"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Anderson, T. W. 2004. An Introduction to Multivariate Statistical\nAnalysis. 3rd ed. New York: John Wiley; Sons.\n\n\nBahaj, Saleem, Angus Foulis, and Gabor Pinter. 2020. “Home Values\nand Firm Behavior.” American Economic Review 110 (7):\n2225–70. https://doi.org/10.1257/aer.20180649.\n\n\nBanbura, M., D. Giannone, and L. Reichlin. 2010. “Large\nBayesian Vector Auto Regressions.” Journal of\nApplied Econometrics 25 (1): 71–92.\n\n\nBlake, Andrew P. 2004. “Analytic Derivatives in Linear Rational\nExpectations Models.” Computational Economics 24 (1):\n77–96.\n\n\nBlake, Andrew P, and Haroon Mumtaz. 2017. Applied Bayesian\nEconometrics for Central Bankers. Revised. Technical Books. Centre\nfor Central Banking Studies, Bank of England. https://www.bankofengland.co.uk/-/media/boe/files/ccbs/resources/applied-bayesian-econometrics-for-central-bankers-updated-2017.pdf.\n\n\nBlanchard, O., and C. Kahn. 1980. “The Solution of Linear\nDifference Models Under Rational Expectations.”\nEconometrica 48 (5): 1305–11.\n\n\nBoehmke, Brad, and Brandon M. Greenwell. 2019. Hands-on Machine\nLearning with R. The R Series. Boca\nRaton: Chapman & Hall/CRC. https://bradleyboehmke.github.io/HOML/.\n\n\nCarter, C. K., and R. Kohn. 1994. “On\nGibbs sampling for state space models.”\nBiometrika 81 (3): 541–53. https://doi.org/10.1093/biomet/81.3.541.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. New\nHaven & London: Yale University Press. https://mixtape.scunning.com/.\n\n\nDurbin, J., and S. J. Koopman. 2001. Time Series Analysis by State\nSpace Methods. Oxford: Oxford University Press.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2019. Regression and\nOther Stories. Cambridge: Cambridge University Press. http://www.stat.columbia.edu/~gelman/regression.\n\n\nGiannone, Domenico, Michele Lenza, and Giorgio E. Primiceri. 2015.\n“Prior Selection for Vector\nAutoregressions.” The Review of Economics and\nStatistics 97 (2): 436–51.\n\n\nGreene, William H. 1997. Econometric Analysis. Third. McGraw\nHill.\n\n\nHamilton, J. D. 1994. Time Series Analysis. Princeton, NJ:\nPrinceton University Press.\n\n\nHamilton, James D. 1994. Time Series Analysis. Princeton, NJ:\nPrinceton University Press.\n\n\nHarvey, Andrew C. 1989. Forecasting, Structural Time Series Models\nand the Kalman Filter. Cambridge: Cambridge University Press.\n\n\nHarvey, Andrew C., and Richard G. Pierse. 1984. “Estimating\nMissing Observations in Economic Time Series.” Journal of the\nAmerican Statistical Association 79 (385): 125–31.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The\nElements of Statistical Learning: Data Mining, Inference, and\nPrediction. 2nd ed. New York, NY: Springer. https://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning\nfor Text Analysis in R. Chapman & Hall: CRC Press.\nhttps://smltar.com/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical Learning: With Applications in\nR. 2nd ed. Springer Texts in Statistics. New York, NY:\nSpringer. https://www.statlearning.com/.\n\n\nJazwinski, Andrew H. 1970. Stochastic Processes and Filtering\nTheory. Mineola, N.Y.: Dover Publications Inc.\n\n\nKalman, R. E. 1960. “A New Approach to Linear Filtering and\nPrediction Problems.” Transactions of the ASME Journal of\nBasic Engineering 82 (Series D): 35–45.\n\n\nKim, Chang-Jin, and Charles R. Nelson. 1999. State-Space Models with\nRegime Switching: Classical and Gibbs-Sampling Approaches with\nApplications. MIT Press.\n\n\nKing, Robert G., and Mark W. Watson. 2002. “System Reduction and\nSolution Algorithms for Singular Linear Difference Systems Under\nRational Expectations.” Computational Economics 20\n(1–2): 57–86.\n\n\nKlein, Paul. 2000. “Using the Generalized Schur Form\nto Solve a Multivariate Linear Rational Expectations Model.”\nJournal of Economic Dynamics and Control 24 (10): 1405–23.\n\n\nKlugman, Stuart A., Harry H. Panjer, and Gordon E. Willmot. 2008.\nLoss Models: From Data to Decisions. Third. Hoboken, N.J.: John\nWiley & Sons, Inc.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019.\nGeocomputation with R. 1st ed. The R\nSeries. Boca Raton: Chapman & Hall/CRC. https://geocompr.robinlovelace.net/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in R and Stan. 2nd ed. Abingdon,\nOxfordshire: CRC Press. https://github.com/rmcelreath/rethinking.\n\n\nPappas, T., A. J. Laub, and N. R. Sandell. 1980. “On the Numerical\nSolution of the Discrete-Time Algebraic Riccati Equation.”\nIEEE Transaction on Automatic Control AC-25.4: 631–41.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal\nInference in Statistics: A Primer. Chichester: John Wiley &\nSons. http://bayes.cs.ucla.edu/PRIMER/.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with\nR: A Tidy Approach. O’Reilly. https://www.tidytextmining.com/.\n\n\nTaddy, Matt. 2019. Business Data Science: Combining Machine Learning\nand Economics to Optimize, Automate, and Accelerate Business\nDecisions. New York, NY: McGraw-Hill Education. https://github.com/TaddyLab/BDS.\n\n\nTheil, Henri, and Arthur S. Goldberger. 1961. “On Pure and Mixed\nStatistical Estimation in Economics.” International Economic\nReview 2: 317–32.\n\n\nTriantafyllopoulos, Kostas. 2021. Bayesian Inference of State Space\nModels: Kalman Filtering and Beyond. Springer Texts in Statistics.\nCham, Switzerland: Springer.\n\n\nVaughan, D. R. 1970. “A Non Recursive Algorithm Solution for the\nDiscrete Riccati Equation.” IEEE Transactions on Automatic\nControl AC-15.5: 597–99.\n\n\nWilkinson, Leland. 2013. The Grammar of Graphics. 2nd ed. New\nYork, NY: Springer-Verlag.\n\n\nYong, Laurel Harbridge, Jon A. Krosnick, and Jeffrey M. Wooldridge.\n2016. “Presidential Approval and Gas Prices: Sociotropic or\nPocketbook Influence?” In Political Psychology, edited\nby Jon A. Krosnick, I-Chant A. Chiang, and Tobias H. Stark, 246–75.\nTaylor; Francis Inc."
  },
  {
    "objectID": "Appendix1.html#plotting-in-the-tidyverse",
    "href": "Appendix1.html#plotting-in-the-tidyverse",
    "title": "Appendix A — Basic ggplot2",
    "section": "A.1 Plotting in the tidyverse",
    "text": "A.1 Plotting in the tidyverse\nggplot2 forms a key part of the tidyverse – for many the only part. It builds on the grammar of graphics proposed by the late Leland Wilkinson, Wilkinson (2013). In essence it provides rules for how graphics should be treated, simple rules that drive you mad until you get it.\nThe process for building a graph is something like the following.\n\nInitiate a plot using ggplot.\nSpecify aesthetics which indicate what you want to plot from some data set.\nCall a geom (or an alternative) to say how you want to plot it.\nAdd modifiers to change how it looks.\n\nThe order of operations is essentially always this, although quite how the ordering is apllied differs subtly, which we will show here."
  },
  {
    "objectID": "Appendix1.html#example",
    "href": "Appendix1.html#example",
    "title": "Appendix A — Basic ggplot2",
    "section": "A.2 Example",
    "text": "A.2 Example\nTo illustrate, we take the wooldridge data set approval from Yong, Krosnick, and Wooldridge (2016), do a little wrangling and (eventually) produce some quite nice plots. Start with the libraries and retrieve data the data.\n\nlibrary(tidyverse)\nlibrary(wooldridge)\ndata(\"approval\")\n\nThe first few columns and rows of this looks like:\n\n\n   id month year   sp500   cpi cpifood approve\n1 302     2 2001 1239.94 184.4   171.8   59.24\n2 303     3 2001 1160.33 185.3   172.2   57.01\n3 304     4 2001 1249.46 185.6   172.4   60.31\n4 305     5 2001 1255.82 185.5   172.9   55.82\n5 306     6 2001 1224.42 185.9   173.4   54.93\n6 307     7 2001 1211.23 186.2   174.0   56.36\n\n\nand all the available variables are\n\nnames(approval)\n\n [1] \"id\"         \"month\"      \"year\"       \"sp500\"      \"cpi\"       \n [6] \"cpifood\"    \"approve\"    \"gasprice\"   \"unemploy\"   \"katrina\"   \n[11] \"rgasprice\"  \"lrgasprice\" \"X11.Sep\"    \"iraqinvade\" \"lsp500\"    \n[16] \"lcpifood\"  \n\n\nTypically we want to investigate trends and correlations and graphing pairs or more of series is a good way to begin.\n\nA.2.1 Scatter plot\nA first scatter plot, using geom_point of food against gas (petrol) prices\n\nggplot(approval, aes(x=lcpifood, y=lrgasprice)) +    # Initiate, set aesthetics\n  geom_point()                                       # Display as points\n\n\n\n\nOK, I guess, but a bit dull – so add some colour. This time, aes is specified in the geom – either is fine, but there are some advantages either way which we will see shortly.\n\nggplot(approval) +\n  geom_point(aes(x=lcpifood, y=lrgasprice, color=month))  # Colours by month\n\n\n\n\nBetter, but how about…\n\nggplot(approval) +\n  geom_point(aes(x=lcpifood, y=lrgasprice, color=approve), size=2, shape=17) + # Colours by popularity!\n  scale_color_gradient(low=\"red\", high=\"green\") \n\n\n\n\nwhere the colours are a gradient we specify. But months can only be one of twelve categories, so a categorical variable (a factor) is needed to get different actual colours, otherwise for a continuous variable I get shades of one colour or a continuous change we need to specify.\nLets do this – and add a different aesthetic, size, for year.\n\nggplot(approval) +\n  geom_point(aes(x=lcpifood, y=lrgasprice, color=as.factor(month), size=as.factor(year)))\n\nWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\nNote there is now a lot going n, and maybe too much. ggplot thinks so!\n\n\nA.2.2 Time series plots\nOur time index is a bit odd as the data set has year and month separately. Create a proper date series using:\n\napproval %&lt;&gt;% \n  unite(date, year, month, sep=\"/\") %&gt;% \n  mutate(date = as.Date(paste0(date,\"/01\"), \"%Y/%m/%d\"))\n\nI’ve used the %&lt;&gt;% pipe operator to send and get back approval so this is now\n\n\n   id       date   sp500   cpi cpifood approve gasprice unemploy katrina\n1 302 2001-02-01 1239.94 184.4   171.8   59.24    148.4      4.6       0\n2 303 2001-03-01 1160.33 185.3   172.2   57.01    144.7      4.5       0\n3 304 2001-04-01 1249.46 185.6   172.4   60.31    156.4      4.2       0\n4 305 2001-05-01 1255.82 185.5   172.9   55.82    172.9      4.1       0\n5 306 2001-06-01 1224.42 185.9   173.4   54.93    164.0      4.7       0\n6 307 2001-07-01 1211.23 186.2   174.0   56.36    148.2      4.7       0\n  rgasprice lrgasprice X11.Sep iraqinvade   lsp500 lcpifood\n1  80.47723   4.387974       0          0 7.122818 5.146331\n2  78.08958   4.357857       0          0 7.056460 5.148656\n3  84.26724   4.433993       0          0 7.130467 5.149817\n4  93.20755   4.534829       0          0 7.135544 5.152713\n5  88.21947   4.479828       0          0 7.110222 5.155601\n6  79.59184   4.376912       0          0 7.099391 5.159055\n\n\nThen I can plot a couple of series using two calls to geom_line\n\nggplot(approval) +\n  geom_line(aes(x=date, y=unemploy), colour=\"red\") +\n  geom_line(aes(x=date, y=cpi), colour=\"blue\") \n\n\n\n\nBut this is pretty inefficient, as I would need a call to geom_line for every series I wanted to plot and even then scales are unsuitable. Plus the labels are not right.\nThis is where things really get interesting. I pivot_longer all the variables into a single column.\n\ndf &lt;- pivot_longer(approval, cols=-c(date, id), names_to= \"Var\", values_to = \"Val\")\nhead(df)\n\n# A tibble: 6 × 4\n     id date       Var         Val\n  &lt;int&gt; &lt;date&gt;     &lt;chr&gt;     &lt;dbl&gt;\n1   302 2001-02-01 sp500    1240. \n2   302 2001-02-01 cpi       184. \n3   302 2001-02-01 cpifood   172. \n4   302 2001-02-01 approve    59.2\n5   302 2001-02-01 gasprice  148. \n6   302 2001-02-01 unemploy    4.6\n\n\nGreat! Now I can plot Val using one call to geom_line. This time, put the graph object into p and then explicitly plot it.\n\np  &lt;- ggplot(df) +\n  geom_line(aes(x=date, y=Val))\nplot(p)\n\n\n\n\nOops! I need to tell ggplot2 to separate out the variables which are stored in Var. For this, use group:\n\np  &lt;- ggplot(df) +\n  geom_line(aes(x=date, y=Val, group=Var))\nplot(p)\n\n\n\n\nBut this could better be done by using an aesthetic like colour which implies group\n\np  &lt;- ggplot(df) +\n  geom_line(aes(x=date, y=Val, colour=Var))\nplot(p)\n\n\n\n\nOK, but can I plot them so we can see what’s going on, like in a grid? This is where facet comes in.\n\np  &lt;- p +\n  facet_wrap(~Var, scales = \"free\")\nplot(p)\n\n\n\n\nA bit more formatting…\n\np  &lt;- p +\n  theme_minimal() + \n  labs(title=\"Facet plots\", x=\"\", y=\"\")\nplot(p)\n\n\n\n\nFinally all in one go, dropping the dummies, don’t store as an object. Also no legend, as series labelled in the facets. And I call a rather handy little function geom_smooth which fits (by default) a Loess smoothing line.\n\napproval %&gt;% \n  select(-iraqinvade, -katrina, -X11.Sep) %&gt;%\n  pivot_longer(cols=-c(date, id), names_to=\"Var\", values_to=\"Val\") %&gt;%\n  ggplot(aes(x=date, y=Val, group=Var, colour=Var)) +\n  geom_line() +\n  geom_smooth() + # Smoother\n  facet_wrap(~Var, scales = \"free\") +\n  theme_minimal() + \n  theme(legend.position = \"none\") +\n  labs(title=\"Facet plots\", x=\"\", y=\"\")\n\n\n\n\nCool, huh?\n\n\n\n\nWilkinson, Leland. 2013. The Grammar of Graphics. 2nd ed. New York, NY: Springer-Verlag.\n\n\nYong, Laurel Harbridge, Jon A. Krosnick, and Jeffrey M. Wooldridge. 2016. “Presidential Approval and Gas Prices: Sociotropic or Pocketbook Influence?” In Political Psychology, edited by Jon A. Krosnick, I-Chant A. Chiang, and Tobias H. Stark, 246–75. Taylor; Francis Inc."
  },
  {
    "objectID": "LinAlg.html#modelling-in-economics-is-linear-algebra",
    "href": "LinAlg.html#modelling-in-economics-is-linear-algebra",
    "title": "Appendix B — Linear algebra",
    "section": "B.1 Modelling in economics is linear algebra",
    "text": "B.1 Modelling in economics is linear algebra\nThat’s a bit extreme, but you mostly need to do linear algebra to program up many of the estimators we need, or to solve a rational expectations models."
  },
  {
    "objectID": "LinAlg.html#beginning-to-program",
    "href": "LinAlg.html#beginning-to-program",
    "title": "Appendix B — Linear algebra",
    "section": "B.2 Beginning to program",
    "text": "B.2 Beginning to program\nA few non-linear algebra things we will need are summarized in the following table.\n\nUseful things\n\n\n\n\n\n\n\n\n\nR\nExample\nNotes\n\n\n\n\nAssign a value\n&lt;-\na &lt;- 4\nAlso legal is a = 4. But I hate it.\n\n\nCreate a list of values\nc(.)\nv &lt;- c(1, -2, 22)\nDefining ‘on the fly’\n\n\nSequence\nseq(i, k, l)\n\\(5\\), \\(7\\), … ,\\(21\\)\nCreate a sequence\n\n\n\ni:k\n\\(i\\), \\(i\\pm 1\\), … ,\\(k\\)\nShort cut for unit in/de-crements\n\n\nLoop commands\nfor (var in seq) expr\nfor (i in 5:1) print(i)\nLoops. We need loops.\n\n\nDraw a random number\nrnorm(k,a,b)\nrnorm(60, 0, 5)\nExample draws 60 values ~ \\(N(0,5)\\)\n\n\nCreate a matrix\nmatrix(v,i,j)\nmatrix(5, 2, 2)\nCreate a \\(2\\times 2\\) matrix of 5s\n\n\n\n\nB.2.1 Functions\nEverything in R is a function (although it doesn’t look like it). Defining a function is simple:\n\nname_of_function &lt;- function(function_arguments){\n  # Body of function where stuff is done  \n}\n\nHere’s one that actually does something:\n\naddaddadd &lt;- function(x,y){\n  z &lt;- 3*(x+y)\n  return(z)\n}\n\nand if we run it:\n\naddaddadd(4,6)\n\n[1] 30"
  },
  {
    "objectID": "LinAlg.html#linear-algebra",
    "href": "LinAlg.html#linear-algebra",
    "title": "Appendix B — Linear algebra",
    "section": "B.3 Linear algebra",
    "text": "B.3 Linear algebra\nAssume the following: \\(A\\) and \\(B\\) are real matrices of dimension \\(n\\times n\\), \\(b\\) and \\(c\\) are real \\(n-\\)vectors, \\(X\\) is a real \\(T\\times k\\) matrix, and \\(S\\) is a symmetric real matrix.\n\nMaths commands essential to linear algebra\n\n\n\n\n\n\n\n\n\nMaths\nR\nNotes\n\n\n\n\nHadamard product\n\\(A\\bigodot B\\)\nA * B\nElement-by-element, \\(A\\), \\(B\\) same size\n\n\nMatrix/vector product\n\\(A\\times B\\), \\(A \\times b\\)\nA %*% B, A %*% b\nNormal product rule\n\n\nInner product\n\\(X'X\\)\nt(X) %*% X\nAlso uses transpose operator, t()\n\n\n\n\ncrossprod(X)\nMore efficient, but less mathy\n\n\n\n\\(A'B\\)\nt(A) %*% B\n\n\n\n\n\ncrossprod(A,B)\n\n\n\nOuter product\n\\(A\\times B'\\)\ntcrossprod(A,B)\n\n\n\nInverse\n\\(A^{-1}\\)\nsolve(A)\nMatrix inverse is a special case of…\n\n\nSolve for \\(d\\)\n\\(Ad = b \\Rightarrow d = A^{-1}b\\)\nd &lt;- solve(A, b)\n…linear solution!\n\n\nCholesky decomp\n\\(S = R'R\\)\nR &lt;- chol(S)\n\\(S\\) is a symmetric, positive definite matrix\n\n\nCholesky inverse\n\\(S^{-1}\\)\nchol2inv(R)\nFast!\n\n\nDeterminant\n\\(\\vert A \\vert\\)\ndet(A)\n\n\n\nDiagonal\n\n\n\n\n\n\\(\\quad\\) of a matrix\n\ndiag(A)\nRetrieve the elements \\(a_{ii}\\), \\(i=1,..,n\\)\n\n\n\\(\\quad\\) in a matrix\n\nA &lt;- diag(b)\nSet the diagonal of \\(A\\) to \\(b\\), zero elsewhere\n\n\n\\(\\quad\\) Identity matrix\n\\(I_n\\)\ndiag(n)\n\n\n\nEigenvalues/vectors\n\nE &lt;- eigen(A)\nReturns a list: E$values, E$vectors"
  },
  {
    "objectID": "LinAlg.html#starting-to-do-linear-algebra",
    "href": "LinAlg.html#starting-to-do-linear-algebra",
    "title": "Appendix B — Linear algebra",
    "section": "B.4 Starting to do linear algebra",
    "text": "B.4 Starting to do linear algebra\n\nB.4.1 Problem\nConsider the following simultaneous system of equations:\n\\[\\begin{align*}\nx_1 + 2x_2 &= 6 \\\\\nx_1 - 3x_2 +2 x_3 &= 0 \\\\\n-2 x_1 + 3 x_3 &= 2\n\\end{align*}\\]\nFind the values of \\(x\\) that solve this using R.\nHint – write the problem in matrix form\n\\[\\begin{equation*}\nAx = b\n\\end{equation*}\\]\nwhere\n\\[\\begin{equation*}\nA = \\left [ \\begin{array}{rrr}\n             1 &  2 & 0 \\\\\n             1 & -3 & 2 \\\\\n            -2 &  0 & 3\n            \\end{array} \\right ], \\qquad\n  b = \\left[ \\begin{matrix}6 \\\\ 0 \\\\ 2\\end{matrix} \\right ]\n\\end{equation*}\\]\nand then use solve.\n\n\nB.4.2 Solution\nR code to create these matrices is:\n\n# Matrices are populated by column by default\nA &lt;- matrix(c(1,1,-2,2,-3,0,0,2,3),3,3)\nb &lt;- matrix(c(6,0,2),3,1)\n\nThe solution is:\n\nx &lt;- solve(A,b)\n\nwhere x is:\n\n\n     [,1]\n[1,]    2\n[2,]    2\n[3,]    2"
  },
  {
    "objectID": "LinAlg.html#eigenvalue-decomposition",
    "href": "LinAlg.html#eigenvalue-decomposition",
    "title": "Appendix B — Linear algebra",
    "section": "B.5 Eigenvalue decomposition",
    "text": "B.5 Eigenvalue decomposition\nAny square matrix can be decomposed into a non-singular matrix \\(V\\) of eigenvectors and a diagonal matrix of eigenvalues \\(\\Lambda\\) such that: \\[\\begin{equation}\n  A V = V \\Lambda \\Rightarrow A = V\\Lambda V^{-1}\n\\end{equation}\\] Call eigen to decompose our previously defined matrix \\(A\\)\n\ne &lt;- eigen(A)\nL &lt;- e$values    # Returns a list, assign vectors/values\nV &lt;- e$vectors\n\nNote \\(A\\) is not symmetric so it may have complex roots, which is does\n\nL\n\n[1] -3.682744+0.000000i  2.341372+0.873683i  2.341372-0.873683i\n\n\nIf we calculate \\(A = V\\Lambda V^{-1}\\) we get\n\ns &lt;- V %*% diag(L) %*% solve(V)\ns\n\n      [,1]             [,2]             [,3]\n[1,]  1+0i  2.000000e+00+0i -2.220446e-16+0i\n[2,]  1+0i -3.000000e+00+0i  2.000000e+00+0i\n[3,] -2+0i -1.054712e-15+0i  3.000000e+00+0i\n\n\nand when we drop the zero imaginary parts\n\nRe(s)\n\n     [,1]          [,2]          [,3]\n[1,]    1  2.000000e+00 -2.220446e-16\n[2,]    1 -3.000000e+00  2.000000e+00\n[3,]   -2 -1.054712e-15  3.000000e+00\n\n\nRound to eliminate numerical error, to get\n\nround(Re(s), digits=12)\n\n     [,1] [,2] [,3]\n[1,]    1    2    0\n[2,]    1   -3    2\n[3,]   -2    0    3"
  },
  {
    "objectID": "LinAlg.html#precision",
    "href": "LinAlg.html#precision",
    "title": "Appendix B — Linear algebra",
    "section": "B.6 Precision",
    "text": "B.6 Precision\nWhy do we round? Take a real matrix \\(A_{mn}\\) with \\(n \\le m\\) and pre-multiply by its own transpose, i.e. \\(S = A'A\\). \\(AA\\) is symmetric, positive semi-definite. If \\(rank(A) = n\\), then \\(rank(S) = n\\) and positive definite, and its inverse exists.\n\nA  &lt;- matrix(c(1, .2, 0, 1), 2, 2)\nS &lt;- t(A) %*% A\nS\n\n     [,1] [,2]\n[1,] 1.04  0.2\n[2,] 0.20  1.0\n\n\nLet’s invert \\(S\\) three different ways.\n\ni1 &lt;- solve(S) \ni2 &lt;- chol2inv(chol(S))\ni3 &lt;- qr.solve(S) \n\nPre-multiplying \\(S\\) by its inverse gives\n\\[\\begin{equation}\n   I_k = \\left[\\begin{matrix}1 &0 \\\\0 &1 \\\\\\end{matrix}\\right]\n\\end{equation}\\]\nLooking at the results of doing this for each method gives\n\ni1 %*% S\n\n             [,1] [,2]\n[1,] 1.000000e+00    0\n[2,] 2.775558e-17    1\n\ni2 %*% S\n\n     [,1]         [,2]\n[1,]    1 5.551115e-17\n[2,]    0 1.000000e+00\n\ni3 %*% S\n\n     [,1]          [,2]\n[1,]    1 -2.775558e-17\n[2,]    0  1.000000e+00\n\n\nwhich are all slightly different but by tiny – and insignificant – amounts. Don’t be fooled by this, they are all numerically the same."
  },
  {
    "objectID": "LinAlg.html#programming-the-regression-problem",
    "href": "LinAlg.html#programming-the-regression-problem",
    "title": "Appendix B — Linear algebra",
    "section": "B.7 Programming the regression problem",
    "text": "B.7 Programming the regression problem\nLet’s look at the familiar regression problem for some generated data. \\[\\begin{equation}\n  y = XB + \\epsilon\n\\end{equation}\\] where \\(\\epsilon \\sim N(0,.2)\\), \\(X\\) is a \\((k+1)\\times n\\) matrix of regressors including a constant and \\(B\\) a \\(k+1\\) vector of coefficients. Let’s generate some random data of an arbitrary sized problem:\n\nX &lt;- matrix(rnorm(180, 2, 1), 60, 3)\nhead(X, 6) # Print first six rows\n\n          [,1]     [,2]      [,3]\n[1,] 2.3588122 1.456386 2.3222084\n[2,] 0.8413073 4.157710 3.5556043\n[3,] 1.5474467 3.126836 2.7864946\n[4,] 0.8815598 3.074986 2.9358657\n[5,] 1.5360504 3.347174 0.4709853\n[6,] 2.0862710 1.968519 2.6678894\n\nX &lt;- cbind(1, X) # Add a constant\ntail(X,6) # Print last six rows\n\n      [,1]       [,2]      [,3]      [,4]\n[55,]    1  1.0427947 2.5979581 -0.316863\n[56,]    1  2.0975083 0.8404734  2.742461\n[57,]    1  1.9497584 2.4245861  2.885887\n[58,]    1 -0.1817872 3.3655608  1.861405\n[59,]    1  1.9967704 2.8199609  2.702852\n[60,]    1  1.5975392 0.3822781  2.379031\n\n\nNow create a dependent variable that is a linear combination of these variables plus some noise. Create the linear relationship first so we know what it is:\n\nB &lt;- matrix(c(0.5,1,-1,.2), 4, 1)\n\nand then the dependent variable:\n\ny &lt;- X %*% B + 0.2*rnorm(60)\n\nWe could now do a regression – i.e. calculate\n\\[\\begin{equation}\n  \\hat B = (X'X)^{-1}X'y\n\\end{equation}\\]\nwhich can be written:\n\nBhat &lt;- solve(t(X)%*%X)%*%t(X)%*%y\n\nwhich gives\n\n\n           [,1]\n[1,]  0.5222154\n[2,]  0.9753704\n[3,] -1.0346446\n[4,]  0.2309827\n\n\nBut I wouldn’t do it like this (we’ll see why in a minute). A better way would be\n\nBhat2 &lt;- chol2inv(chol(crossprod(X))) %*% crossprod(X,y)\n\nor even\n\nBhat3 &lt;- qr.solve(X,y)\n\nwhich both evaluate to the same \\(\\hat B\\) values. Each is better in different circumstances.\n\nB.7.1 Test timings\nWhy does it matter how you do things? It should be obvious that it might, but it turns out some fairly trivial things can make a lot of difference. We set some parameters so we can create a bigger problem.\n\nn &lt;- 400\nk &lt;- 12\n\nWe will use seven different methods to calculate an estimate of \\(B\\). These are two variations on the three calculations below (where the brackets matter!): \\[\\begin{align}\n\\hat B_1 &= ((X'X)^{-1}) X'y \\\\\n\\hat B_2 &= ((X'X)^{-1}) (X'y) \\\\\n\\hat B_3 &= ((X'X)^{-1} (X'y))\n\\end{align}\\] where we do each of these either ‘by hand’ or using crossprod, with a final solution using qr.solve.\n\nlibrary(tictoc)\n\nreps &lt;- 10000\nt    &lt;- list()\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B &lt;- matrix(runif(k+1), k+1, 1)\n  X &lt;- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y &lt;- X %*% B + 0.2 * rnorm(n) \n  Bhat &lt;- solve(t(X) %*% X) %*% t(X) %*% y \n  }\nt[[1]] &lt;- toc(quiet = TRUE)\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B &lt;- matrix(runif(k+1), k+1, 1)\n  X &lt;- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y &lt;- X %*% B + 0.2 * rnorm(n) \n  Bhata &lt;- solve(t(X)%*%X) %*% (t(X)%*%y) \n}\nt[[2]] &lt;- toc(quiet = TRUE)\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B &lt;- matrix(runif(k+1), k+1, 1)\n  X &lt;- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y &lt;- X %*% B + 0.2 * rnorm(n) \n  Bhatb &lt;- solve((t(X) %*% X), (t(X) %*% y)) \n  }\nt[[3]] &lt;- toc(quiet = TRUE)\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B &lt;- matrix(runif(k+1), k+1, 1)\n  X &lt;- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y &lt;- X %*% B + 0.2 * rnorm(n) \n  Bhat2 &lt;- solve(crossprod(X)) %*% crossprod(X,y) \n  }\nt[[4]] &lt;- toc(quiet = TRUE)\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B &lt;- matrix(runif(k+1), k+1, 1)\n  X &lt;- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y &lt;- X %*% B + 0.2 * rnorm(n) \n  Bhat2a &lt;- solve(crossprod(X), crossprod(X,y)) \n  }\nt[[5]] &lt;- toc(quiet = TRUE)\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B &lt;- matrix(runif(k+1), k+1, 1)\n  X &lt;- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y &lt;- X %*% B + 0.2 * rnorm(n)\n  Bhat2b &lt;- chol2inv(chol(crossprod(X))) %*% crossprod(X,y) \n  }\nt[[6]] &lt;- toc(quiet = TRUE)\n\nset.seed(123)\ntic()\nfor (i in 1:reps) { \n  B &lt;- matrix(runif(k+1), k+1, 1)\n  X &lt;- cbind(1L, matrix(rnorm(n*k, 2, 1), n, k))\n  y &lt;- X %*% B + 0.2 * rnorm(n) \n  Bhat3 &lt;- qr.solve(X,y) \n  }\nt[[7]] &lt;- toc(quiet = TRUE)\n\nHow do we display the timings we saved in t? We could do a simple (but dull) table, or something a bit nicer.\n\nlibrary(tidyverse)\n\nFF &lt;- c(\"solve(t(X)%*%X) %*% t(X) %*% y\",\n        \"solve(t(X)%*%X) %*% (t(X)%*%y)\",\n        \"solve(t(X)%*%X, (t(X)%*%y))\",\n        \"solve(crossprod(X)) %*% crossprod(X,y)\",\n        \"solve(crossprod(X), crossprod(X,y))\",\n        \"chol2inv(chol(crossprod(X))) %*% crossprod(X,y)\",\n        \"qr.solve(X,y)\")\n\ntms &lt;- as.numeric(gsub(\" sec elapsed\", \"\", unlist(t)[seq(3,21,3)]))\nv   &lt;- tibble(Method  = 1:7, \n              Times   = tms,\n              Formula = FF) %&gt;% \n  mutate(Col = case_when(Times == min(Times) ~ \"red\",\n                         TRUE ~ \"blue\"\n  ))\n\nggplot(v) + \n  geom_col(aes(x=Method, y=Times, fill=as.factor(Method)), alpha=.6) + \n  geom_text(aes(x=Method, y=.1, label=Formula, color=Col), size=5.5, hjust=0) +\n  theme_minimal() + \n  scale_color_identity() + \n  theme(legend.position = \"none\") + \n  theme(axis.text.y = element_blank()) + \n  coord_flip() + \n  labs(y=paste(\"Seconds taken to do\",reps,\"replications\"), x=\"\", \n       title=\"Timings of different numerical regression methods\")"
  },
  {
    "objectID": "index.html#sec-genesis",
    "href": "index.html#sec-genesis",
    "title": "Quantiles, Networks, Time",
    "section": "Genesis",
    "text": "Genesis\nCentral banks have been around quite a while. They began as a useful institution that mostly benefited banks, and were not universally seen as something that particularly benefited society. Central banking was long associated with grey men in grey suits, pondering deeply the impenetrable machinations of high finance, fuelled by cigar smoke and mystique. In truth, this – at best – Capraesque view of central bankers is substantially out of date, and has been for decades. They are no longer monochrome, or exclusively male, and need skills their forebears could have barely imagined – and they have most decidedly different priorities.\nLike all institutions, central banks are the people who work in them. And as with many occupations with a public service element, the substantial expertise embodied in those people for the very particular – and evolving – challenges of central banking could usefully be shared. Global challenges often need global solutions, and local challenges are faced everywhere and someone, somewhere has probably faced the same one as you. Recognising this led to a particular initiative taken by the Bank of England in the early 1990s. It was decided that the Bank should create a forum where the central bankers of the world could gather to commune, discuss, and above all, learn together.\nThe timing, of course, was not incidental and the initiative turned out to be a prescient one. This was at a major historical turning point, one that signalled a burgeoning new world order, as the Iron Curtain crumbled, the European experiment gathered momentum, and industrial might continued an inexorable shift eastwards. Economic policy had shifted too. Monetary policy was beginning a new and – as it turned out – lasting phase. There was indeed much to learn, and new monetary policy needed new approaches better suited to those policies. (Post-Great Financial Crisis, the necessary tool kit would expand again.)\nAnd so the Centre for Central Banking Studies was founded. Hammond (2006) provides a history of the early years of the CCBS, charting an ambitious project that had an immediate impact on international central banking practices. More than thirty years later it remains a key forum for learning, discussion and networking, just as intended. Literally thousands of central bankers have taken part in CCBS events, and many alumni now occupy senior policymaking positions around the world. Activities have evolved to include many more of the disparate areas of responsibility that now involve the central banking community, and with a truly global reach.\nThis book is about a small part of that output. It is (mostly) about applied economics and central banking. It is born of the experience of the many who have participated in events over the years, with literally thousands of suggestions that have improved and expanded the content delivered to properly reflect the daily concerns of the central bank economist.\nMostly, but not entirely. Mostly, because the tool kit continues to expand and the techniques of data analysis evolve. We are all data scientists of some sort now, with a domain specialisation of central banking. And that domain is somewhat different to economists in academia or industry, with more of a focus on what we might dub causal forecasting. Underlying much of central bank analysis are a number of useful statistical methods and machine learning models that complement the default econometric approach. Taken together these constitute a language that central bankers need to understand. Some of this book is about how to build, interpret, and use models that used to be anathema to the econometrician but are increasingly part of the predictive landscape.\n\n\n\n\n\n\nPuppet, Yogyakarta\n\n\n\n\n\n\n\nAbu Dhabi, United Arab Emirates\n\n\n\n\n\n\n\n\n\nVilla Sterne, Pretoria\n\n\n\n\n\n\n\nInsadong, Seoul\n\n\n\n\n\n\n\n\nOld Town, Montevideo\n\n\nPlaceholders abound.\n\n\n\n\n\n\nDisclaimer\n\n\n\nThe Bank of England does not accept any liability for misleading or inaccurate information or omissions in the information provided. The subject matter reflects the views of the author and not the wider Bank of England or its Policy Committees.\n\n\n\n\n\n\nHammond, Gill. 2006. “The Centre for Central Banking Studies.” Quarterly Bulletin Q2: 191–95. https://www.bankofengland.co.uk/-/media/boe/files/quarterly-bulletin/2006/the-centre-for-central-banking-studies.pdf."
  },
  {
    "objectID": "Unemp.html#survey-data-is-messy-people-store-it-in-strange-ways",
    "href": "Unemp.html#survey-data-is-messy-people-store-it-in-strange-ways",
    "title": "6  Plotting unemployment forecasts",
    "section": "6.1 Survey data is messy, people store it in strange ways",
    "text": "6.1 Survey data is messy, people store it in strange ways\nWe’re going to look at survey data because it has some useful characteristics that we might want to explore. The data is obtained from Philadelphia Federal Reserve’s Survey of Professional Forecasters. Their website explains the data.\n\n\n\n\n\n\nThe SPF\n\n\n\nOn the landing page they explain how the survey began and continues:\n’The Survey of Professional Forecasters is the oldest quarterly survey of macroeconomic forecasts in the United States. The survey began in 1968 and was conducted by the American Statistical Association and the National Bureau of Economic Research. The Federal Reserve Bank of Philadelphia took over the survey in 1990.\n‘The Survey of Professional Forecasters’ web page offers the actual releases, documentation, mean and median forecasts of all the respondents as well as the individual responses from each economist. The individual responses are kept confidential by using identification numbers.’\nFrom the Survey of Professional Forecasters\n\n\nSo we can look at the individual responses by ID number as well as the aggregate ones, and we will look at both. The data is stored in single files for different statistics and for different transforms. We will download the underlying, ID-level responses as well as the average (mean) level response, but there are also growth rates and so on."
  },
  {
    "objectID": "Unemp.html#load-packages",
    "href": "Unemp.html#load-packages",
    "title": "6  Plotting unemployment forecasts",
    "section": "6.2 Load packages",
    "text": "6.2 Load packages\nWe load the packages we use a lot or would be fiddly to reference in the code. These are all in the tidyverse including the packages for date, lubridate, which we in particular will use to deal with dates including the quarter number as a number.\n\nlibrary(tidyverse)\n\nThe following downloads the data:\n\nu &lt;- \"https://www.philadelphiafed.org/-/media/frbp/assets/surveys-and-data/survey-of-professional-forecasters/historical-data/\"\n\ndownload.file(paste0(u,\"meanlevel.xlsx\"), destfile = \"meanlevel.xlsx\", mode = \"wb\")\ndownload.file(paste0(u,\"spfmicrodata.xlsx\"), destfile = \"spfmicrodata.xlsx\", mode=\"wb\")\n\nThis is a ‘belt-and-braces’ approach, we download the data to a file rather than reading the contents to an object. This is partly because institutional firewall settings can cause difficulties for the latter that seem absent from the former. Note the option mode = \"wb\" is crucial for Excel files."
  },
  {
    "objectID": "Unemp.html#levels-plots",
    "href": "Unemp.html#levels-plots",
    "title": "6  Plotting unemployment forecasts",
    "section": "6.3 Levels plots",
    "text": "6.3 Levels plots\nNow read in the levels data to UNEMP and create a date series. We need to do the latter as the dates in the files are stored as the year and the quarter as two separate variables. Also, we drop UNEMPA to UNEMPD which are annual variables we don’t want, retaining the variables called UNEMP1 to UNEMP6.\nWe create the date using yq from lubridate for a suitably concatenated and formatted input, using:\n\nUNEMP &lt;- readxl::read_excel(\"meanlevel.xlsx\", na=\"#N/A\", sheet=\"UNEMP\") %&gt;% \n  mutate(Date = yq(paste(YEAR, QUARTER))) %&gt;%\n  select(Date, num_range(\"UNEMP\", 1:6))\n\nknitr::kable(head(UNEMP))\n\n\n\n\nDate\nUNEMP1\nUNEMP2\nUNEMP3\nUNEMP4\nUNEMP5\nUNEMP6\n\n\n\n\n1968-10-01\n3.5974\n3.6218\n3.8359\n4.0231\n3.9910\n3.9397\n\n\n1969-01-01\n3.4000\n3.5656\n3.7738\n3.8525\n3.8623\nNA\n\n\n1969-04-01\n3.3036\n3.4607\n3.6446\n3.8196\n3.9111\nNA\n\n\n1969-07-01\n3.5036\n3.6375\n3.8214\n3.9304\n3.9875\nNA\n\n\n1969-10-01\n3.7055\n3.8382\n4.1509\n4.4127\n4.4036\n4.3473\n\n\n1970-01-01\n3.6034\n4.0310\n4.3586\n4.4086\n4.3345\nNA\n\n\n\n\n\n\n\n\n\n\n\nWhat are these variables?\n\n\n\nConsulting Table 3 on page 22 of the documentation we find:\nAt each survey date, we record the projections for various horizons in the same row. UNEMP1 is the real-time quarterly historical value for the previous quarter—that is, the quarter before the quarter when we conducted the survey. UNEMP2 is the forecast (nowcast) for the current quarter—that is, the quarter when we conducted the survey. UNEMP3 to UNEMP6 are the forecasts for the following four quarters…\nwhere we have replaced the actual variable name with UNEMP as Table 3 uses NGDP for illustration.\n\n\nNow we need to organize this by including a date series for each forecast vintage. We rename the UNEMP variables consistent with their forecast horizon except for UNEMP1 which we rename UNRATE and time shift to get in the right period.1 We use the group_by(Date) to create a new variable FP that starts from each date and goes four periods into the future.\n\nU2 &lt;- UNEMP %&gt;%\n  mutate(UNRATE=lead(UNEMP1,1)) %&gt;%\n  select(Date, UNRATE, num_range(\"UNEMP\", 2:6)) %&gt;%\n  rename_with(~ paste0(\"UNEMP\", 0:4), starts_with(\"UNEMP\")) %&gt;%\n  pivot_longer(cols = starts_with(\"UNE\")) %&gt;% \n  group_by(Date) %&gt;% \n  mutate(FP = seq.Date(Date[1], by=\"quarter\", length.out=5)) %&gt;%\n  ungroup() \n\nThe following plots of the mean forecasts with the early estimate of unemployment rate supplied:\n\nggplot(U2) + \n  geom_line(aes(x=Date, y=UNRATE), colour=\"black\") + \n  geom_line(aes(x=FP, y=value, group=Date, colour=as.factor(Date))) + \n  theme_minimal() +\n  theme(legend.position = \"none\") + \n  labs(title=\"Mean forecasts\", x=\"\", y=\"\")\n\n\n\n\nThere are clear characteristics – on the way up unemployment is on average under-predicted, one the way down over-predicted. This gets worse the further out the forecast horizon. And maybe this century forecasting has got better. Recent events unsurprisingly look like an huge outlier."
  },
  {
    "objectID": "Unemp.html#underlying-forecasts",
    "href": "Unemp.html#underlying-forecasts",
    "title": "6  Plotting unemployment forecasts",
    "section": "6.4 Underlying forecasts",
    "text": "6.4 Underlying forecasts\nWe do the same for underlying data, at the ID level. Reading the data we get:\n\nmicro &lt;- readxl::read_excel(\"spfmicrodata.xlsx\", na=\"#N/A\", sheet=\"UNEMP\")  %&gt;%\n  mutate(Date = yq(paste(YEAR, QUARTER))) %&gt;% \n  select(Date, ID, UNRATE=UNEMP1, num_range(\"UNEMP\", 2:6)) %&gt;%\n  rename_with(~ paste0(\"UNEMP\", 0:4), starts_with(\"UNEMP\"))\n\nknitr::kable(head(micro))\n\n\n\n\nDate\nID\nUNRATE\nUNEMP0\nUNEMP1\nUNEMP2\nUNEMP3\nUNEMP4\n\n\n\n\n1968-10-01\n1\n3.6\n3.7\n4.0\n4.0\n3.7\n3.7\n\n\n1968-10-01\n2\n3.6\n3.5\n3.5\n3.5\n3.6\n3.6\n\n\n1968-10-01\n3\n3.6\n3.7\n3.9\n4.2\n4.2\n4.1\n\n\n1968-10-01\n4\n3.6\n3.8\n4.0\n4.2\n4.0\n4.0\n\n\n1968-10-01\n5\n3.6\n3.6\n3.7\n3.9\n3.7\n3.7\n\n\n1968-10-01\n6\n3.5\n3.6\n3.7\n3.9\n4.0\n3.9\n\n\n\n\n\nWhat is apparent from this is that the data is actually at the two-digit level, and that the historical unemployment rate used by the individual forecasters isn’t necessarily the same. So we can guess that the series UNRATE in the mean levels file is the average unemployment rates in the previous period observed by the forecaster. Turns out it is. (Have a look yourself.) We could look at the spread of these conditioning values, if we want, but here instead is the difference between the mean and the median value of the information.\n\nmicro %&gt;% \n  group_by(Date) %&gt;% \n  summarise(u = mean(UNRATE, na.rm=TRUE), v = median(UNRATE, na.rm=TRUE)) %&gt;%\n  ggplot() +\n  geom_line(aes(x=Date, y=u-v), color = \"pink\", linewidth = 1.2) +\n  theme_minimal() +\n  labs(x=\"\", y=\"\", title=\"Mean minus median of all forecasters' conditioning data by forecast period\")\n\n\n\n\nAs we can see, usually this is nearly nothing, although in one case the discrepancy is almost 0.1 percentage points. On average using the mean seems fine as a measure of the contemporaneous estimate of the latest unemployment rate, but perhaps there would be a case for using the median.\nWe can do a lot with this data. Let’s count and then plot how many forecasters there are for each period.\n\nmicro %&gt;% \n  pivot_longer(cols = starts_with(\"UNEMP\")) %&gt;% \n  filter(!is.na(value)) %&gt;% \n  group_by(Date, name) %&gt;% \n  summarise(n = n()) %&gt;%\n  filter(year(Date) &gt; 1979) %&gt;% \n  ggplot() + \n  geom_col(aes(x=Date, y=n, group=name, fill=as.factor(Date)), color=NA) +\n  theme_minimal() +\n  labs(title=\"UNEMP: No. forecasters forecasting 0 to 4 steps ahead\", x=\"\", y=\"\") +\n  theme(legend.position=\"none\") +\n  facet_wrap( ~ name, ncol=5)\n\n\n\n\nSo not everybody does all the steps ahead forecasts but most do, as all these graphs look about the same – look at the right hand graph where you can spot a few missing.\nIn the late 1980s the number of forecasters had really declined, but it picked up in the 1990s and hasn’t fallen since, although if anything recessions seem to increase the number of forecasters. Let’s do another check – is this pattern special to UNEMP? We plot the same for INDPROD and NGDP (we hide the code) and see whilst clearly not everybody forecasts all the variables the patterns of numbers of forecasters is very similar, implying for example that it wasn’t just that people stopped forecasting unemployment, they just stopped forecasting.\n\n\n\n\n\n\n\n\nUnemployment is clearly not the most popular series to forecast, but as a UK resident I was surprised how relatively popular it was – UK forecasts of unemployment rates were rare before Mark Carney became governor of the Bank of England.2\nNext, we do the same as before with vintages of forecast and dates, and we also create IDVin so we know the vintage of the forecast for each ID. All of this is put into a long data set to facilitate what we do next.\n\nmicro_long &lt;- micro %&gt;% \n  pivot_longer(cols = starts_with(\"UNE\")) %&gt;% \n  mutate(IDVin = paste0(\"ID\",ID,\"_Vin:\",Date)) %&gt;% \n  group_by(ID, Date) %&gt;% \n  mutate(FP = seq.Date(Date[1], by=\"quarter\", length.out=5)) %&gt;%\n  ungroup() %&gt;% \n  filter(!is.na(value)) \n\n\n6.4.1 Aggregating the forecasts\nCan we recreate the previous graph of the mean forecasts? Turns out we can and rather easily too. As we did to average the conditioning unemployment rates, we can use the summarise function to take the mean across the different IDs. We also redo the conditional rate too but for the long data set. The plot looks like the following:\n\nav &lt;- micro_long %&gt;% \n  group_by(Date, FP, name) %&gt;% \n  summarise(m = mean(value, na.rm=TRUE))\n\nu &lt;- micro_long %&gt;% \n  group_by(Date) %&gt;% \n  summarise(ur = mean(UNRATE, na.rm=TRUE)) %&gt;% \n  mutate(ur = lead(ur, 1))\n\nggplot(av) + \n  geom_line(data = u, aes(x=Date, y=ur), colour=\"black\") + \n  geom_line(aes(x=FP, y=m, group=Date, colour=as.factor(Date))) + \n  theme_minimal() +\n  theme(legend.position = \"none\") + \n  labs(title=\"Means of the dis-aggregate forecasts\", x=\"\", y=\"\")\n\n\n\n\nThat looks pretty much like we had before.3 But we can also look at the dispersion, and obtain various measures in the same way as for the mean.\n\nd &lt;- micro_long %&gt;% \n  group_by(Date, FP, name) %&gt;% \n  summarise(v = var(value, na.rm=TRUE), \n            a = mad(value, na.rm=TRUE), \n            i = IQR(value, na.rm=TRUE))\n\nd %&gt;% \n  filter(year(Date) &lt; 2020 & year(Date) &gt; 1979) %&gt;%\n  ggplot() + \n  geom_line(aes(x=FP, y=sqrt(v), group=name), color=\"red\", alpha=.5, linewidth=.8) + \n  geom_point(aes(x=FP, y=a, group=name), color=\"blue\", alpha=.5, size=1.2) + \n  geom_point(aes(x=FP, y=i/1.349, group=name), color=\"darkgreen\", alpha=.5, size=1.2) + \n  theme_minimal() +\n  labs(title=\"Dispersion of the dis-aggregate forecasts\", x=\"\", y=\"\",\n       subtitle=\"Standard error (red); Mean Absolute Deviation (blue); IQR/1.349 (green)\") +\n  facet_wrap(~ name, ncol = 5)\n\n\n\n\nSo already, this looks useful. We can see that the dispersion measures – standard deviation, mean absolute deviation and inter-quartile range4 – conform to type, getting larger the longer the horizon, bigger at turning points etc.\n\n\n6.4.2 Everything\nWhat about plotting all the underlying forecasts?\n\nggplot(micro_long) + \n  geom_line(aes(x=FP, y=value, group=IDVin, colour=as.factor(ID))) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(title=\"All unemployment forecasts\", x=\"\", y=\"Rate (percentage points)\")\n\n\n\n\n\n\n6.4.3 Sub groups of forecasters\nRecent forecasts by forecasters with earlier (i.e. lower value) IDs could be the most experienced, but we are relying on the SPF classification. So here’s a rough split based on ID number, which shows some differences that we might ascribe to experience.\n\nmicro_long %&gt;% \n  mutate(Split_ID = case_when(\n    ID &lt; 450 ~ \"ID 1-449\",\n    ID &gt; 550 ~ \"ID 551+\",\n    TRUE     ~ \"ID 450-550\")) %&gt;%\n  ggplot() + \n  geom_line(aes(x=FP, y=value, group=IDVin, colour=as.factor(ID))) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  facet_wrap(~ Split_ID) +\n  labs(x=\"\",y=\"\")\n\n\n\n\nBut we can be more accurate. We just work out when they did their first forecast and then set the groups accordingly. Here, we detect those that forecasted since the last millennium5, since the onset of the last recession, or in between. So are the new kids better – or worse?\n\nmicro_long %&gt;%\n  group_by(ID) %&gt;% \n  mutate(Earliest = min(Date)) %&gt;% \n  ungroup() %&gt;% \n  mutate(Recent = case_when(\n    year(Earliest) &gt; 2008 ~ \"3. First forecasted after 2008\",\n    year(Earliest) &lt; 2001 ~ \"1. First forecasted before 2001\",\n    TRUE                  ~ \"2. Started forecasting 2001-2008\")) %&gt;% \n  filter(year(FP) &gt; 2000) %&gt;%\n  ggplot() + \n  geom_line(aes(x=FP, y=value, group=IDVin, colour=as.factor(ID))) +\n  facet_wrap(~Recent) + \n  theme_minimal() +\n  theme(legend.position = \"none\") + \n  labs(title=\"US unemployment forecasts by when forecaster first operated\", x=\"\", y=\"\")\n\n\n\n\nWorse, right? Kids today, don’t know they’re born. Maybe, maybe not…. Here’s a plot of some of the forecasts which reveal breaks:\n\nggplot(filter(micro, ID&gt;=400 & ID &lt; 434)) + \n  geom_point(aes(x=Date, y=UNEMP1, group=ID, color=as.factor(ID)), show.legend = FALSE) + \n  facet_wrap(~ ID) + \n  theme_minimal() + \n  labs(x=\"\", y=\"\", title=\"UNEMP1 for ID numbers in the early 400s\")\n\n\n\n\nThis seems to imply that the caveats that some ID numbers may have been re-used when forecasters left the panel could be important – did 422 go on a prolonged holiday? Did 429 figure unemployment was too hard to forecast in the great financial crisis? Or were they (ironically) unemployed at that time?…"
  },
  {
    "objectID": "Unemp.html#footnotes",
    "href": "Unemp.html#footnotes",
    "title": "6  Plotting unemployment forecasts",
    "section": "",
    "text": "Leading the variable one period is the correct shift, although it seems the wrong way to me every time I think about it.↩︎\nThis was for a number of reasons, but forecasters were probably quite pleased not to be judged on their performance in a difficult case.↩︎\nActually if you look at all the numbers properly there is the odd discrepancy, but it is not possible to say if the source of the differences lies in the disaggregate data or the mean levels. There are lots of good reasons for small discrepancies.↩︎\nWe deflate this by 1.349 to convert it to an asymptotically valid (given normality) estimate of the standard error.↩︎\nAnd yes, 2000 was in the last millennium.↩︎"
  },
  {
    "objectID": "GDP.html#simple-implementation-of-gdp-at-risk-using-r",
    "href": "GDP.html#simple-implementation-of-gdp-at-risk-using-r",
    "title": "8  GDP@Risk",
    "section": "8.1 Simple implementation of GDP-at-Risk using R",
    "text": "8.1 Simple implementation of GDP-at-Risk using R\nA somewhat fashionable use of quantile regression is by Adrian, Boyarchenko, and Giannone (2019), and their idea has become known as GDP-at-risk, rather like VaR. The idea is to use a simple forecasting model that uses some financial indicator and find the “at risk” value of growth. There are three elements:\n\nEstimate a “forecasting model” using quantile regression that depends on some forward-looking indicator.\nFit a skew Student-t model to the output of their quantile estimation procedure, and use this to find the “at risk” value.\nWe will plot the result as a ridgeline graph, which are really quite cool.\n\nThis is computationally quite a bit harder than our other applications."
  },
  {
    "objectID": "GDP.html#prelims",
    "href": "GDP.html#prelims",
    "title": "8  GDP@Risk",
    "section": "8.2 Prelims",
    "text": "8.2 Prelims\nWe need a lot of libraries for this one, although we have relatively little code!\n\nlibrary(jsonlite)          # Read in UK data\nlibrary(quantreg)          # Quantile regression\nlibrary(ggridges)          # Ridgeline plots\nlibrary(viridis)           # Colours for graphs\nlibrary(tidyverse)         # Usual\nlibrary(readxl)            # And more...\nlibrary(sn)                # Skew-t distribution"
  },
  {
    "objectID": "GDP.html#data",
    "href": "GDP.html#data",
    "title": "8  GDP@Risk",
    "section": "8.3 Data",
    "text": "8.3 Data\nDownloaded data up until the repsent for UK GDP growth in JSON format from ONS UK in a file called ihyr.json.\n\njson  &lt;- fromJSON(\"ihyr.json\")  # Use jsonlite to parse file\n\n# Retrieve quarterly data, dates etc and calculate lags\nqdata &lt;- json$quarters %&gt;% \n  mutate(Date   = yq(date), \n         Growth = as.numeric(value)) %&gt;%\n  select(Date, Growth) %&gt;%\n  mutate(Growth_1 = lag(Growth, 1), \n         Growth_4 = lag(Growth, 4)) %&gt;%\n  drop_na()\n\nBIS credit data is available here – be warned there is a lot of it, and you need to get all of it to find the bits you want.\n\ntotcredit &lt;- read_excel(\"totcredit.xlsx\", \n                        sheet = \"Quarterly Series\",\n                        col_types = c(\"date\", rep(\"text\", 1133))) %&gt;%   # This my need adjusting\n  select(Date = \"Back to menu\", starts_with(\"United K\")) %&gt;%            # Find UK\n  slice(-c(1:3)) %&gt;% \n  select(-contains(\"US Dollar\"), -contains(\"Unadjusted\"), -contains(\"Domestic currency\")) %&gt;%\n  mutate(Date = ymd(Date)) \n\nWarning: Expecting date in A4 / R4C1: got 'Period'\n\n\nNames in the BIS file are very long so kludge them to something a bit more readable.\n\nnn  &lt;- gsub(\"United Kingdom - \", \"\", names(totcredit))\nnn  &lt;- gsub(\" - Adjusted for breaks\", \"\", nn)\nnn  &lt;- gsub(\" - Percentage of GDP\", \"\", nn)\nnn  &lt;- gsub(\" at Market value\", \"\", nn)\n\ntotcredit &lt;- totcredit %&gt;%\n  rename_with( ~ nn)\n\n\n8.3.1 Plots of pivoted data\n\ndd &lt;- totcredit %&gt;%\n  pivot_longer(cols=-Date) %&gt;%\n  mutate(value = as.numeric(value)) %&gt;%\n  filter(!is.na(value))\n\nggplot(dd) + \n  geom_line(aes(x=Date, y=value, colour=name), show.legend=FALSE) +\n  facet_wrap(~ name, scales = \"free\") + \n  theme_minimal() +\n  labs(x=\"\", y=\"\", title=\"Credit data; all as percentage of GDP\")\n\n\n\n\n\n\n8.3.2 Difference data at required interval\nWe will use some measure of long run credit growth as a predictor of financial fragility. Pick an interval – we choose five years – and calculate the growth rate.\n\nlagv &lt;- 20\n\ndd2 &lt;- dd %&gt;% \n  group_by(name) %&gt;% \n  mutate(value = 100*(value/lag(value,lagv)-1)) %&gt;% \n  ungroup() \n\nggplot(dd2) + \n  geom_line(aes(x=Date, y=value, colour=name), show.legend = FALSE) +\n  facet_wrap(~ name, scales = \"free\") + \n  theme_minimal() +\n  labs(x=\"\",y=\"\", title=paste(\"Credit data; Percentage difference over\", lagv, \"quarters\"))\n\n\n\n\n\n\n8.3.3 Choose a variable\nWe select the variable we want, plot it to check, and then create a data set to use in the quantile regression. What are the variables?\n\n# Recall all the names are in nn\nprint(nn)\n\n[1] \"Date\"                                                          \n[2] \"Credit to Non financial sector from All sectors\"               \n[3] \"Credit to General government from All sectors\"                 \n[4] \"Credit to General government from All sectors at Nominal value\"\n[5] \"Credit to Households and NPISHs from All sectors\"              \n[6] \"Credit to Non-financial corporations from All sectors\"         \n[7] \"Credit to Private non-financial sector from All sectors\"       \n[8] \"Credit to Private non-financial sector from Banks, total\"      \n\n\nLet’s go for number 7.\n\ndd2 &lt;- filter(dd2, name == nn[7]) %&gt;% \n  select(Date, value) %&gt;% \n  rename_with(~ c(\"Date\", \"GCredit\")) %&gt;% \n  mutate(Date = floor_date(Date, unit=\"quarter\")) %&gt;% \n  arrange(Date)\n\n# Quick plot to check we have the right one\nggplot(dd2) + \n  geom_line(aes(x=Date, y=GCredit), color = \"red\") +\n  theme_minimal()\n\n\n\ndataz &lt;- left_join(qdata, dd2, by=\"Date\") %&gt;% \n  mutate(GCredit_1 = lag(GCredit,1)) %&gt;% \n  mutate(GCredit_4 = lag(GCredit,4)) %&gt;% \n  drop_na() %&gt;%\n  filter(year(Date)&lt;2021)\n\nhead(dataz)\n\n        Date Growth Growth_1 Growth_4    GCredit  GCredit_1  GCredit_4\n1 1969-01-01    1.7      5.7      6.1 -0.8333333 14.9905123 11.1913357\n2 1969-04-01    2.9      1.7      4.2  5.1107325 -0.8333333 17.3674589\n3 1969-07-01    1.5      2.9      5.8  5.2173913  5.1107325 16.1410019\n4 1969-10-01    1.6      1.5      5.7  5.6939502  5.2173913 14.9905123\n5 1970-01-01    1.1      1.6      1.7 -7.4960128  5.6939502 -0.8333333\n6 1970-04-01    2.8      1.1      2.9 -3.7277147 -7.4960128  5.1107325"
  },
  {
    "objectID": "GDP.html#equation-and-estimates",
    "href": "GDP.html#equation-and-estimates",
    "title": "8  GDP@Risk",
    "section": "8.4 Equation and estimates",
    "text": "8.4 Equation and estimates\nRun a single quantile regression. We will do just one, and then look at in-sample predictions. Really we ought to do this recursively. A couple of parameters let us choose bits of the model.\n\nfcast &lt;- 4\ninccg &lt;- 1\n\nif (inccg &gt; 0) {\n  eqn.q &lt;- formula(paste0(\"Growth ~ Growth_\", fcast, \" + GCredit_\", fcast))\n} else {\n  eqn.q &lt;- formula(paste0(\"Growth ~ Growth_\", fcast))  \n}\n\nqvals  &lt;- seq(.05,.95,.025)\nq.inst &lt;- rq(eqn.q, data=dataz, tau=qvals)\n# summary(q.inst)\nq.inst\n\nCall:\nrq(formula = eqn.q, tau = qvals, data = dataz)\n\nCoefficients:\n             tau= 0.050  tau= 0.075  tau= 0.100   tau= 0.125  tau= 0.150\n(Intercept) -3.60730272 -2.65975882 -2.03373503 -0.283117717  0.08723187\nGrowth_4    -0.01349979 -0.01961265  0.14950306  0.326307071  0.29884421\nGCredit_4    0.04892147  0.02966876  0.02880223 -0.006781692 -0.01380020\n             tau= 0.175  tau= 0.200  tau= 0.225  tau= 0.250  tau= 0.275\n(Intercept)  0.52998577  0.67211457  0.78265307  0.87493621  0.96717528\nGrowth_4     0.23938813  0.26226668  0.28785310  0.29513320  0.27729443\nGCredit_4   -0.01400174 -0.01139684 -0.01308326 -0.01193709 -0.01211441\n              tau= 0.300   tau= 0.325  tau= 0.350   tau= 0.375   tau= 0.400\n(Intercept)  1.132454167  1.294438029  1.44292183  1.463810672  1.562583315\nGrowth_4     0.253116511  0.231861348  0.20268022  0.215751945  0.195380042\nGCredit_4   -0.009604268 -0.006338726 -0.00567619 -0.002420902 -0.001360645\n              tau= 0.425   tau= 0.450    tau= 0.475   tau= 0.500    tau= 0.525\n(Intercept)  1.663627576 1.6862397478  1.9086308843  2.039651688  2.0709412090\nGrowth_4     0.177560201 0.1771913047  0.1460039958  0.158547435  0.1639409996\nGCredit_4   -0.000887589 0.0004844992 -0.0009085638 -0.003658792 -0.0005871492\n             tau= 0.550  tau= 0.575  tau= 0.600   tau= 0.625 tau= 0.650\n(Intercept) 2.120989585 2.175379958 2.375690044  2.548885682  2.6555717\nGrowth_4    0.161292582 0.170752556 0.142450983  0.127591069  0.1586058\nGCredit_4   0.002333639 0.000429499 0.002191878 -0.001498171 -0.0024793\n              tau= 0.675   tau= 0.700   tau= 0.725  tau= 0.750  tau= 0.775\n(Intercept)  2.691126204 2.7527507431  2.886669318 2.934754152 2.999541321\nGrowth_4     0.192269383 0.1886024512  0.178036916 0.170590019 0.160463858\nGCredit_4   -0.000934141 0.0006228696 -0.002012307 0.001794157 0.001253084\n              tau= 0.800   tau= 0.825   tau= 0.850   tau= 0.875  tau= 0.900\n(Intercept)  3.130746672  3.433999347  3.672727571  3.840716859 4.009738583\nGrowth_4     0.217223769  0.195635922  0.189628209  0.242774436 0.274378382\nGCredit_4   -0.003218895 -0.008257329 -0.008929732 -0.000484291 0.005502021\n              tau= 0.925  tau= 0.950\n(Intercept) 4.3225662301 4.475711862\nGrowth_4    0.3251010478 0.357499115\nGCredit_4   0.0004444649 0.001945412\n\nDegrees of freedom: 208 total; 205 residual\n\n\n\n8.4.1 Non-parametric estimated quantiles\nWe can easily plot the estimated quantiles as ridgeline plots, see Wilke (2020). First we retrieve and then organize the predicted values.\n\nq.predict &lt;- t(predict(q.inst)) %&gt;%           # In-sample predictions\n  as_tibble(.name_repair = ~ as.character(dataz$Date)) %&gt;%\n  mutate(q = qvals) %&gt;%\n  pivot_longer(cols=-q, names_to=\"Date\") %&gt;%\n  mutate(Date = ymd(Date)) %&gt;% \n  filter(year(Date) &gt; 2012)\n\nNext we plot them as a non-parametric estimate of the cumulative density in a ridgeline plot.\n\nsc &lt;- 1000\n\nq.predict %&gt;% \n  ggplot() + \n  geom_ridgeline(aes(x=value, height=q, y=Date, group=Date, scale=sc, fill=as.factor(Date)),  \n                          colour=NA, show.legend=FALSE) +\n  scale_fill_cyclical(values = c(\"orange\", \"yellow\")) +\n  theme_ridges() + \n  labs(x=\"\", y=\"\", title = \"GDP@Risk: Non-parametric cumulative density estimates\")\n\n\n\n\nWhat about with the tail probabilities emphasized? Now can use geom_ridgeline_gradient, where the fill is over the continuous x-axis.\n\nq.predict %&gt;% \n  ggplot() + \n  geom_ridgeline_gradient(aes(x=value, height=q, y=Date, group=Date, scale=sc, \n                              fill=0.5-abs(q-0.5)),  \n                          colour=\"grey77\", show.legend=FALSE) +\n  scale_fill_viridis(option=\"D\", direction=-1, alpha=.67) +\n  theme_ridges() + \n  labs(x=\"\", y=\"\", title = \"GDP@Risk: Non-parametric cumulative density estimates\")\n\n\n\n\nThere are important features. There’s quite a long, messy left hand tail, and the density looks often multi-modal, as the colors switch. A way of tidying this up is to fit a parametric distribution and to treat that as the actual distribution. This has implications of course. This is what the original authors do.\n\n\n8.4.2 Parametric results\nWe now fit a skew-t to the predicted quantiles, and then work with these estimated densities afterwards.\n\nfitst &lt;- function(e, p, q=qvals) {\n  sum((p - qst(q, xi=e[1], omega=e[2], alpha=e[3], nu=(e[4])))^2)\n  }\n\n# fitsti &lt;- function(e, p, q) {\n#   ss &lt;- rep(0, 30)\n#   for (i in 1:30) {\n#     ss[i] &lt;- optim(e, fitsti, gr=NULL, p, q)\n#   }\n#   e[4] &lt;- which(ss==min(ss))\n#   }\n\ndens &lt;- NULL # Store densities\neall &lt;- NULL # Store estimated parameters\n\nx   &lt;- seq(-5,7,0.05)                        # Evaluate fitted density over this interval\n\n# sel &lt;- c(0.05, 0.15, 0.25, 0.36, 0.45, 0.5, 0.55, 0.65, 0.75, 0.85, 0.95)\n# sel &lt;- c(0.05, 0.25, 0.75, 0.95)\nsel &lt;- qvals\n\ndte  &lt;- q.predict$Date[88]\nkvar &lt;- 0.1\nfor (dte in unique(q.predict$Date)) {\n\n  pp  &lt;- q.predict %&gt;% \n    filter(Date==dte) # Predicted vals for i\n  \n  p  &lt;- pp$value\n  q  &lt;- pp$q\n  e0 &lt;- c(p[q==0.5], 1, 0, 1)\n  \n  fst &lt;- optim(e0, fitst, gr=NULL, p[q %in% sel], sel, \n               method = \"L-BFGS-B\",\n               lower=c(-20, 0,  -Inf, 1),\n               upper=c(20,  Inf, Inf, 30),\n               control = list(factr=1e4))\n  e   &lt;- fst$par                                                    # Fitted values\n  y   &lt;- dst(x,    xi=e[1], omega=e[2], alpha=e[3], nu=(e[4]))   # Fitted density\n  vr  &lt;- qst(kvar, xi=e[1], omega=e[2], alpha=e[3], nu=(e[4]))   # k% quantile\n  dr  &lt;- dst(vr,   xi=e[1], omega=e[2], alpha=e[3], nu=(e[4]))   # Density at that point\n  \n  dens &lt;- bind_rows(dens, tibble(x=x,   \n                                 y=y,  \n                                 Date=as.Date(dte),\n                                 vr=vr, \n                                 dr=dr,\n                                 v=as.numeric(x&gt;vr)))\n  eall &lt;- bind_rows(eall, tibble(Date=as.Date(dte), xi=e[1], omega=e[2], alpha=e[3], nu=(e[4])))\n}\n\nThe coefficients don’t show an obvious pattern as we can see from the plots below:\n\neall %&gt;%\n  pivot_longer(-Date) %&gt;%\n  ggplot() + \n  geom_line(aes(x=Date, y=value, color=name), show.legend=FALSE) + \n  theme_minimal() + \n  labs(x=\"\", y=\"\", title=\"GDP@Risk: Fitted skew-t coefficients\") +\n  facet_wrap(~ name, scales=\"free_y\")\n\n\n\n\nPlots of calculated densities is easy – we need to scale them – here with the 10% value as dots.\n\nsc   &lt;- 1750                                    # Scale factor\n\ndens %&gt;% \n  ggplot() + \n  geom_ridgeline(aes(x=x, height=y, y=Date, group=Date), colour=\"grey77\", fill=\"slateblue1\", scale=sc) +\n  geom_point(data = . %&gt;% select(Date, vr) %&gt;% unique(), aes(x=vr,y=Date), color=\"red\", size=1.1) + \n  theme_ridges() + \n  labs(x=\"\", y=\"\", title = \"GDP@Risk: Fitted skew-t\")\n\n\n\n\nWe can do a lot better, and recreate the density shaded by quantiles using v, calculated above. Note the variable v is zero to the left of the 10% value, and 1 otherwise. we can use this to shade the areas using:\n\ncpt &lt;- paste0(\"Shade indicates \", kvar*100,\"%\")\n\ndens %&gt;% \n  ggplot() + \n  geom_ridgeline_gradient(aes(x=x, height=y, y=Date, group=Date, scale=sc, fill=factor(v)),  \n                          colour=\"grey77\", show.legend=FALSE) +\n  scale_fill_viridis_d(option=\"E\", direction=-1, alpha=.67) +\n  theme_ridges() + \n  labs(x=\"\", y=\"\", title=\"GDP@Risk: Fitted skew-t\", caption=cpt)\n\n\n\n\nCool, huh?\n\n\n\n\nAdrian, Tobias, Nina Boyarchenko, and Domenico Giannone. 2019. “Vulnerable Growth.” American Economic Review 109 (4): 1263–89.\n\n\nWilke, Claus O. 2020. ggridges: Ridgeline Plots in ’ggplot2’. https://CRAN.R-project.org/package=ggridges."
  },
  {
    "objectID": "FansQR.html#quantile-regression",
    "href": "FansQR.html#quantile-regression",
    "title": "7  Quantile fan charts",
    "section": "7.1 Quantile regression",
    "text": "7.1 Quantile regression\nQR is a powerful technique building on the insight that the predicted value of a regression need not be the conditional mean. Instead QR models a chosen conditional quantile of the distribution of the target variable.\nThe predicted quantiles can be used individually, perhaps as a proxy for risk, or used to approximate a complete predicted density. It is the last of these that particularly concerns us.\nGaglianone and Lima (2012) estimate forecast densities for the U.S. unemployment rate using the consensus forecast from the Survey of Professional Forecasters (SPF). This generalizes the proposition of Capistrán and Timmermann (2009), that we can efficiently combine forecasts by regressing the consensus (usually mean) forecast on the out-turn and use the resulting equation for point-forecasting unemployment, to one where the density of the combined forecast is predicted.\nThey suggest:\n\n‘The resulting density forecast is far from normal and is therefore able to reflect the current increased risk of a higher unemployment rate in the U.S. economy provoked by the recent subprime crisis.’ Gaglianone and Lima (2012), p. 1598\n\nThis highlights the two useful features that makes this technique so valuable. The estimated forecast density need not be normal and is potentially state dependent. But it is difficult to tell how significant this effect is from one graph, or whether the upward spread of the density is associated with the subprime crisis. What happens if the unemployment rate is radically different from the experience of 2010? As the subprime crisis is no longer quite so recent we consider the following: when and how are the QR-forecast densities for U.S. unemployment asymmetric?\nCapistrán and Timmermann (2009) suggested combining forecasts by regressing the mean forecast on the out-turn, which for unemployment forecasts would be \\[\n   u_{t+k} = \\beta_0+\\beta_1 \\hat u_{t,t+k}+\\varepsilon_{t+k}\n\\] where \\(u_t\\) is the quarterly unemployment rate expressed as a percentage and \\(\\hat u_{t,t+k}\\) is the SPF’s mean forecast, \\(k =\\) 1 to 4 quarters ahead. They find this an effective way to combine forecasts from different sources particularly with a non-stationary panel of forecasters, characteristic of the SPF. This models the conditional mean of unemployment as a ‘bias corrected’ forecast from the aggregated information.\nGaglianone and Lima (2012) instead use QR to predict the \\(\\alpha\\)-quantile of \\(u_{t+k}\\) so that \\[\n  Q_\\alpha(u_{t+k}) = \\beta_0(\\alpha) + \\beta_1(\\alpha) \\hat u_{t,t+k} + \\varepsilon{(\\alpha)}_{t+k}\n\\] where \\(Q_\\alpha(\\cdot)\\) is the quantile function. This yields a sequence of models, indexed both by \\(\\alpha\\) and the forecast horizon. It is a simple matter to derive the required forecast density from them, smoothed using a kernel method."
  },
  {
    "objectID": "FansQR.html#a-fan-chart",
    "href": "FansQR.html#a-fan-chart",
    "title": "7  Quantile fan charts",
    "section": "7.2 A fan chart",
    "text": "7.2 A fan chart\nGaglianone and Lima (2012) don’t actually produce a fan chart, they plot the implied densities. We can do that, but the fan chart is more useful.\nThe following code does a lot. It read the SPF individual forecasts and then averages them (you’ll see why later). Then it does the QR, calculates the points for the fans a plots a fan chart.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(quantreg)\nlibrary(moments)\n\nfls    &lt;- \"spfmicrodata.xlsx\"\nUNRATE &lt;- read_csv(\"UNRATE (1).csv\", \n                   col_types=cols(DATE=col_date(format=\"%d/%m/%Y\"))) %&gt;%\n  select(Date=DATE, UNRATE) %&gt;% \n  mutate(UNRATE=as.numeric(UNRATE)) %&gt;%\n  drop_na()\n\nmx &lt;- read_excel(fls, sheet=\"UNEMP\", na=\"#N/A\", col_types=\"numeric\") %&gt;%\n  unite(Date, c(YEAR, QUARTER), sep=\" \") %&gt;% \n  mutate(Date = yq(Date)) %&gt;% \n  select(Date, UNEMP0=UNEMP2, UNEMP1=UNEMP3, UNEMP2=UNEMP4, UNEMP3=UNEMP5, UNEMP4=UNEMP6, ID) %&gt;%\n  pivot_longer(cols=-c(Date, ID), names_to=\"Horizon\") %&gt;% \n  group_by(Date, Horizon) %&gt;% \n  summarise(av   = mean(value, na.rm=TRUE)) %&gt;% \n  mutate(Horizon = as.integer(str_sub(Horizon,6,6))) %&gt;% \n  ungroup() %&gt;%\n  drop_na() \n\n# Pivot wider to put each series in a column to merge with UNEMP data\nmxb &lt;- mx %&gt;% \n  pivot_longer(cols = -c(Date, Horizon), names_to = \"Variable\") %&gt;% \n  unite(\"Names\", Variable:Horizon, sep=\"\") %&gt;% \n  pivot_wider(names_from = Names) %&gt;%\n  slice(-n())\n\nUNEMP &lt;- UNRATE %&gt;% \n  #group_by(paste(year(Date), quarter(Date))) %&gt;% \n  #summarise(Date=min(Date), UNRATE=mean(UNRATE)) %&gt;% \n  #select(Date, UNRATE) %&gt;% \n  right_join(mxb, by=\"Date\") \n\n# QReg \nsf &lt;- seq(.05,.95,.15)[-4]\n\ntail_colour   &lt;- \"grey95\"\ncentre_colour &lt;- \"seagreen\"\n\nnq  &lt;- length(sf)\nnv  &lt;- length(centre_colour)\ncol &lt;- colorRampPalette(c(rbind(tail_colour, centre_colour), tail_colour))(nv*nq+1)[-1]\n\nystart &lt;- 2017\nares   &lt;- NULL\nfor (i in 0:4) {\n          reg  &lt;- c(paste0(\"av\",i))\n          eq   &lt;- formula(paste0(\"lead(UNRATE,\",(i+1),\") ~ \", reg))\n          eqrq &lt;- rq(eq, data=UNEMP, tau=sf)\n          res  &lt;- broom::tidy(eqrq) %&gt;% \n            group_by(tau) %&gt;%\n            mutate(dta = unlist(c(1, slice(select(UNEMP, reg), n())))) %&gt;% \n            mutate(q   = sum(estimate*dta)) %&gt;% \n            ungroup %&gt;%\n            mutate(Vintage = max(UNEMP$Date), Horizon = i) \n          ares &lt;- bind_rows(ares, res)\n}\n\naresx &lt;- ares %&gt;% \n  select(tau, q, Vintage, Horizon) %&gt;% \n  distinct() %&gt;%\n  left_join(select(UNEMP, Date, UNRATE), by=c(\"Vintage\" = \"Date\")) %&gt;% \n  mutate(q = if_else(Horizon == 0, UNRATE, q)) %&gt;% \n  pivot_wider(names_from = tau, names_prefix = \"tau=\", values_from = q) %&gt;% \n  group_by(Vintage) %&gt;% \n  mutate(fdate = seq.Date(from = Vintage[1], by = \"quarter\", length.out = 5)) %&gt;% \n  ungroup() %&gt;% \n  pivot_longer(starts_with(\"tau\"), names_to = \"q\", values_to = \"Vals\") %&gt;% \n  mutate(qs = paste0(\"Q\", q)) %&gt;%\n  mutate(qs = as_factor(desc(qs))) %&gt;% \n  select(-UNRATE)\n\nbck &lt;- UNEMP %&gt;% \n  select(Date, UNRATE) %&gt;%\n  filter(year(Date) &gt; ystart) %&gt;%\n  mutate(Vintage = list(unique(aresx$Vintage)))  %&gt;% \n  unnest(cols = Vintage) %&gt;% \n  group_by(Vintage) %&gt;% \n  filter(Date &lt;= Vintage) %&gt;% \n  ungroup() %&gt;% \n  arrange(Vintage, Date)\n\nggplot(aresx) +\n  geom_rect(aes(xmin=fdate, xmax=max(fdate), ymin=-Inf, ymax=Inf, group=q), fill=tail_colour) +\n  geom_area(aes(x=fdate, y=Vals, group=qs, fill=qs), position=\"identity\") +\n  scale_fill_manual(values=col) +\n  theme_minimal() + \n  theme(legend.position = \"none\") +\n  scale_x_date(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0)) +\n  geom_line(data=bck, aes(x=Date, y=UNRATE), colour=\"grey44\") + \n  facet_grid(Vintage ~ .) +\n  labs(title=paste(\"US unemployment rate and forecast fanchart\"), y=\"\", x=\"\")"
  },
  {
    "objectID": "FansQR.html#different-horizons",
    "href": "FansQR.html#different-horizons",
    "title": "7  Quantile fan charts",
    "section": "7.3 Different horizons",
    "text": "7.3 Different horizons\nWe compute recursive quantile regressions are calculated in pseudo-real time (we don’t take revisions to \\(u_t\\) into account, but it should be noted these are relatively minor). We use the available SPF forecasts from the last period used in the estimation to make the quantile predictions.\nAdd we add a regressor – a measure of uncertainty.\n\n# Add some scale/skew measures\nmx &lt;- read_excel(fls, sheet=\"UNEMP\", na=\"#N/A\", col_types=\"numeric\") %&gt;%\n  unite(Date, c(YEAR, QUARTER), sep=\" \") %&gt;%\n  mutate(Date = yq(Date)) %&gt;%\n  select(Date, UNEMP0=UNEMP2, UNEMP1=UNEMP3, UNEMP2=UNEMP4, UNEMP3=UNEMP5, UNEMP4=UNEMP6, ID) %&gt;%\n  pivot_longer(cols=-c(Date, ID), names_to=\"Horizon\") %&gt;%\n  group_by(Date, Horizon) %&gt;%\n  summarise(n    = n(),\n            av   = mean(value, na.rm=TRUE),\n            IQR  = IQR(value,  na.rm=TRUE)/1.349,\n            Var  = var(value,  na.rm=TRUE),\n            SE   = sqrt(Var),\n            MAD  = mad(value,  na.rm=TRUE),\n            Skew = skewness(value, na.rm=TRUE)) %&gt;%\n  mutate(Skew = replace_na(Skew, 0)) %&gt;%\n  mutate(Horizon = as.integer(str_sub(Horizon,6,6))) %&gt;%\n  ungroup() %&gt;%\n  drop_na()\n\n# Pivot wider to put each series in a column to merge with UNEMP data\nmxb &lt;- mx %&gt;%\n  pivot_longer(cols = -c(Date, Horizon), names_to = \"Variable\") %&gt;%\n  unite(\"name\", Variable:Horizon, sep=\"\") %&gt;%\n  pivot_wider() %&gt;%\n  slice(-n())\n\nUNEMP &lt;- UNRATE %&gt;%\n  #group_by(paste(year(Date), quarter(Date))) %&gt;%\n  #summarise(Date=min(Date), UNRATE=mean(UNRATE)) %&gt;%\n  select(Date, UNRATE) %&gt;%\n  right_join(mxb, by=\"Date\")\n\n# QReg \nsf &lt;- seq(.05,.95,.15)[-4]\n\ntail_colour   &lt;- \"grey95\"\ncentre_colour &lt;- c(\"maroon4\",\"seagreen\")\n\nnq  &lt;- length(sf)\nnv  &lt;- length(centre_colour)\ncol &lt;- colorRampPalette(c(rbind(tail_colour, centre_colour), tail_colour))(nv*nq+1)[-1]\n\nN   &lt;- nrow(UNEMP)\n\nK   &lt;- 12\nnt  &lt;- 6\n\nystart &lt;- 2017\nmeas   &lt;- c(\"None\") #c(\"None\", \"Var\")\nsetype &lt;- \"boot\" # \"rank\" # \"boot\" \"nid\"\n\nares &lt;- NULL\nfor (samp in seq(N-K,N,nt)) {\n    UNEMPs &lt;- head(UNEMP, samp)\n    for (nm in 1:length(meas)) {\n        for (i in 0:4) {\n          reg  &lt;- c(paste0(\"av\",i))\n          if(nm &gt; 1) reg &lt;- c(reg, paste0(meas[nm],i))\n          eq   &lt;- formula(paste0(\"lead(UNRATE,\",(i+1),\") ~ \", paste(reg, collapse = \" + \")))\n          eqrq &lt;- rq(eq, data=UNEMPs, tau=sf)\n          res  &lt;- broom::tidy(eqrq, se.type=setype) %&gt;% \n            group_by(tau) %&gt;%\n            mutate(dta = unlist(c(1, slice(select(UNEMPs, all_of(reg)), n())))) %&gt;% \n            mutate(q   = sum(estimate*dta)) %&gt;% \n            ungroup %&gt;%\n            mutate(Vintage = max(UNEMPs$Date), Horizon = i, Dispersion = meas[nm]) \n          ares &lt;- bind_rows(ares, res)\n        }\n    }\n}\n\naresx &lt;- ares %&gt;% \n  select(tau, q, Vintage, Horizon, Dispersion) %&gt;% \n  distinct() %&gt;%\n  left_join(select(UNEMP, Date, UNRATE), by=c(\"Vintage\" = \"Date\")) %&gt;% \n  mutate(q = if_else(Horizon == 0, UNRATE, q)) %&gt;% \n  pivot_wider(names_from = tau, names_prefix = \"tau=\", values_from = q) %&gt;% \n  group_by(Dispersion, Vintage) %&gt;% \n  mutate(fdate = seq.Date(from = Vintage[1], by = \"quarter\", length.out = 5)) %&gt;% \n  ungroup() %&gt;% \n  pivot_longer(starts_with(\"tau\"), names_to = \"q\", values_to = \"Vals\") %&gt;% \n  mutate(qs = paste0(Dispersion, gsub(\"tau=\", \"\", q))) %&gt;%\n  mutate(qs = factor(desc(qs))) %&gt;% \n  mutate(Dispersion = as_factor(Dispersion)) %&gt;% \n  select(-UNRATE)\n\nbck &lt;- UNEMP %&gt;% \n  select(Date, UNRATE) %&gt;%\n  filter(year(Date) &gt; ystart) %&gt;%\n  mutate(Vintage = list(unique(aresx$Vintage)))  %&gt;% \n  unnest(cols = Vintage) %&gt;% \n  group_by(Vintage) %&gt;% \n  filter(Date &lt;= Vintage) %&gt;% \n  ungroup() %&gt;% \n  arrange(Vintage, Date)\n\naresx %&gt;%\n  ggplot() +\n  geom_rect(aes(xmin=fdate, xmax=Vintage %m+% months(3), ymin=-Inf, ymax=Inf, group=q), fill=tail_colour) +\n  geom_area(aes(x=fdate, y=Vals, group=qs, fill=qs), position = \"identity\") +\n  scale_fill_manual(values=col) +\n  theme_minimal() + \n  theme(legend.position = \"none\") +\n  scale_x_date(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0)) +\n  geom_line(data=bck, aes(x=Date, y=UNRATE), colour=\"grey44\") + \n  facet_grid(Vintage ~ Dispersion) +\n  labs(title=paste(\"US unemployment rate and forecast fanchart\"), y=\"\", x=\"\") \n\n\n\n\n\n\n\n\nCapistrán, Carlos, and Allan Timmermann. 2009. “Forecast Combination with Entry and Exit of Experts.” Journal of Business & Economic Statistics 27 (4): 428–40.\n\n\nGaglianone, W. P., and L. R. Lima. 2012. “Constructing Density Forecasts from Quantile Regressions.” Journal of Money, Credit and Banking 44 (8): 1589–1607."
  }
]