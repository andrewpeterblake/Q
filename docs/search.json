[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantiles",
    "section": "",
    "text": "Genesis\nThis book is the result of an initiative that the Bank of England began in the early 1990s. This was a different world with a burgeoning new world order, as the Iron Curtain crumbled and the European experiment gathered momentum.\nPlaceholders abound.\nDisclaimer: The Bank of England does not accept any liability for misleading or inaccurate information or omissions in the information provided. The subject matter reflects the views of the individual presenter and not the wider Bank of England or its Policy Committees."
  },
  {
    "objectID": "intro.html#reading-is-good-for-you",
    "href": "intro.html#reading-is-good-for-you",
    "title": "1  Introduction",
    "section": "1.1 Reading is good for you",
    "text": "1.1 Reading is good for you\nFor me, the best (although slightly dated) text is Hastie, Tibshirani, and Friedman (2009) The Elements of Statistical Learning and the best source for the mathematics, with an easy-reading version by some of the same authors James et al. (2021) Introduction to Statistical Learning.\nI also rather like Boehmke and Greenwell (2019) Hands-On Machine Learning with R which is something of a cookbook rather than a technical manual but with wide scope. Taddy (2019) is more elementary.\nOn text, just read Silge and Robinson (2017) Text Mining with R: A Tidy Approach and then Hvitfeldt and Silge (2021) Supervised Machine Learning for Text Analysis in R. That’s it.\nTwo books I would solidly recommend to make us all into better statisticians and not just econometricians are Gelman, Hill, and Vehtari (2019) Regression and Other Stories, and McElreath (2020) Statistical Rethinking.\n\n\n\n\nBlake, Andrew P, and Haroon Mumtaz. 2017. Applied Bayesian Econometrics for Central Bankers. Revised. Technical Books. Centre for Central Banking Studies, Bank of England. https://www.bankofengland.co.uk/-/media/boe/files/ccbs/resources/applied-bayesian-econometrics-for-central-bankers-updated-2017.pdf.\n\n\nBoehmke, Brad, and Brandon M. Greenwell. 2019. Hands-on Machine Learning with R. The R Series. Boca Raton: Chapman & Hall/CRC. https://bradleyboehmke.github.io/HOML/.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2019. Regression and Other Stories. Cambridge: Cambridge University Press. http://www.stat.columbia.edu/~gelman/regression.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. New York, NY: Springer. https://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning for Text Analysis in R. Chapman & Hall: CRC Press. https://smltar.com/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. 2nd ed. Springer Texts in Statistics. New York, NY: Springer. https://www.statlearning.com/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. Abingdon, Oxfordshire: CRC Press. https://github.com/rmcelreath/rethinking.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. O’Reilly. https://www.tidytextmining.com/.\n\n\nTaddy, Matt. 2019. Business Data Science: Combining Machine Learning and Economics to Optimize, Automate, and Accelerate Business Decisions. New York, NY: McGraw-Hill Education. https://github.com/TaddyLab/BDS."
  },
  {
    "objectID": "R2021.html#selected-ml-and-dataviz-techniques-using-r",
    "href": "R2021.html#selected-ml-and-dataviz-techniques-using-r",
    "title": "2  Non-econometric methods for econometricians",
    "section": "2.1 Selected ML and Dataviz techniques using R",
    "text": "2.1 Selected ML and Dataviz techniques using R\nEconometricians are used to handling data, performing analysis and reporting results. But somewhere along the line data became big and unstructured, analysis was now machines learning about something and outputs became visualisations.\nThis online seminar takes some big(ish) datasets, sets the machines on them and draws some great graphs. If you ever wondered what use a tree was for forecasting or why everything is a network (including neural ones), or wanted to draw a map with your house in it, or to understand a document without the bother of reading it (or a few other things besides) then you might find something to interest you. All done in R.\nSpecifically, this seminar is designed to introduce some of the key methods used outside of econometrics that econometricians will find very useful in their work in a central bank. This includes some important machine learning techniques as a gateway to others, particularly tree-based methods and neural networks, as well as text processing and map making. All the way through there is an emphasis on the network properties of many of these techniques. We make extensive use of the tidyverse, including ggplot2 and tidytext, and a number of statistics, machine learning, geographical data and other packages.\nThe framework for each day is the following:\n\nEach day is divided into two two-hour sessions starting at 10.30 am and 2.00 pm GMT.\nThe first hour of each will be an online presentation covering a particular topic (or topics) with a look at both techniques and code.\nAfter a quick break the second hour will be largely devoted to the code itself or resources to understand how to code the material.\n\nWe may run polls during the event to prioritize the topics covered in the webinars as it is not expected that everyone will be able to try out everything.\n\n2.1.1 The code\nAll code and some of the data will be made available through the Juno portal. For each presentation the .Rmd (R markdown) file is supplied that creates the presentation, an HTML file of the presentation for you to step through which can be re-created from the .Rmd file, and a further .R file of the code that we use. Some additional code and data is included, including links to a number of videos that cover some additional aspects both in this file and in the presentations.\nSome data will need to be downloaded from original other sites if all the examples are to be followed. All code is additionally available at https://github.com/andrewpeterblake/R2020 or https://github.com/andrewpeterblake/R2021 or through the QR codes below.\n\n\n\n\n\nGitHub: 2020 (grey, left), 2021 (pink, right)\n\n\n\n\n\n\n\nGitHub: 2020 (grey, left), 2021 (pink, right)"
  },
  {
    "objectID": "R2021.html#how-to-ensure-rstudio-finds-the-code",
    "href": "R2021.html#how-to-ensure-rstudio-finds-the-code",
    "title": "2  Non-econometric methods for econometricians",
    "section": "2.2 HOW TO ENSURE RSTUDIO FINDS THE CODE",
    "text": "2.2 HOW TO ENSURE RSTUDIO FINDS THE CODE\nTo use the code, in particular so that R Studio finds the data files etc, create a directory for each topic, (e.g. Trees, ANN etc) and copy the contents from the zip file or GitHub. Then create a new project in R Studio that uses that directory as its home directory, using “File/New Project” in the drop down menu. Opening files within a project sets the home directory to that directory, so everything (including the sub-directories) can be found."
  },
  {
    "objectID": "R2021.html#typical-program-structure",
    "href": "R2021.html#typical-program-structure",
    "title": "2  Non-econometric methods for econometricians",
    "section": "2.3 Typical program structure",
    "text": "2.3 Typical program structure\n\n2.3.1 Day 1: Trees and maps\n\n2.3.1.1 Trees\n\nClassification and regression trees\nEconometrics strikes back: Bootstrap/bagging and Boosting/Model selection\nRandom forests\nVisualising decision trees\nUse example: House prices\n\nThe presentations for this are Trees.html and LondonHP.html; The two programs TreeCancer.R and TreeNW.R are the use examples.\n\n\n2.3.1.2 Maps\n\nHow to draw a map in R\nA guide to some resources\nChoropleths\nUse examples: Climate change, regional data, postcode wrangling\n\nThe presentation for this is MapAER.html (see also Weatherpretty.html); The program MapAERcode.R is the main map drawing code, I’ve included ZAF.R as as short simple way and source for two countries, and the directory Trendz contains the program (app.R) and data for the weather example.\n\n\n\n\n\n\n\n\n\nI’ve included an additional video (red QR code) for more about Shiny. This uses unemployment data from the Survey of Professional Forecasters. The code we look at is for climate change data World Bank data.\nA comprehensive treatment of maps is Lovelace, Nowosad, and Muenchow (2019) Geocomputation in R, but it is quite a lot to assimilate all at once.\n\n\n\n\n\n2.3.2 Day 2: Networks\n\n2.3.2.1 Neural networks\n\nWhat is an ANN? Deep learning?\nFunction approximation via a network\nData: fit, validate, test\nNetwork architecture\nUse examples: House prices revisited\n\nThe presentation for this is IntroANN.html; The program ANN.R replicates the ANN estimation. The data used is the same as for Day 1.\n\n\n2.3.2.2 Networks (real ones)\n\nDAGs and ANNs as network graphs\nIncidence matrices\nMeasuring connectivity: Degree and betweenness\nPlotting with igraph\nUse examples: Industry inter-relationships\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe presentation used for the first part of this is DAG.html and the program Draw_DAG_ANN.R draws the ANN examples from Day 2 Session 1 as well as some of the DAG examples. The example is modified from Cunningham (2021) Causal Inference: The Mixtape, which is a great read with R code. The pdf HandShake3.pdf is the source of the director network graphs, and Graph101a.R is a subset of the analytical work on the corruption data set as described in the post Graph Theory 101 (purple QR code), which is the work of Marina Medina (blue QR code link to presentation site).\n\n\n\n\n2.3.3 Day 3: Text\n\n2.3.3.1 Text modelling, a ‘tidytext’ approach (Session 1)\n\nData cleaning\nSentiment\nTopic modelling\n\n\n\n2.3.3.2 Text modelling, a ‘tidytext’ approach (Session 2)\n\nParts-of-speech tagging\nText regression\nUse examples: Central bank minutes, reports\n\n\n\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. New Haven & London: Yale University Press. https://mixtape.scunning.com/.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with R. 1st ed. The R Series. Boca Raton: Chapman & Hall/CRC. https://geocompr.robinlovelace.net/."
  },
  {
    "objectID": "QR.html#getting-the-data",
    "href": "QR.html#getting-the-data",
    "title": "4  Quantile regression",
    "section": "4.1 Getting the data",
    "text": "4.1 Getting the data\nWe download the data and save it locally.\n\nh <- \"https://www.philadelphiafed.org/-/media/frbp/assets/surveys-and-data/survey-of-professional-forecasters/historical-data/\"\nf <- \"meanlevel.xlsx\"\n\ndownload.file(paste0(h, f), destfile=f, mode=\"wb\")\n\nRetrieve the unemployment data for the average unemployment forecast.\n\nUNEMP <- f %>%\n  read_excel(na=\"#N/A\", sheet=\"UNEMP\") %>% \n  mutate(Date=as.Date(as.yearqtr(paste(YEAR, QUARTER), format=\"%Y %q\"))) \n\nUsel <- UNEMP %>% \n  select(Date, UNEMP1, UNEMP3, UNEMP4, UNEMP5, UNEMP6) %>%\n  mutate(UNRATE = lead(UNEMP1,1)) %>%\n  select(Date, UNRATE, \n         UNEMP1=UNEMP3, UNEMP2=UNEMP4, UNEMP3=UNEMP5, UNEMP4=UNEMP6) %>%\n  mutate(UNEMP1 = lag(UNEMP1,1), \n         UNEMP2 = lag(UNEMP2,2), \n         UNEMP3 = lag(UNEMP3,3), \n         UNEMP4 = lag(UNEMP4,4)) %>%\n  pivot_longer(cols = -c(Date, UNRATE), names_to=\"Which\", values_to=\"Val\") %>%\n  filter(year(Date) > 2000)"
  },
  {
    "objectID": "QR.html#plots",
    "href": "QR.html#plots",
    "title": "4  Quantile regression",
    "section": "4.2 Plots",
    "text": "4.2 Plots\n\nUsel %>% \n  ggplot(aes(x=Date)) + \n  geom_line(aes(y=UNRATE), colour=\"red\") + \n  geom_point(aes(y=Val, colour=Which, shape=Which)) +\n  theme_light() + \n  labs(title=\"Mean unemployment forecasts\", x=\"\", y=\"\", caption=\"Source: SPF\")"
  },
  {
    "objectID": "Stemp.html#study-question-1.3.2",
    "href": "Stemp.html#study-question-1.3.2",
    "title": "5  Causal Inference",
    "section": "5.1 Study question 1.3.2",
    "text": "5.1 Study question 1.3.2\nData:\n\nlibrary(tidyverse)\ned <- tibble(Gender = c(\"M\",\"M\",\"M\",\"M\",\"F\",\"F\",\"F\",\"F\"),\n             eLevel = c(\"U\",\"H\",\"C\",\"G\",\"U\",\"H\",\"C\",\"G\"),\n             num    = c(112,231,595,242,136,189,763,172)) %>%\n  mutate(total = sum(num))\n\nwhich we tabulate as\n\ned %>%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\n\n\n\n\nM\nU\n112\n2440\n\n\nM\nH\n231\n2440\n\n\nM\nC\n595\n2440\n\n\nM\nG\n242\n2440\n\n\nF\nU\n136\n2440\n\n\nF\nH\n189\n2440\n\n\nF\nC\n763\n2440\n\n\nF\nG\n172\n2440"
  },
  {
    "objectID": "Stemp.html#exercises-and-answers",
    "href": "Stemp.html#exercises-and-answers",
    "title": "5  Causal Inference",
    "section": "5.2 Exercises and answers",
    "text": "5.2 Exercises and answers\n\n5.2.1 Find \\(P(eLevel = H)\\)\n\ned %>%\n  filter(eLevel == \"H\") %>%\n  mutate(p_H = sum(num)/total) %>%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\np_H\n\n\n\n\nM\nH\n231\n2440\n0.1721311\n\n\nF\nH\n189\n2440\n0.1721311\n\n\n\n\n\n\n\n5.2.2 Find \\(P(eLevel = H\\ \\vee \\ Gender = F)\\)\n\ned %>%\n  filter(Gender == \"F\" | eLevel == \"H\") %>%\n  mutate(p_HorF = sum(num)/total) %>%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\np_HorF\n\n\n\n\nM\nH\n231\n2440\n0.6110656\n\n\nF\nU\n136\n2440\n0.6110656\n\n\nF\nH\n189\n2440\n0.6110656\n\n\nF\nC\n763\n2440\n0.6110656\n\n\nF\nG\n172\n2440\n0.6110656\n\n\n\n\n\n\n\n5.2.3 Find \\(P(eLevel = H\\ |\\ Gender = F)\\)\n\ned %>%\n  filter(Gender == \"F\") %>%\n  mutate(tcond = sum(num)) %>% \n  filter(eLevel == \"H\") %>%\n  mutate(p_HgivenF = sum(num)/tcond) %>%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\ntcond\np_HgivenF\n\n\n\n\nF\nH\n189\n2440\n1260\n0.15\n\n\n\n\n\n\n\n5.2.4 Find \\(P(Gender = F\\ | \\ eLevel = H)\\)\n\ned %>%\n  filter(eLevel == \"H\") %>%\n  mutate(tcond = sum(num)) %>% \n  filter(Gender == \"F\") %>%\n  mutate(p_FgivenH = sum(num)/tcond) %>%\n  kable()\n\n\n\n\nGender\neLevel\nnum\ntotal\ntcond\np_FgivenH\n\n\n\n\nF\nH\n189\n2440\n420\n0.45\n\n\n\n\n\n\n\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal Inference in Statistics: A Primer. Chichester: John Wiley & Sons. http://bayes.cs.ucla.edu/PRIMER/."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "5  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever…"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bahaj, Saleem, Angus Foulis, and Gabor Pinter. 2020. “Home Values\nand Firm Behavior.” American Economic Review 110 (7):\n2225–70. https://doi.org/10.1257/aer.20180649.\n\n\nBanbura, M., D. Giannone, and L. Reichlin. 2010. “Large\nBayesian Vector Auto Regressions.” Journal of\nApplied Econometrics 25 (1): 71–92.\n\n\nBlake, Andrew P, and Haroon Mumtaz. 2017. Applied Bayesian\nEconometrics for Central Bankers. Revised. Technical Books. Centre\nfor Central Banking Studies, Bank of England. https://www.bankofengland.co.uk/-/media/boe/files/ccbs/resources/applied-bayesian-econometrics-for-central-bankers-updated-2017.pdf.\n\n\nBoehmke, Brad, and Brandon M. Greenwell. 2019. Hands-on Machine\nLearning with R. The R Series. Boca\nRaton: Chapman & Hall/CRC. https://bradleyboehmke.github.io/HOML/.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. New\nHaven & London: Yale University Press. https://mixtape.scunning.com/.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2019. Regression and\nOther Stories. Cambridge: Cambridge University Press. http://www.stat.columbia.edu/~gelman/regression.\n\n\nGiannone, Domenico, Michele Lenza, and Giorgio E. Primiceri. 2015.\n“Prior Selection for Vector\nAutoregressions.” The Review of Economics and\nStatistics 97 (2): 436–51.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. The\nElements of Statistical Learning: Data Mining, Inference, and\nPrediction. 2nd ed. New York, NY: Springer. https://web.stanford.edu/~hastie/ElemStatLearn/.\n\n\nHvitfeldt, Emil, and Julia Silge. 2021. Supervised Machine Learning\nfor Text Analysis in R. Chapman & Hall: CRC Press.\nhttps://smltar.com/.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2021. An Introduction to Statistical Learning: With Applications in\nR. 2nd ed. Springer Texts in Statistics. New York, NY:\nSpringer. https://www.statlearning.com/.\n\n\nLovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019.\nGeocomputation with R. 1st ed. The R\nSeries. Boca Raton: Chapman & Hall/CRC. https://geocompr.robinlovelace.net/.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course\nwith Examples in R and Stan. 2nd ed. Abingdon,\nOxfordshire: CRC Press. https://github.com/rmcelreath/rethinking.\n\n\nPearl, Judea, Madelyn Glymour, and Nicholas P. Jewell. 2016. Causal\nInference in Statistics: A Primer. Chichester: John Wiley &\nSons. http://bayes.cs.ucla.edu/PRIMER/.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with\nR: A Tidy Approach. O’Reilly. https://www.tidytextmining.com/.\n\n\nTaddy, Matt. 2019. Business Data Science: Combining Machine Learning\nand Economics to Optimize, Automate, and Accelerate Business\nDecisions. New York, NY: McGraw-Hill Education. https://github.com/TaddyLab/BDS.\n\n\nTheil, Henri, and Arthur S. Goldberger. 1961. “On Pure and Mixed\nStatistical Estimation in Economics.” International Economic\nReview 2: 317–32."
  },
  {
    "objectID": "R2021.html#selected-ml-and-dataviz-techniques-in-r",
    "href": "R2021.html#selected-ml-and-dataviz-techniques-in-r",
    "title": "2  Non-econometric methods for econometricians",
    "section": "2.1 Selected ML and Dataviz techniques in R",
    "text": "2.1 Selected ML and Dataviz techniques in R\nEconometricians are used to handling data, performing analysis and reporting results. But somewhere along the line data became big and unstructured, analysis was now machines learning about something and outputs became visualisations.\nThis online seminar takes some big(ish) datasets, sets the machines on them and draws some great graphs. If you ever wondered what use a tree was for forecasting or why everything is a network (including neural ones), or wanted to draw a map with your house in it, or to understand a document without the bother of reading it (or a few other things besides) then you might find something to interest you. All done in R.\nSpecifically, this seminar is designed to introduce some of the key methods used outside of econometrics that econometricians will find very useful in their work in a central bank. This includes some important machine learning techniques as a gateway to others, particularly tree-based methods and neural networks, as well as text processing and map making. All the way through there is an emphasis on the network properties of many of these techniques. We make extensive use of the tidyverse, including ggplot2 and tidytext, and a number of statistics, machine learning, geographical data and other packages.\nThe framework for each day is the following:\n\nEach day is divided into two two-hour sessions starting at 10.30 am and 2.00 pm GMT.\nThe first hour of each will be an online presentation covering a particular topic (or topics) with a look at both techniques and code.\nAfter a quick break the second hour will be largely devoted to the code itself or resources to understand how to code the material.\n\nWe may run polls during the event to prioritize the topics covered in the webinars as it is not expected that everyone will be able to try out everything.\n\n2.1.1 The code\nAll code and some of the data will be made available through the Juno portal. For each presentation the .Rmd (R markdown) file is supplied that creates the presentation, an HTML file of the presentation for you to step through which can be re-created from the .Rmd file, and a further .R file of the code that we use. Some additional code and data is included, including links to a number of videos that cover some additional aspects both in this file and in the presentations.\nSome data will need to be downloaded from original other sites if all the examples are to be followed. All code is additionally available at https://github.com/andrewpeterblake/R2020 or https://github.com/andrewpeterblake/R2021 or through the QR codes below.\n\n\n\n\n\nGitHub: 2020 (grey, left), 2021 (pink, right)\n\n\n\n\n\n\n\nGitHub: 2020 (grey, left), 2021 (pink, right)"
  },
  {
    "objectID": "index.html#genesis",
    "href": "index.html#genesis",
    "title": "Quantiles",
    "section": "Genesis",
    "text": "Genesis\nThis book is the result of an initiative that the Bank of England began in the early 1990s. This was a different world with a burgeoning new world order, as the Iron Curtain crumbled and the European experiment gathered momentum.\nPlaceholders abound."
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Quantiles",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe Bank of England does not accept any liability for misleading or inaccurate information or omissions in the information provided. The subject matter reflects the views of the individual presenter and not the wider Bank of England or its Policy Committees."
  },
  {
    "objectID": "index.html#sec-genesis",
    "href": "index.html#sec-genesis",
    "title": "Quantiles, Networks, Time",
    "section": "Genesis",
    "text": "Genesis\nThis book is the result of an initiative the Bank of England began in the early 1990s, and a prescient one. This was at a major historical turning point, one that signalled a burgeoning new world order, as the Iron Curtain crumbled, the European experiment gathered momentum, and industrial might began an inexorable shift eastwards.\n\nPlaceholders abound."
  },
  {
    "objectID": "index.html#sec-disclaimer",
    "href": "index.html#sec-disclaimer",
    "title": "Quantiles, Networks, Time",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe Bank of England does not accept any liability for misleading or inaccurate information or omissions in the information provided. The subject matter reflects the views of the individual presenter and not the wider Bank of England or its Policy Committees."
  },
  {
    "objectID": "Maps.html#how-heterogenous-is-uk-house-price-inflation",
    "href": "Maps.html#how-heterogenous-is-uk-house-price-inflation",
    "title": "5  Mapping regional house price inflation",
    "section": "5.1 How heterogenous is UK house price inflation?",
    "text": "5.1 How heterogenous is UK house price inflation?\nA simple enough question, and one that Bahaj, Foulis, and Pinter (2020) thought was best answered with a map – actually a referee asked for one. As I know how to draw a map in R they asked me if I could do it. Well yes, but there are some particular difficulties.\n\nThe UK (actually Great Britain) is an awkward (but not too awkward) shape.\nPopulation in the UK is heavily concentrated in a small number of centres, such as London or Manchester.\nThere are three different periods to compare.\nIt has to be in grayscale.\n\nBefore all of this we need some data, with boundaries that correspond to areas that we have data for. The regional inflation data is available at the level of the Land Registry, which almost by local authority but amalgamates a number of the areas. So a map at Local Authority level would be fine as long as we can amalgamate some of the regions.\nThe map data used here is available from the UK’s ONS geoportal, with a lot of administrative data available including local authority boundaries. The Local Authority data is specifically available from here, where I use the clipped full extent version. There are a number of possibilities, but in general high water mark, and enough but not too much detail is needed.\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(sf)\n\nThe information in the map file is comprehensive, and by Local Authority as of December 2015.\n\nfle <- \"LAD_Dec_2015_GCB_GB\"\nshape <- read_sf(dsn=\".\", layer=fle)\n\nWe can look at the attributes using summary.\n\nsummary(shape)\n\n   lad15cd            lad15nm            lad15nmw           GlobalID        \n Length:380         Length:380         Length:380         Length:380        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n          geometry  \n MULTIPOLYGON :380  \n epsg:27700   :  0  \n +proj=tmer...:  0  \n\n\nThis can be plotted straightforwardly using ggplot.\n\nshape %>%\n  ggplot() +\n  geom_sf(aes(geometry=geometry, fill=lad15nm), \n          color=NA, alpha=.66, show.legend=FALSE) +\n  theme_void()\n\n\n\n\n\n\n\n\nLooking at the read-out above, each of the 380 regions have some metadata associated, which are contained in each of the listed attributes. It should be obvious that objectid is just a sequence from 1 to 380. lad15nm turns out to be a list of names of the regions – I suspect lad for Local Authority District, 15 for 2015 and nm for name – and it is easy to specify this as the name to use for the region when using tidy.\nNow this can be plotted using ggplot, using geometry for the \\(x\\) and \\(y\\) coordinates. The choice of fill colour is determined by fill and we can set the colour of the lines by colour (or color). The two extra arguments are for a suitable blank style and to impose an appropriate ratio of height to width.\nImmediately, the awkward shape of the British Isles is apparent. (Note this is a plot of Great Britain, and there is no Northern Ireland.) The islands to the far north are somewhat unnecessary, although quite rightly the inhabitants get a bit tired of being left off maps! Nonetheless I’ll do exactly the same by filtering out the polygons associated with Orkney Islands and Shetland Islands.\nFewer Scottish Islands makes the graphs a lot clearer with little loss of information, given the tiny number of transactions in the Orkneys and the Shetlands, very far to the north.\nIn what follows we filter out the islands using\n\nshape <- read_sf(dsn=\".\", layer=fle) %>%\n  filter(!lad15nm %in% c(\"Shetland Islands\",\"Orkney Islands\")) %>%\n  mutate(Country=str_sub(lad15cd, 1, 1), .after=1)\n\nwhere we also create an indicator of country using the first letter of the code string.\nSo the country map is\n\nshape %>%\n  group_by(Country) %>%\n  summarise() %>%\n  ggplot() +\n  geom_sf(aes(fill=Country), color=\"grey77\", linewidth=.25, alpha=.66) +\n  theme_void()\n\n\n\n\n\n\n\n\nNote the really nice feature – if we group by something, in this case country, we can summarise to amalgamate the geometries!\nYou may have noticed, one thing that that’s missing on the LA graphs is the boundaries. They aren’t, they’re just invisible. That’s because I set colour = NA, so I can fix that by choosing a colour and making the lines very thin so they don’t swamp the map, as in the country one.\nOne further amendment, the fill is moved inside the aes() specification and made conditional. R now chooses unique colours for each of the regions.\nTwo things now need to be done to get the map colours right to illustrate regional inflation rates. First we need to amalgamate some of the Local Authority boundaries to the Land Registry definitions, and second we need to assign the inflation rate to each area."
  },
  {
    "objectID": "Maps.html#inflation-in-grayscale",
    "href": "Maps.html#inflation-in-grayscale",
    "title": "5  Mapping regional house price inflation",
    "section": "5.2 Inflation in grayscale",
    "text": "5.2 Inflation in grayscale\nAll the information required to plot the Land Registry-based regional inflation rates is now available. As you can see from the buckinghamshire data above, there are three average rates in three different periods, so I’ll focus on one, 2002-2007 to begin with.\nFirst, augment the geographic data with the inflation data, and call them something better.\n\ngg %>%\n  ggplot() +\n  geom_sf(aes(geometry=geometry, fill=land_reg_region), \n          color=NA, alpha=.66, show.legend = FALSE) +\n  theme_void()\n\n\n\n\n\n\n\n\nThen calculate some limits for the gradient, and modify the specify a plot to include specifically gray and put the legend at the bottom.\n\nnms <- gsub(\"av_hp_growth\", \"HPI\", colnames(hp_data))\n\nhp_data %>%\n  rename_all( ~ nms) %>%\n  select(land_reg_region, starts_with(\"HPI\")) %>%\n  distinct() %>%\n  left_join(gg) %>%\n  ggplot() +\n  geom_sf(aes(geometry=geometry, fill=HPI_02_07), \n          color=NA, alpha=.66, show.legend = FALSE) +\n  theme_void() +\n  scale_fill_gradient(low=grey(0.9), high=grey(0.05)) +\n  theme(legend.direction = \"horizontal\", \n        legend.position  = c(0.75,0.05),\n        legend.title     = element_blank())\n\nJoining with `by = join_by(land_reg_region)`\n\n\n\n\n\n\n\n\n\n\n\n\n\nBahaj, Saleem, Angus Foulis, and Gabor Pinter. 2020. “Home Values and Firm Behavior.” American Economic Review 110 (7): 2225–70. https://doi.org/10.1257/aer.20180649."
  },
  {
    "objectID": "Maps.html#inflation-data-and-regions",
    "href": "Maps.html#inflation-data-and-regions",
    "title": "5  Mapping regional house price inflation",
    "section": "5.2 Inflation data and regions",
    "text": "5.2 Inflation data and regions\nWe have a map, and we have that data in a form that is easy to understand. If we can suitably attach an inflation rate to each area then we can fill the individual areas with a colour unique to each individual inflation rates.\nRecall that the Land Registry areas aren’t quite what we have, and will need amalgamating. Bahaj, Foulis, and Pinter (2020) supplied me the areas that needed amalgamating (and the inflation rates) using the ONS codes. This is contained in the metadata lad15cd above.\nThe data is structured in ‘wide’ format with one row for each Land Registry region. The details aren’t very important for us now, but what it means is I can manipulate it to get\n\n# Price data by Land Registry region, converted to long format\nhp_data <- read_excel(\"house_price_data_figure_1.xls\")  %>% \n  select(\"land_reg_region\", starts_with(\"e_\"), starts_with(\"av_\")) %>% \n  pivot_longer(names_to  = \"name\", \n               values_to = \"lad15cd\", \n               cols      = c(-land_reg_region, -starts_with(\"av_\"))) %>% \n  drop_na() %>%\n  select(land_reg_region, lad15cd, starts_with(\"av_\")) \n\ncodes <- hp_data %>% \n  select(lad15cd, land_reg_region) \n\nThe important thing that the pivot_longer achieves is that for every land_reg_region I get a list of all the ONS codes that makes up the Local Authority level. So if I look at buckinghamshire as an example there are four ONS codes now associated with it.\n\nfilter(codes, land_reg_region == \"buckinghamshire\")\n\n# A tibble: 4 × 2\n  lad15cd   land_reg_region\n  <chr>     <chr>          \n1 E07000004 buckinghamshire\n2 E07000005 buckinghamshire\n3 E07000006 buckinghamshire\n4 E07000007 buckinghamshire\n\n\nJoin these together\n\n# Join polygons defined by Land Registry regions\ngg <- shape %>%\n  select(starts_with(c(\"lad\",\"C\"))) %>% \n  left_join(codes, by=\"lad15cd\") %>%\n  group_by(land_reg_region) %>%\n  summarise() \n\nwhich produces a match between the Land Registry and the Local Authority areas, plus the inflation rates.\n\n5.2.1 Inflation in grayscale\nAll the information required to plot the Land Registry-based regional inflation rates is now available. As you can see from the buckinghamshire data above, there are three average rates in three different periods, so I’ll focus on one, 2002-2007 to begin with.\nFirst, augment the geographic data with the inflation data, and call them something better.\n\ngg %>%\n  ggplot() +\n  geom_sf(aes(geometry=geometry, fill=land_reg_region), \n          color=NA, alpha=.66, show.legend = FALSE) +\n  theme_void()\n\n\n\n\n\n\n\n\nThen specify gray and put the legend at the bottom.\n\nnms <- gsub(\"av_hp_growth\", \"HPI\", colnames(hp_data))\n\nhp_data %>%\n  rename_all( ~ nms) %>%\n  select(land_reg_region, starts_with(\"HPI\")) %>%\n  distinct() %>%\n  left_join(gg) %>%\n  ggplot() +\n  geom_sf(aes(geometry=geometry, fill=HPI_02_07), \n          color=NA, alpha=.66, show.legend = TRUE) +\n  theme_void() +\n  scale_fill_gradient(low=grey(0.9), high=grey(0.05)) +\n  theme(legend.direction = \"horizontal\", \n        legend.position  = c(0.75,0.05),\n        legend.title     = element_blank())\n\nJoining with `by = join_by(land_reg_region)`\n\n\n\n\n\n\n\n\n\n\n\n\n\nBahaj, Saleem, Angus Foulis, and Gabor Pinter. 2020. “Home Values and Firm Behavior.” American Economic Review 110 (7): 2225–70. https://doi.org/10.1257/aer.20180649."
  },
  {
    "objectID": "BVAR.html#estimating-bvars-using-us-data",
    "href": "BVAR.html#estimating-bvars-using-us-data",
    "title": "3  BVAR with dummies",
    "section": "3.1 Estimating BVARs using US data",
    "text": "3.1 Estimating BVARs using US data\nWe will the Fed Funds rate, annual GDP growth and annual CPI inflation data from FRED, retrieved 2023-05-23. These are:\n\n\n\n\n\n\n\n\n\nWe will build a variety and two and three variable BVARs. More details on the data are given below."
  },
  {
    "objectID": "BVAR.html#bvars-with-dummy-variable-priors",
    "href": "BVAR.html#bvars-with-dummy-variable-priors",
    "title": "3  BVAR with dummies",
    "section": "3.2 BVARs with dummy variable priors",
    "text": "3.2 BVARs with dummy variable priors\nRather than combine a prior distribution with a likelihood and draw from the resulting joint posterior distribution there is another convenient way of parameterizing the problem. We can instead add some ‘dummy variables’ that have the same properties of the prior so we have a single modified likelihood that incorporates the prior information. This approach was most obviously adopted by Banbura, Giannone, and Reichlin (2010). Further discussion of this can be found in Giannone, Lenza, and Primiceri (2015). In general this is a version of the Theil and Goldberger (1961) mixed estimator given a Bayesian interpretation.\n\n3.2.1 VAR model\nSimple bi-variate two-lag VAR model: \\[\n  \\left[\\matrix{g_t \\cr \\pi_t}\\right] =\n  \\left[\\matrix{c_1 \\cr c_2}\\right] +\n  \\left[\\matrix{b_{11} & b_{12} \\cr\n    b_{21} & b_{22}}\\right]\n\\left[\\matrix{g_{t-1} \\cr \\pi_{t-1} }\\right] +\n   \\left[\\matrix{d_{11} & d_{12} \\cr\n    d_{21} & d_{22}}\\right]\n\\left[\\matrix{g_{t-2} \\cr \\pi_{t-2} }\\right] +\n  \\left[\\matrix{\\nu_{g,t} \\cr \\nu_{\\pi,t}}\\right]\n\\] \\[\n  \\left[\\matrix{\\nu_{g,t} \\cr \\nu_{\\pi,t}}\\right] \\sim N(0, \\Sigma)\n\\]"
  },
  {
    "objectID": "BVAR.html#bvar-hyperparameters",
    "href": "BVAR.html#bvar-hyperparameters",
    "title": "3  BVAR with dummies",
    "section": "3.3 BVAR hyperparameters",
    "text": "3.3 BVAR hyperparameters\nWe will (similarly to the straightforward Minnesota prior) need some control parameters:\n\n\\(\\tau\\) controls the overall tightness of the prior for the AR coefficients\n\\(d\\) controls the prior on higher lags;\n\\(\\lambda\\) controls the prior on constants;\n\\(\\gamma\\) controls the prior on the sum of coefficients;\n\\(\\delta\\) controls the cointegration prior;\n\nwhere\n\n\\(\\sigma_i\\) standard deviation of error terms from individual OLS regressions;\n\\(\\mu_i\\) sample means of the data.\n\n\n3.3.0.1 First lag\nNow consider the following artificial data for the first lag. We construct some dummy observations of the dependent and explanatory variables that look like: \\[\n  Y_{D,1} = \\left[\\matrix{\\frac{1}{\\tau}\\sigma_1 & 0 \\cr\n    0 & \\frac{1}{\\tau}\\sigma_2}\\right]\n\\] and \\[\n  X_{D,1} = \\left [ \\matrix{0 & \\frac{1}{\\tau}\\sigma_1 & 0 & 0 & 0\\cr\n    0 & 0 & \\frac{1}{\\tau}\\sigma_2 & 0 & 0}\\right]\n\\] Intuition: \\[\n  \\left[\\matrix{\\frac{\\sigma_1}{\\tau} & 0 \\cr\n    0 & \\frac{\\sigma_2}{\\tau}}\\right] =\n  \\left[\\matrix{0 & \\frac{\\sigma_1}{\\tau} & 0 & 0 & 0\\cr\n   0 & 0 & \\frac{\\sigma_2}{\\tau} & 0 & 0} \\right]\n\\left[\\matrix{c_1    & c_2 \\cr\n  b_{11} & b_{21} \\cr\n  b_{12} & b_{22} \\cr\n  d_{11} & d_{21} \\cr\n  d_{12} & d_{22}}\\right]\n+\n  \\left[\\matrix{\\xi_{11} & \\xi_{12} \\cr \\xi_{21} & \\xi_{22} }\\right]\n\\] Multiplying out we get: \\[\n  \\left[\\matrix{\\frac{\\sigma_1}{\\tau} & 0 \\cr\n    0 & \\frac{\\sigma_2}{\\tau}}\\right]\n=\n  \\left[\\matrix{\\frac{\\sigma_1}{\\tau}b_{11} &   \\frac{\\sigma_1}{\\tau}b_{21}\\cr\n  \\frac{\\sigma_2}{\\tau}b_{12} &  \\frac{\\sigma_2}{\\tau}b_{22}} \\right]\n+\n  \\left[\\matrix{\\xi_{11} & \\xi_{12} \\cr \\xi_{21} & \\xi_{22} }\\right]\n\\] Concentrating on the first row, notice: \\[\n  \\frac{\\sigma_1}{\\tau} = \\frac{\\sigma_1}{\\tau}b_{11} + \\xi_{11}\n\\] implying: \\[\n  b_{11} = 1 - \\frac{\\tau}{\\sigma_1}\\xi_{11}\n\\] so we can write: \\[\n  b_{11} \\sim N\\left(1, \\frac{\\tau^2var(\\xi_{11})}{\\sigma^2_1}\\right)\n\\] as \\(E[b_{11}] = 1 - \\frac{\\tau}{\\sigma_1}E[\\xi_{11}] = 1\\) and the variance is easily derived. Similarly: \\[\n  b_{12} = - \\frac{\\tau}{\\sigma_1}\\xi_{12}\n\\] which is clearly zero in expectation.\n\n\n3.3.1 Further priors\n\n3.3.1.1 Higher lags\nRather than derive the implications we state the rest of the dummy priors. Consider the following artificial data for the second lag: \\[\nY_{D,2} = \\left[\\matrix{0 & 0 \\cr 0 & 0}\\right]\n\\] and: \\[\nX_{D,2} = \\left [ \\matrix{0 & 0 & 0 & \\frac{\\sigma_1 2^d}{\\tau} & 0 \\cr\n    0 & 0 & 0 & 0 & \\frac{\\sigma_2 2^d}{\\tau} }\\right]\n\\] We can multiply these out and check the properties, in particular we can verify in the same way as for the first lag that: \\[\n  b_{ji} \\sim N\\left(0, \\frac{1}{4}\\frac{\\tau^2var(\\xi_{ji})} {2^d\\sigma^2_j}\\right)\n\\] for \\(j=1,...N\\), \\(i=1,...l\\).\n\n\n3.3.1.2 Constant\nConsider the following artificial data for the constant: \\[\n  Y_{D,3} = \\left[\\matrix{0 & 0 }\\right]\n\\] \\[\n  X_{D,3} = \\left [ \\matrix{\\lambda & 0 & 0 & 0 & 0 }\\right]\n\\] so \\(\\lambda c_1 = \\varepsilon_1\\) and \\(\\lambda c_2 = \\varepsilon_2\\). As \\(\\lambda \\rightarrow \\infty\\) the prior is implemented more tightly.\n\n\n3.3.1.3 Covariances\nDummy observations to implement the prior on the error covariance matrix are: \\[\n  Y_{D,4} = \\left[\\matrix{\\sigma_1 & 0 \\cr 0 & \\sigma_2}\\right]\n\\] and \\[\n  X_{D,4} = \\left [ \\matrix{0 & 0 & 0 & 0 & 0 \\cr\n    0 & 0 & 0 & 0 & 0 }\\right]\n\\] with the magnitude of the diagonal elements of \\(\\Sigma\\) controlled by the scale of the diagonal elements of \\(Y_{D,4}\\), as larger diagonal elements implement the prior belief that the variance of \\(\\nu_1\\) and \\(\\nu_2\\) is larger.\nBanbura, Giannone, and Reichlin (2010) stop here, but there are additional priors that could be added.\n\n\n3.3.1.4 Sum of coefficients\nWe could add a prior that reflects the belief that the sum of coefficients on ‘own’ lags add up to 1. This is an additional ‘unit root’-style prior. Consider: \\[\n  Y_{D,5} = \\left[\\matrix{\\gamma\\mu_1 & 0\\cr 0 & \\gamma\\mu_2}\\right]\n\\] and \\[\n  X_{D,5} = \\left [ \\matrix{0 & \\gamma\\mu_1 & 0 & \\gamma\\mu_1 & 0\\cr\n    0 & 0 & \\gamma\\mu_2 & 0 & \\gamma\\mu_2}\\right]\n\\] where \\(\\mu_1\\) is the sample mean of \\(y_t\\) and \\(\\mu_2\\) is the sample mean of \\(x_t\\). Note that these dummy observations imply prior means of the form \\(b_{ii} + d_{ii} = 1\\) where \\(i = 1, 2\\) and \\(\\gamma\\) controls the tightness of the prior. As \\(\\gamma \\rightarrow \\infty\\) the prior is implemented more tightly. Forecast growth rates eventually converge to their sample averages.\n\n\n3.3.1.5 Trends\nWe can also specify common stochastic trend dummies: \\[\n  Y_{D,6} = \\left[\\matrix{\\delta\\mu_1 & \\delta\\mu_2 }\\right]\n\\] and \\[\n  X_{D,6} = \\left [ \\matrix{\\delta & \\delta\\mu_1 & \\delta\\mu_2 & \\delta\\mu_1 & \\delta\\mu_2 }\\right]\n\\] where this imposes that the coefficients are consistent with limiting the amount of drift between the predictions at their average values.\n\n\n\n3.3.2 Implementation\nThe data and the artificial data are now stacked: \\[\n  Y^* = \\left[\\matrix{ g_3 & \\pi_3 \\cr\n    \\vdots & \\vdots \\cr\n    g_T & \\pi_T \\cr\n    \\frac{1}{\\tau}\\sigma_1 & 0 \\cr\n    0 & \\frac{1}{\\tau}\\sigma_2\\cr\n    0 & 0 \\cr\n    0 & 0 \\cr\n    0 & 0 \\cr\n    \\sigma_1 & 0 \\cr\n    0 & \\sigma_2 \\cr\n    \\gamma\\mu_1 & 0 \\cr\n    0 & \\gamma\\mu_2 \\cr\n    \\delta\\mu_1 & \\delta\\mu_2 }\\right], \\quad\nX^* = \\left [ \\matrix{1 & g_2 & \\pi_2 & g_1 & \\pi_1 \\cr\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\cr\n  1 & g_{T-1} & \\pi_{T-1} & g_{T-2} & \\pi_{T-2} \\cr\n  0 & \\frac{1}{\\tau}\\sigma_1 & 0 & 0 & 0\\cr\n  0 & 0 & \\frac{1}{\\tau}\\sigma_2 & 0 & 0\\cr\n  0 & 0 & 0 & \\frac{\\sigma_1 2^d}{\\tau} & 0 \\cr\n  0 & 0 & 0 & 0 & \\frac{\\sigma_2 2^d}{\\tau} \\cr\n  \\lambda & 0 & 0 & 0 & 0 \\cr\n  0 & 0 & 0 & 0 & 0 \\cr\n  0 & 0 & 0 & 0 & 0 \\cr\n  0 & \\gamma\\mu_1 & 0 & \\gamma\\mu_1 & 0 \\cr\n  0 & 0 & \\gamma\\mu_2 & 0 & \\gamma\\mu_2 \\cr\n  \\delta & \\delta\\mu_1 & \\delta\\mu_2 & \\delta\\mu_1 & \\delta\\mu_2 }\\right]\n\\] Estimation via Gibbs sampling now proceeds in a very straightforward way. There is no need to draw for the prior separately."
  },
  {
    "objectID": "BVAR.html#examples",
    "href": "BVAR.html#examples",
    "title": "3  BVAR with dummies",
    "section": "3.4 Examples",
    "text": "3.4 Examples\nFirst we use quarterly US Growth (FRED series A191RO1Q156NBEA) and CPI (FRED series CPALTT01USQ661S) expressed as the annual inflation rate from 1961-01-01 to 2023-01-01 in a bi-variate BVAR. The last ten observations are:\n\n\n\n\n\nDate\nGrowth\nInflation\n\n\n\n\n2020-10-01\n-1.5\n1.224176\n\n\n2021-01-01\n1.2\n1.905310\n\n\n2021-04-01\n12.5\n4.776278\n\n\n2021-07-01\n5.0\n5.264633\n\n\n2021-10-01\n5.7\n6.765892\n\n\n2022-01-01\n3.7\n8.023109\n\n\n2022-04-01\n1.8\n8.556077\n\n\n2022-07-01\n1.9\n8.284860\n\n\n2022-10-01\n0.9\n7.110821\n\n\n2023-01-01\n1.6\n5.769521\n\n\n\n\n\nWe specify a VAR with two lags, and use it to forecast 12 periods ahead. The BVAR are specified using the names above, with only tau particularly binding in this case. We set the total number of iterations in each case to 20000 and discard the first half. The parameter nb is used to set how much back data should appear in a fan chart.\n\n#########\n# Options\n#########\nnf <- 12 # Max forecast horizon\nnb <- 21 # No. back periods plotted in graphs\nl  <- 2  # Number of lags in VAR\n\n# specify parameters of the Minnesota-type prior\ntau    <- .1   # controls prior on own 1st lags (1 makes wibbly)\nd      <- 1    # decay for higher lags\nlambda <- 1    # prior for the constant\ngamma  <- 1    # sum of coefficients unit roots\ndelta  <- 1    # cointegration prior\n\n# Gibbs control\nreps <- 20000 # total numbers of Gibbs iterations\nburn <- 10000 # number of burn-in iterations\n\nIn what follows we vary tau and the lag length to illustrate their effects. To do this we create the augmented data and then run the Gibbs sampler, using:\n\n# Create augmented data\nYplus <- augmentData(Y, l, tau, d, lambda, gamma, delta)\n\n# Run Gibbs sampler\nout   <- Gibbs_estimate(Yplus[[1]], Yplus[[2]], reps, burn, 1, nf)\n\nwhere Y contains the data in a dataframe/tibble with the date in the first column as in the data example above. The code strips out the date and then uses the remaining \\(N\\) columns in the BVAR. See the Code Appendix for the details of the functions.\n\n\n\nThe output contains any forecast draws from the Gibbs sampler in the third list element from the Gibbs_estimate() function. The first two elements are coefficient draws. Two further functions plots the fan charts using the Gibbs draws:\n\n# String to put in subtitle\ncontrols <- paste0(\"Lag length \", l, \": tau=\", tau, \", d=\", d,\n                   \", lambda=\", lambda, \", gamma=\", gamma, \", delta=\", delta)\n\n# Plots\nfan_chart(Y, out[[3]], controls, nb)\np           <- coeff_plot(Y, l, out[[1]], out[[2]], 333, controls)\npnum        <- pnum+1\npce[[pnum]] <- p[[1]]\n\nwhere the string controls is put in the chart subtitle and the coefficient densities. It can be anything but is a good place to remind yourself of how you specified the model. Notice we save the coefficient plots for later use.\n\n3.4.0.1 Example 1: BVAR(2) with \\(\\tau=0.1\\)\n\n\n\n\n\n\n\n\n\n\n\n3.4.0.2 Example 2: BVAR(2) with \\(\\tau=1\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4.0.3 Example 3: BVAR(6) with \\(\\tau=0.1\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4.0.4 Example 4: BVAR(6) with \\(\\tau=1\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4.0.5 Coefficient estimates\nAll of these have underlying parameters. Their estimated posterior densities are:"
  },
  {
    "objectID": "BVAR.html#tri-variate-bvar",
    "href": "BVAR.html#tri-variate-bvar",
    "title": "3  BVAR with dummies",
    "section": "3.5 Tri-variate BVAR",
    "text": "3.5 Tri-variate BVAR\nNow we add the FedFunds rate (FRED series FEDFUNDS), so the last ten periods of the data set is now:\n\n\n\n\n\nDate\nGrowth\nInflation\nFedFunds\n\n\n\n\n2020-10-01\n-1.5\n1.224176\n0.09\n\n\n2021-01-01\n1.2\n1.905310\n0.09\n\n\n2021-04-01\n12.5\n4.776278\n0.07\n\n\n2021-07-01\n5.0\n5.264633\n0.10\n\n\n2021-10-01\n5.7\n6.765892\n0.08\n\n\n2022-01-01\n3.7\n8.023109\n0.08\n\n\n2022-04-01\n1.8\n8.556077\n0.33\n\n\n2022-07-01\n1.9\n8.284860\n1.68\n\n\n2022-10-01\n0.9\n7.110821\n3.08\n\n\n2023-01-01\n1.6\n5.769521\n4.33\n\n\n\n\n\nTwo more examples follow.\n\n3.5.0.1 Example 5: BVAR(4) with \\(\\tau=.1\\), 3 variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.0.2 Example 6: BVAR(6) with \\(\\tau=1\\), 3 variables"
  },
  {
    "objectID": "BVAR.html#code-appendix",
    "href": "BVAR.html#code-appendix",
    "title": "3  BVAR with dummies",
    "section": "3.6 Code appendix",
    "text": "3.6 Code appendix\nYou can download the program and functions used for the estimates above from the links below. Put them in the same directory and they should recreate exactly (within sampling error) the same graphs as above. Ensure you have all the libraries available that are loaded at the top of BVARdum.R.\nMain program:\n\n\nDownload BVARdum.R\n\n\nFunctions:\n\n\nDownload BVARdumFUNCs.R\n\n\n\n\n\n\nBanbura, M., D. Giannone, and L. Reichlin. 2010. “Large Bayesian Vector Auto Regressions.” Journal of Applied Econometrics 25 (1): 71–92.\n\n\nGiannone, Domenico, Michele Lenza, and Giorgio E. Primiceri. 2015. “Prior Selection for Vector Autoregressions.” The Review of Economics and Statistics 97 (2): 436–51.\n\n\nTheil, Henri, and Arthur S. Goldberger. 1961. “On Pure and Mixed Statistical Estimation in Economics.” International Economic Review 2: 317–32."
  }
]